This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitattributes
.github/CONTRIBUTING.md
.github/ISSUE_TEMPLATE/code_issue.yml
.github/ISSUE_TEMPLATE/opinion.yml
.gitignore
chapter_directory_parser.py
config_manager.py
config.example.json
consistency_checker.py
embedding_adapters.py
icon.ico
install-pyenv-win.ps1
LICENSE
llm_adapters.py
main.py
main.spec
novel_generator/__init__.py
novel_generator/architecture.py
novel_generator/blueprint.py
novel_generator/chapter.py
novel_generator/common.py
novel_generator/finalization.py
novel_generator/knowledge.py
novel_generator/qa.py
novel_generator/vectorstore_utils.py
prompt_definitions.py
README.md
READMEEN.md
requirements.txt
tooltips.py
ui/__init__.py
ui/chapters_tab.py
ui/character_tab.py
ui/config_tab.py
ui/context_menu.py
ui/directory_tab.py
ui/generation_handlers.py
ui/helpers.py
ui/main_tab.py
ui/main_window.py
ui/novel_params_tab.py
ui/other_settings.py
ui/role_library.py
ui/setting_tab.py
ui/summary_tab.py
utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto
</file>

<file path=".github/CONTRIBUTING.md">
# Contributing to This Project

首先，感谢你愿意为本项目贡献力量！在提交任何形式的反馈或 Pull Request 之前，请先阅读以下内容。

---

## 一、反馈类型说明

1. **代码问题（Code Issue）**  
   - 仅限与项目代码本身相关的问题：如编译失败、运行报错、逻辑缺陷等。  
   - 反馈之前，请确认该问题与你的环境或配置无关。  
   - 如果确认是代码本身导致的错误，请使用 [代码问题反馈模板](?template=code_issue.yml)。

2. **意见或建议（Opinion / Enhancement）**  
   - 如果你有关于功能新增、代码重构、性能优化或其他方面的意见或建议，请使用 [意见/建议模板](?template=opinion.yml)。  
   - 我们会积极审阅并讨论可行性，但可能不会立刻实现，视项目计划而定。

3. **接口/配置/部署等问题**  
   - 本项目不针对接口配置、环境部署或第三方服务的参数设置等问题提供支持。  
   - 遇到此类问题，请阅读官方文档、社区讨论区或自行搜索相关信息。

---

## 二、在提交 Issues 之前

1. **搜索现有的 Issues**  
   - 避免重复提交相同问题。  
   - 如果发现类似问题可以补充你的信息或在对应 Issue 下评论。

2. **提供尽可能详细的信息**  
   - 提交问题时，尽量提供可复现的步骤、日志信息、环境说明等。  
   - 提交意见或建议时，需要清楚说明理由和期望。

3. **保持尊重与礼貌**  
   - 请尊重项目维护者和其他贡献者。  
   - 交流中请使用恰当、礼貌的语言。

---

## 三、Pull Request 提交指南

1. **先 Fork 再修改**  
   - 在你自己的 Fork 中进行修改和测试。  
   - 确保修改内容不会引入新的 Bug。

2. **遵守代码风格**  
   - 保持原有代码风格，遵循项目的 Lint 规则（如有）。  
   - 减少不必要的格式改动，保证可读性。

3. **更新文档或注释**  
   - 如果你的修改影响到了文档或注释，请及时补充或更新。

4. **描述清楚修改内容**  
   - Pull Request 标题与描述中需包含本次修改的目的、解决的问题以及修改的主要内容。

---

## 四、其他说明

- 我们对所有 Issue 和 Pull Request 均会尽量及时处理，但无法保证立即回复。  
- 对于不符合上述规则的 Issue 或 Pull Request，我们保留关闭或忽略的权利。

如果你对上述要求有任何疑问，欢迎在意见区进行讨论。再次感谢你的贡献！

---
</file>

<file path=".github/ISSUE_TEMPLATE/code_issue.yml">
name: "代码问题反馈"
description: "此模板仅用于反馈代码相关问题，例如出现编译错误、运行报错、逻辑缺陷。"
title: "[Code Issue]: "
labels: ["bug", "code issue"]
assignees: []

body:
  - type: markdown
    attributes:
      value: |
        **⚠ 注意：此处仅受理代码本身的问题，包括但不限于编译错误、运行报错、逻辑异常等。**  
        **如果是接口配置或环境部署等问题，请自行阅读文档或在讨论区寻求帮助。**  
        **如果是意见或建议，请使用 [意见模板](?template=opinion.yml)。**  
        感谢你的配合！

  - type: textarea
    id: description
    attributes:
      label: "问题描述"
      description: "请清晰、简要地描述代码出现的问题。"
      placeholder: "例如：运行时报错xxx，或逻辑存在xxx。"
    validations:
      required: true

  - type: textarea
    id: steps
    attributes:
      label: "复现步骤"
      description: "请提供完整的复现步骤，以便我们定位和解决问题。"
      placeholder: |
        1. ...
        2. ...
        3. ...
    validations:
      required: true

  - type: input
    id: environment
    attributes:
      label: "环境信息"
      description: "如编译器、操作系统、依赖版本等。"
      placeholder: "示例：Windows 10, Node.js v14, Python 3.9, etc."

  - type: textarea
    id: logs
    attributes:
      label: "日志信息（如适用）"
      description: "如果有报错日志或截图，可以贴在此处。"
      placeholder: "请粘贴日志内容或相关截图链接（可选）"
    validations:
      required: false

  - type: textarea
    id: additional
    attributes:
      label: "补充信息"
      description: "如果有更多信息，可在此补充。"
      placeholder: "任何与问题相关的额外背景说明..."
    validations:
      required: false
</file>

<file path=".github/ISSUE_TEMPLATE/opinion.yml">
name: "意见或建议"
description: "如果你有对项目的需求、功能建议、或其他意见，请使用此模板。"
title: "[Opinion]: "
labels: ["enhancement", "discussion"]
assignees: []

body:
  - type: markdown
    attributes:
      value: |
        **⚠ 注意：此处不用于反馈代码报错或编译问题，如果是纯代码报错或逻辑问题，请使用 [代码问题反馈模板](?template=code_issue.yml)。**  
        感谢你的宝贵意见或建议，我们会酌情采纳！

  - type: textarea
    id: suggestion
    attributes:
      label: "意见/建议内容"
      description: "请简要描述你的想法或建议。"
      placeholder: "例如：希望新增xx功能，或者修改xx逻辑。"
    validations:
      required: true

  - type: textarea
    id: reason
    attributes:
      label: "为什么需要这个功能或修改？"
      description: "简单说明你提出此意见/建议的原因或背景需求。"
      placeholder: "例如：在实际项目中遇到xx需求场景；希望提升xx效率；等等。"
    validations:
      required: true

  - type: input
    id: relevance
    attributes:
      label: "相关链接或参考"
      description: "如果你有看到类似实现或参考资料，可在此提供链接。"
      placeholder: "例如：相关文档链接、RFC、规范文档等"
    validations:
      required: false

  - type: textarea
    id: additional
    attributes:
      label: "补充信息"
      description: "如果有更多信息，可在此补充。"
      placeholder: "任何与意见或建议相关的额外说明..."
    validations:
      required: false
</file>

<file path="chapter_directory_parser.py">
# chapter_blueprint_parser.py
# -*- coding: utf-8 -*-
import re

def parse_chapter_blueprint(blueprint_text: str):
    """
    解析整份章节蓝图文本，返回一个列表，每个元素是一个 dict：
    {
      "chapter_number": int,
      "chapter_title": str,
      "chapter_role": str,       # 本章定位
      "chapter_purpose": str,    # 核心作用
      "suspense_level": str,     # 悬念密度
      "foreshadowing": str,      # 伏笔操作
      "plot_twist_level": str,   # 认知颠覆
      "chapter_summary": str     # 本章简述
    }
    """

    # 先按空行进行分块，以免多章之间混淆
    chunks = re.split(r'\n\s*\n', blueprint_text.strip())
    results = []

    # 兼容是否使用方括号包裹章节标题
    # 例如：
    #   第1章 - 紫极光下的预兆
    # 或
    #   第1章 - [紫极光下的预兆]
    chapter_number_pattern = re.compile(r'^第\s*(\d+)\s*章\s*-\s*\[?(.*?)\]?$')

    role_pattern     = re.compile(r'^本章定位：\s*\[?(.*)\]?$')
    purpose_pattern  = re.compile(r'^核心作用：\s*\[?(.*)\]?$')
    suspense_pattern = re.compile(r'^悬念密度：\s*\[?(.*)\]?$')
    foreshadow_pattern = re.compile(r'^伏笔操作：\s*\[?(.*)\]?$')
    twist_pattern       = re.compile(r'^认知颠覆：\s*\[?(.*)\]?$')
    summary_pattern = re.compile(r'^本章简述：\s*\[?(.*)\]?$')

    for chunk in chunks:
        lines = chunk.strip().splitlines()
        if not lines:
            continue

        chapter_number   = None
        chapter_title    = ""
        chapter_role     = ""
        chapter_purpose  = ""
        suspense_level   = ""
        foreshadowing    = ""
        plot_twist_level = ""
        chapter_summary  = ""

        # 先匹配第一行（或前几行），找到章号和标题
        header_match = chapter_number_pattern.match(lines[0].strip())
        if not header_match:
            # 不符合“第X章 - 标题”的格式，跳过
            continue

        chapter_number = int(header_match.group(1))
        chapter_title  = header_match.group(2).strip()

        # 从后面的行匹配其他字段
        for line in lines[1:]:
            line_stripped = line.strip()
            if not line_stripped:
                continue

            m_role = role_pattern.match(line_stripped)
            if m_role:
                chapter_role = m_role.group(1).strip()
                continue

            m_purpose = purpose_pattern.match(line_stripped)
            if m_purpose:
                chapter_purpose = m_purpose.group(1).strip()
                continue

            m_suspense = suspense_pattern.match(line_stripped)
            if m_suspense:
                suspense_level = m_suspense.group(1).strip()
                continue

            m_foreshadow = foreshadow_pattern.match(line_stripped)
            if m_foreshadow:
                foreshadowing = m_foreshadow.group(1).strip()
                continue

            m_twist = twist_pattern.match(line_stripped)
            if m_twist:
                plot_twist_level = m_twist.group(1).strip()
                continue

            m_summary = summary_pattern.match(line_stripped)
            if m_summary:
                chapter_summary = m_summary.group(1).strip()
                continue

        results.append({
            "chapter_number": chapter_number,
            "chapter_title": chapter_title,
            "chapter_role": chapter_role,
            "chapter_purpose": chapter_purpose,
            "suspense_level": suspense_level,
            "foreshadowing": foreshadowing,
            "plot_twist_level": plot_twist_level,
            "chapter_summary": chapter_summary
        })

    # 按照 chapter_number 排序后返回
    results.sort(key=lambda x: x["chapter_number"])
    return results


def get_chapter_info_from_blueprint(blueprint_text: str, target_chapter_number: int):
    """
    在已经加载好的章节蓝图文本中，找到对应章号的结构化信息，返回一个 dict。
    若找不到则返回一个默认的结构。
    """
    all_chapters = parse_chapter_blueprint(blueprint_text)
    for ch in all_chapters:
        if ch["chapter_number"] == target_chapter_number:
            return ch
    # 默认返回
    return {
        "chapter_number": target_chapter_number,
        "chapter_title": f"第{target_chapter_number}章",
        "chapter_role": "",
        "chapter_purpose": "",
        "suspense_level": "",
        "foreshadowing": "",
        "plot_twist_level": "",
        "chapter_summary": ""
    }
</file>

<file path="consistency_checker.py">
# consistency_checker.py
# -*- coding: utf-8 -*-
from llm_adapters import create_llm_adapter

# ============== 增加对“剧情要点/未解决冲突”进行检查的可选引导 ==============
CONSISTENCY_PROMPT = """\
请检查下面的小说设定与最新章节是否存在明显冲突或不一致之处，如有请列出：
- 小说设定：
{novel_setting}

- 角色状态（可能包含重要信息）：
{character_state}

- 前文摘要：
{global_summary}

- 已记录的未解决冲突或剧情要点：
{plot_arcs}  # 若为空可能不输出

- 最新章节内容：
{chapter_text}

如果存在冲突或不一致，请说明；如果在未解决冲突中有被忽略或需要推进的地方，也请提及；否则请返回“无明显冲突”。
"""

def check_consistency(
    novel_setting: str,
    character_state: str,
    global_summary: str,
    chapter_text: str,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float = 0.3,
    plot_arcs: str = "",
    interface_format: str = "OpenAI",
    max_tokens: int = 2048,
    timeout: int = 600
) -> str:
    """
    调用模型做简单的一致性检查。可扩展更多提示或校验规则。
    新增: 会额外检查对“未解决冲突或剧情要点”（plot_arcs）的衔接情况。
    """
    prompt = CONSISTENCY_PROMPT.format(
        novel_setting=novel_setting,
        character_state=character_state,
        global_summary=global_summary,
        plot_arcs=plot_arcs,
        chapter_text=chapter_text
    )

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )

    # 调试日志
    print("\n[ConsistencyChecker] Prompt >>>", prompt)

    response = llm_adapter.invoke(prompt)
    if not response:
        return "审校Agent无回复"
    
    # 调试日志
    print("[ConsistencyChecker] Response <<<", response)

    return response
</file>

<file path="install-pyenv-win.ps1">
<#
    .SYNOPSIS
    Installs pyenv-win

    .DESCRIPTION
    Installs pyenv-win to $HOME\.pyenv
    If pyenv-win is already installed, try to update to the latest version.

    .PARAMETER Uninstall
    Uninstall pyenv-win. Note that this uninstalls any Python versions that were installed with pyenv-win.

    .INPUTS
    None.

    .OUTPUTS
    None.

    .EXAMPLE
    PS> install-pyenv-win.ps1

    .LINK
    Online version: https://pyenv-win.github.io/pyenv-win/
#>
    
param (
    [Switch] $Uninstall = $False
)
    
$PyEnvDir = "${env:USERPROFILE}\.pyenv"
$PyEnvWinDir = "${PyEnvDir}\pyenv-win"
$BinPath = "${PyEnvWinDir}\bin"
$ShimsPath = "${PyEnvWinDir}\shims"
    
Function Remove-PyEnvVars() {
    $PathParts = [System.Environment]::GetEnvironmentVariable('PATH', "User") -Split ";"
    $NewPathParts = $PathParts.Where{ $_ -ne $BinPath }.Where{ $_ -ne $ShimsPath }
    $NewPath = $NewPathParts -Join ";"
    [System.Environment]::SetEnvironmentVariable('PATH', $NewPath, "User")

    [System.Environment]::SetEnvironmentVariable('PYENV', $null, "User")
    [System.Environment]::SetEnvironmentVariable('PYENV_ROOT', $null, "User")
    [System.Environment]::SetEnvironmentVariable('PYENV_HOME', $null, "User")
}

Function Remove-PyEnv() {
    Write-Host "Removing $PyEnvDir..."
    If (Test-Path $PyEnvDir) {
        Remove-Item -Path $PyEnvDir -Recurse
    }
    Write-Host "Removing environment variables..."
    Remove-PyEnvVars
}

Function Get-CurrentVersion() {
    $VersionFilePath = "$PyEnvDir\.version"
    If (Test-Path $VersionFilePath) {
        $CurrentVersion = Get-Content $VersionFilePath
    }
    Else {
        $CurrentVersion = ""
    }

    Return $CurrentVersion
}

Function Get-LatestVersion() {
    $LatestVersionFilePath = "$PyEnvDir\latest.version"
    (New-Object System.Net.WebClient).DownloadFile("https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/.version", $LatestVersionFilePath)
    $LatestVersion = Get-Content $LatestVersionFilePath

    Remove-Item -Path $LatestVersionFilePath

    Return $LatestVersion
}

Function Main() {
    If ($Uninstall) {
        Remove-PyEnv
        If ($? -eq $True) {
            Write-Host "pyenv-win successfully uninstalled."
        }
        Else {
            Write-Host "Uninstallation failed."
        }
        exit
    }

    $BackupDir = "${env:Temp}/pyenv-win-backup"
    
    $CurrentVersion = Get-CurrentVersion
    If ($CurrentVersion) {
        Write-Host "pyenv-win $CurrentVersion installed."
        $LatestVersion = Get-LatestVersion
        If ($CurrentVersion -eq $LatestVersion) {
            Write-Host "No updates available."
            exit
        }
        Else {
            Write-Host "New version available: $LatestVersion. Updating..."
            
            Write-Host "Backing up existing Python installations..."
            $FoldersToBackup = "install_cache", "versions", "shims"
            ForEach ($Dir in $FoldersToBackup) {
                If (-not (Test-Path $BackupDir)) {
                    New-Item -ItemType Directory -Path $BackupDir
                }
                Move-Item -Path "${PyEnvWinDir}/${Dir}" -Destination $BackupDir
            }
            
            Write-Host "Removing $PyEnvDir..."
            Remove-Item -Path $PyEnvDir -Recurse
        }   
    }

    New-Item -Path $PyEnvDir -ItemType Directory

    $DownloadPath = "$PyEnvDir\pyenv-win.zip"

    (New-Object System.Net.WebClient).DownloadFile("https://github.com/pyenv-win/pyenv-win/archive/master.zip", $DownloadPath)

    Start-Process -FilePath "powershell.exe" -ArgumentList @(
        "-NoProfile",
        "-Command `"Microsoft.PowerShell.Archive\Expand-Archive -Path \`"$DownloadPath\`" -DestinationPath \`"$PyEnvDir\`"`""
    ) -NoNewWindow -Wait

    Move-Item -Path "$PyEnvDir\pyenv-win-master\*" -Destination "$PyEnvDir"
    Remove-Item -Path "$PyEnvDir\pyenv-win-master" -Recurse
    Remove-Item -Path $DownloadPath

    # Update env vars
    [System.Environment]::SetEnvironmentVariable('PYENV', "${PyEnvWinDir}\", "User")
    [System.Environment]::SetEnvironmentVariable('PYENV_ROOT', "${PyEnvWinDir}\", "User")
    [System.Environment]::SetEnvironmentVariable('PYENV_HOME', "${PyEnvWinDir}\", "User")

    $PathParts = [System.Environment]::GetEnvironmentVariable('PATH', "User") -Split ";"

    # Remove existing paths, so we don't add duplicates
    $NewPathParts = $PathParts.Where{ $_ -ne $BinPath }.Where{ $_ -ne $ShimsPath }
    $NewPathParts = ($BinPath, $ShimsPath) + $NewPathParts
    $NewPath = $NewPathParts -Join ";"
    [System.Environment]::SetEnvironmentVariable('PATH', $NewPath, "User")

    If (Test-Path $BackupDir) {
        Write-Host "Restoring Python installations..."
        Move-Item -Path "$BackupDir/*" -Destination $PyEnvWinDir
    }
    
    If ($? -eq $True) {
        Write-Host "pyenv-win is successfully installed. You may need to close and reopen your terminal before using it."
    }
    Else {
        Write-Host "pyenv-win was not installed successfully. If this issue persists, please open a ticket: https://github.com/pyenv-win/pyenv-win/issues."
    }
}

Main
</file>

<file path="LICENSE">
GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.
</file>

<file path="main.py">
# main.py
# -*- coding: utf-8 -*-
import customtkinter as ctk
from ui import NovelGeneratorGUI

def main():
    app = ctk.CTk()
    gui = NovelGeneratorGUI(app)
    app.mainloop()

if __name__ == "__main__":
    main()
</file>

<file path="novel_generator/qa.py">
# novel_generator/qa.py
# -*- coding: utf-8 -*-
import logging
import traceback
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter
from novel_generator.common import invoke_with_cleaning
from novel_generator.vectorstore_utils import load_vector_store

# 问答专用提示词
QA_PROMPT_TEMPLATE = """\
你是一名熟悉这部小说的助手。请严格根据下方的【相关原文片段】来回答用户的提问。

【相关原文片段】：
{context}

【用户提问】：
{question}

【回答要求】：
1. 答案必须基于原文片段。如果原文片段中没有相关信息，请直接回答“根据现有章节内容，暂时无法找到相关信息”。
2. 语言通顺，逻辑清晰。
3. 不要编造原文中不存在的情节。

请回答：
"""

def answer_novel_question(
    filepath: str,
    question: str,
    # LLM 配置
    llm_api_key: str, llm_base_url: str, llm_model_name: str, interface_format: str,
    # Embedding 配置 (用于检索)
    emb_api_key: str, emb_base_url: str, emb_model_name: str, emb_interface_format: str,
    top_k: int = 5
) -> str:
    """
    全书问答核心逻辑
    """
    # 1. 加载向量库
    try:
        embedding_adapter = create_embedding_adapter(
            emb_interface_format, emb_api_key, emb_base_url, emb_model_name
        )
        
        # === 【核心修复】参数顺序对调：先传 Adapter，再传路径 ===
        vector_store = load_vector_store(embedding_adapter, filepath)
        
        if not vector_store:
            return "错误：无法加载向量库。请确保您已经对至少一个章节进行了【定稿 (Finalize)】操作，且向量库文件存在。"
            
    except Exception as e:
        logging.error(f"加载向量库失败: {traceback.format_exc()}")
        return f"加载知识库失败: {str(e)}"

    # 2. 检索相关内容 (Search)
    try:
        # 搜索最相关的 K 个片段
        docs = vector_store.similarity_search(question, k=top_k)
        if not docs:
            return "未在知识库中检索到相关内容。"
            
        # 拼接上下文
        context_text = "\n\n".join([f"---片段---\n{d.page_content}" for d in docs])
        
    except Exception as e:
        logging.error(f"检索失败: {traceback.format_exc()}")
        return f"检索失败: {str(e)}"

    # 3. 调用大模型回答 (Generation)
    try:
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=llm_base_url,
            model_name=llm_model_name,
            api_key=llm_api_key,
            temperature=0.3, # 问答需要准确，温度调低
            max_tokens=2048,
            timeout=600
        )
        
        prompt = QA_PROMPT_TEMPLATE.format(
            context=context_text,
            question=question
        )
        
        response = invoke_with_cleaning(llm_adapter, prompt)
        return response

    except Exception as e:
        logging.error(f"生成答案失败: {traceback.format_exc()}")
        return f"生成答案失败: {str(e)}"
</file>

<file path="READMEEN.md">
# 📖 Automatic Novel Generation Tool

>- Currently I don't have much energy to maintain this project. The project brings no revenue, and with graduation approaching I have many other priorities. If time permits in the future I may consider a refactor using newer technologies. — 2025/09/24

<div align="center">
  
✨ **Core Features** ✨

| Module                | Key Capabilities                        |
|-----------------------|-----------------------------------------|
| 🎨 Novel Setting Workshop | Worldbuilding / Character Design / Plot Blueprint |
| 📖 Intelligent Chapter Generation | Multi-stage generation to ensure plot coherence |
| 🧠 State Tracking System | Character development trajectory / Foreshadowing management |
| 🔍 Semantic Search Engine | Vector-based long-term context consistency |
| 📚 Knowledge Base Integration | Supports local document references |
| ✅ Automatic Proofreading | Detects plot contradictions and logical conflicts |
| 🖥 Visual Workbench | Full-process GUI for configuration / generation / proofreading |

</div>

> A multifunctional novel generator built on large language models. Helps you efficiently create long-form stories with consistent settings and rigorous logic.

---

## 📑 Table of Contents
1. [Environment Preparation](#-environment-preparation)  
2. [Project Structure](#-project-structure)  
3. [Configuration Guide](#⚙️-configuration-guide)  
4. [Run Instructions](#🚀-run-instructions)  
5. [User Guide](#📘-user-guide)  
6. [FAQ](#❓-faq)  

---

## 🛠 Environment Preparation
Ensure the environment meets the following requirements:
- **Python 3.9+** (recommended 3.10–3.12)
- **pip** package manager
- Valid API keys:
   - Cloud services: OpenAI / DeepSeek, etc.
   - Local services: Ollama or other OpenAI-compatible interfaces

---

## 📥 Installation
1. **Download the project**  
    - Download the project ZIP from [GitHub](https://github.com) or clone the repository:
       ```bash
       git clone https://github.com/YILING0013/AI_NovelGenerator
       ```


2. **Install build tools (optional)**  
    - If some packages fail to install, visit [Visual Studio Build Tools](https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/) to download and install C++ build tools required by some modules.
    - By default the installer includes MSBuild only; make sure to select **C++ Desktop Development** from the workload list.

3. **Install dependencies and run**  
    - Open a terminal and change to the project directory:
       ```bash
       cd AI_NovelGenerator
       ```
    - Install project dependencies:
       ```bash
       pip install -r requirements.txt
       ```
    - After installation run the main program:
       ```bash
       python main.py
       ```

If some dependencies are still missing, manually run:
```bash
pip install <package-name>
```
to install them.


## 🗂 Project Structure
```
novel-generator/
├── main.py                      # Entry file, runs the GUI
├── consistency_checker.py       # Consistency checks to prevent plot conflicts
|—— chapter_directory_parser.py  # Directory parsing
|—— embedding_adapters.py        # Embedding interface wrappers
|—— llm_adapters.py              # LLM interface wrappers
├── prompt_definitions.py        # AI prompt templates
├── utils.py                     # Utility functions and file operations
├── config_manager.py            # Configuration manager (API keys, base URL)
├── config.json                  # User configuration (optional)
├── novel_generator/             # Core chapter generation logic
├── ui/                          # Graphical user interface
└── vectorstore/                 # (Optional) Local vector DB storage
```

---

## ⚙️ Configuration Guide
### 📌 Basic configuration (`config.json`)
```json
{
   "api_key": "sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
   "base_url": "https://api.openai.com/v1",
   "interface_format": "OpenAI",
   "model_name": "gpt-4o-mini",
   "temperature": 0.7,
   "max_tokens": 4096,
   "embedding_api_key": "sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
   "embedding_interface_format": "OpenAI",
   "embedding_url": "https://api.openai.com/v1",
   "embedding_model_name": "text-embedding-ada-002",
   "embedding_retrieval_k": 4,
   "topic": "The protagonist of Star Rail travels to Genshin Impact's Teyvat continent, saves it, and develops complex relationships with its characters.",
   "genre": "Fantasy",
   "num_chapters": 120,
   "word_number": 4000,
   "filepath": "D:/AI_NovelGenerator/filepath"
}
```

### 🔧 Explanation
1. **Generation model configuration**
   - `api_key`: API key for the LLM service
   - `base_url`: API endpoint (for local services use the Ollama address)
   - `interface_format`: Interface mode
   - `model_name`: Main generation model (e.g., gpt-4, claude-3)
   - `temperature`: Creativity parameter (0–1, higher is more creative)
   - `max_tokens`: Maximum model response length

2. **Embedding model configuration**
   - `embedding_model_name`: Embedding model name (e.g., Ollama's nomic-embed-text)
   - `embedding_url`: Service endpoint
   - `embedding_retrieval_k`: Number of nearest neighbors to retrieve

3. **Novel parameters**
   - `topic`: Core story theme
   - `genre`: Genre
   - `num_chapters`: Total number of chapters
   - `word_number`: Target words per chapter
   - `filepath`: Path to save generated files

---

## 🚀 Run Instructions
### Method 1 — Run with Python
```bash
python main.py
```
This launches the GUI for interactive use.

### Method 2 — Build an executable
If you want to run the tool on machines without Python, package it with **PyInstaller**:
```bash
pip install pyinstaller
pyinstaller main.spec
```
After packaging an executable (e.g., `main.exe` on Windows) will appear in the `dist/` folder.

---

## 📘 User Guide
1. **After launching the app, fill in the basic parameters:**  
   - **API Key & Base URL** (e.g., `https://api.openai.com/v1`)  
   - **Model name** (e.g., `gpt-3.5-turbo`, `gpt-4o`)  
   - **Temperature** (0–1, controls creative variance)  
   - **Topic** (e.g., "AI uprising in a post-apocalyptic world")  
   - **Genre** (e.g., "Sci-fi" / "Fantasy" / "Urban Fantasy")  
   - **Number of chapters** and **words per chapter** (e.g., 10 chapters × ~3000 words)  
   - **Save path** (create a new output folder for results)

2. **Click "Step1. Generate Settings"**  
   - The system will generate, based on topic/genre/chapter count:  
     - `Novel_setting.txt`: Worldbuilding, characters, trigger points and foreshadowing.  
   - You can view or edit these settings after generation.

3. **Click "Step2. Generate Directory"**  
   - The system will use `Novel_setting.txt` to produce:  
     - `Novel_directory.txt`: Chapter titles and short prompts.  
   - You can review and modify chapter titles and descriptions.

4. **Click "Step3. Generate Chapter Draft"**  
   - Before generating a chapter you can:  
     - Set the chapter number (e.g., `1`)  
     - Provide chapter-specific guidance in the "This chapter guidance" box  
   - When you generate a chapter the system will:  
     - Read prior settings, `Novel_directory.txt`, and finalized chapters  
     - Use vector retrieval to recall relevant context for coherence  
     - Produce an outline (`outline_X.txt`) and chapter text (`chapter_X.txt`)  
   - You can view and edit the draft in the editor pane.

5. **Click "Step4. Finalize Current Chapter"**  
   - The system will:  
     - Update the global summary (`global_summary.txt`)  
     - Update character states (`character_state.txt`)  
     - Update the vector store (so future chapters can use the latest info)  
     - Update major plot points (e.g., `plot_arcs.txt`)  
   - After finalizing you will see the finalized text in `chapter_X.txt`.

6. **Consistency check (optional)**  
   - Click the "[Optional] Consistency Proofread" button to scan the latest chapter for conflicts (character logic, plot contradictions, etc.).  
   - If conflicts are detected, detailed messages will appear in the log area.

7. **Repeat steps 4–6** until all chapters are generated and finalized.

> Vector retrieval tips:
> 1. Explicitly set the embedding interface and model name.
> 2. For local Ollama embeddings start the Ollama service first:
>    ```bash
>    ollama serve  # Start the service
>    ollama pull nomic-embed-text  # Download/enable the model
>    ```
> 3. Clear the `vectorstore` directory after switching embedding models.
> 4. For cloud embeddings ensure the API permissions are enabled.

---

## ❓ FAQ
### Q1: Expecting value: line 1 column 1 (char 0)

This error usually indicates the API did not return valid JSON—sometimes an HTML error page or other unexpected content was returned.

### Q2: HTTP/1.1 504 Gateway Timeout?

Check the stability of the API endpoint and network connectivity.

### Q3: How do I switch Embedding providers?

Enter the new provider settings in the GUI fields for embedding configuration.

---

If you have further questions or feature requests, please open an issue on the project repository.
</file>

<file path="ui/__init__.py">
# ui/__init__.py
from .main_window import NovelGeneratorGUI
</file>

<file path="ui/context_menu.py">
# ui/context_menu.py
# -*- coding: utf-8 -*-
import tkinter as tk
import customtkinter as ctk

class TextWidgetContextMenu:
    """
    为 customtkinter.TextBox 或 tkinter.Text 提供右键复制/剪切/粘贴/全选的功能。
    """
    def __init__(self, widget):
        self.widget = widget
        self.menu = tk.Menu(widget, tearoff=0)
        self.menu.add_command(label="复制", command=self.copy)
        self.menu.add_command(label="粘贴", command=self.paste)
        self.menu.add_command(label="剪切", command=self.cut)
        self.menu.add_separator()
        self.menu.add_command(label="全选", command=self.select_all)
        
        # 绑定右键事件
        self.widget.bind("<Button-3>", self.show_menu)
        
    def show_menu(self, event):
        if isinstance(self.widget, ctk.CTkTextbox):
            try:
                self.menu.tk_popup(event.x_root, event.y_root)
            finally:
                self.menu.grab_release()
            
    def copy(self):
        try:
            text = self.widget.get("sel.first", "sel.last")
            self.widget.clipboard_clear()
            self.widget.clipboard_append(text)
        except tk.TclError:
            pass  # 没有选中文本时忽略错误

    def paste(self):
        try:
            text = self.widget.clipboard_get()
            self.widget.insert("insert", text)
        except tk.TclError:
            pass  # 剪贴板为空时忽略错误

    def cut(self):
        try:
            text = self.widget.get("sel.first", "sel.last")
            self.widget.delete("sel.first", "sel.last")
            self.widget.clipboard_clear()
            self.widget.clipboard_append(text)
        except tk.TclError:
            pass  # 没有选中文本时忽略错误

    def select_all(self):
        self.widget.tag_add("sel", "1.0", "end")
</file>

<file path="ui/helpers.py">
# ui/helpers.py
# -*- coding: utf-8 -*-
import logging
import traceback

def log_error(message: str):
    logging.error(f"{message}\n{traceback.format_exc()}")
</file>

<file path="ui/other_settings.py">
# ui/other_settings.py
import customtkinter as ctk
from ui.config_tab import create_label_with_help
from tkinter import messagebox
from config_manager import load_config, save_config
import requests
from requests.auth import HTTPBasicAuth
import os
from xml.etree import ElementTree as ET
import shutil
import time
def build_other_settings_tab(self):
    self.other_settings_tab = self.tabview.add("Other Settings")
    self.other_settings_tab.rowconfigure(0, weight=1)
    self.other_settings_tab.columnconfigure(0, weight=1)
    if "webdav_config" not in self.loaded_config:
        self.loaded_config["webdav_config"] = {
            "webdav_url": "",
            "webdav_username": "",
            "webdav_password": ""
        }

    self.webdav_url_var.set(self.loaded_config["webdav_config"].get("webdav_url", ""))
    self.webdav_username_var.set(self.loaded_config["webdav_config"].get("webdav_username", ""))
    self.webdav_password_var.set(self.loaded_config["webdav_config"].get("webdav_password", ""))


    def save_webdav_settings():
        self.loaded_config["webdav_config"]["webdav_url"] = self.webdav_url_var.get().strip()
        self.loaded_config["webdav_config"]["webdav_username"] = self.webdav_username_var.get().strip()
        self.loaded_config["webdav_config"]["webdav_password"] = self.webdav_password_var.get().strip()
        save_config(self.loaded_config, self.config_file)


    def test_webdav_connection(test = True):
        try:
            client = WebDAVClient(self.webdav_url_var.get().strip(),self.webdav_username_var.get().strip(),self.webdav_password_var.get().strip())
            client.list_directory()
            if not test:
                save_webdav_settings()
                return True
            messagebox.showinfo("成功", "WebDAV 连接成功！")
            save_webdav_settings()
            return True

        except Exception as e:
            print(e)

            messagebox.showerror("错误", f"发生未知错误: {e}")
            return False

    def backup_to_webdav():
        try:
            target_dir = "AI_Novel_Generator"
            client = WebDAVClient(self.webdav_url_var.get().strip(),self.webdav_username_var.get().strip(),self.webdav_password_var.get().strip())
            if not client.ensure_directory_exists(target_dir):
                client.create_directory(target_dir)
            client.upload_file(self.config_file, f"{target_dir}/config.json")
            messagebox.showinfo("成功", "配置备份成功！")
        except Exception as e:
            print(e)
            messagebox.showerror("错误", f"发生未知错误: {e}")
            return False







    def restore_from_webdav():
        try:
            target_dir = "AI_Novel_Generator"
            client = WebDAVClient(self.webdav_url_var.get().strip(),self.webdav_username_var.get().strip(),self.webdav_password_var.get().strip())
            client.download_file(f"{target_dir}/config.json", self.config_file)
            self.loaded_config = load_config(self.config_file)
            messagebox.showinfo("成功", "配置恢复成功！")

        except Exception as e:
            print(e)
            messagebox.showerror("错误", f"发生未知错误: {e}")
            return False




    dav_frame = ctk.CTkFrame(self.other_settings_tab)
    dav_frame.pack(padx=20, pady=20, fill="x")

    dav_title = ctk.CTkLabel(dav_frame, text="webdav设置", font=("Microsoft YaHei", 16, "bold"))
    dav_title.pack(anchor="w", padx=5, pady=(0, 5))
    dav_warp_frame = ctk.CTkFrame(dav_frame, corner_radius=10, border_width=2, border_color="gray")
    dav_warp_frame.pack(fill="x", padx=5)
    dav_warp_frame.columnconfigure(1, weight=1)

    

    create_label_with_help(self, parent=dav_warp_frame, label_text="Webdav URL", tooltip_key="webdav_url",row=0, column=0, font=("Microsoft YaHei", 12), sticky="w")
    dav_url_entry = ctk.CTkEntry(dav_warp_frame, textvariable=self.webdav_url_var, font=("Microsoft YaHei", 12))
    dav_url_entry.grid(row=0, column=1, padx=5, pady=5, sticky="w")

    create_label_with_help(self, parent=dav_warp_frame, label_text="Webdav用户名", tooltip_key="webdav_username",row=1, column=0, font=("Microsoft YaHei", 12), sticky="w")
    dav_username_entry = ctk.CTkEntry(dav_warp_frame, textvariable=self.webdav_username_var, font=("Microsoft YaHei", 12))
    dav_username_entry.grid(row=1, column=1, padx=5, pady=5, sticky="w")

    create_label_with_help(self, parent=dav_warp_frame, label_text="Webdav密码", tooltip_key="webdav_password",row=2, column=0, font=("Microsoft YaHei", 12), sticky="w")
    dav_password_entry = ctk.CTkEntry(dav_warp_frame, textvariable=self.webdav_password_var, font=("Microsoft YaHei", 12), show="*")
    dav_password_entry.grid(row=2, column=1, padx=5, pady=5, sticky="w")

    button_frame = ctk.CTkFrame(dav_warp_frame)
    button_frame.grid(row=3, column=0, columnspan=2, padx=5, pady=10, sticky="w")
    
    # 测试连接按钮
    test_btn = ctk.CTkButton(button_frame, text="测试连接", font=("Microsoft YaHei", 12),
                            command=test_webdav_connection)
    test_btn.pack(side="left", padx=5)
    
    # 保存设置按钮
    save_btn = ctk.CTkButton(button_frame, text="备份", font=("Microsoft YaHei", 12),
                            command=backup_to_webdav)
    save_btn.pack(side="left", padx=5)
    
    # 重置按钮
    reset_btn = ctk.CTkButton(button_frame, text="恢复", font=("Microsoft YaHei", 12),
                             command=restore_from_webdav)
    reset_btn.pack(side="left", padx=5)







class WebDAVClient:
    def __init__(self, base_url, username, password):
        """初始化WebDAV客户端"""
        self.base_url = base_url.rstrip('/') + '/'
        self.auth = HTTPBasicAuth(username, password)
        self.headers = {
            'User-Agent': 'Python WebDAV Client',
            'Accept': '*/*'
        }
        # WebDAV命名空间
        self.ns = {'d': 'DAV:'}

    def _get_url(self, path):
        """获取完整的资源URL"""
        return self.base_url + path.lstrip('/')

    def directory_exists(self, path):
        """
        检查目录是否存在
        :param path: 目录路径
        :return: 布尔值，表示目录是否存在
        """
        url = self._get_url(path)
        headers = self.headers.copy()
        headers['Depth'] = '0'  # 只检查当前资源
        
        try:
            # 发送PROPFIND请求检查资源是否存在
            response = requests.request('PROPFIND', url, headers=headers, auth=self.auth)
            
            # 207 Multi-Status表示成功，说明资源存在
            if response.status_code == 207:
                # 解析XML响应，确认是目录
                root = ET.fromstring(response.content)
                # 查找资源类型属性
                res_type = root.find('.//d:resourcetype', namespaces=self.ns)
                # 如果包含collection元素，则是目录
                if res_type is not None and res_type.find('d:collection', namespaces=self.ns) is not None:
                    return True
            return False
        except requests.exceptions.RequestException as e:
            print(f"检查目录存在性时出错: {e}")
            return False

    def create_directory(self, path):
        """
        创建远程目录
        :param path: 要创建的目录路径
        :return: 是否创建成功
        """
        url = self._get_url(path)
        
        try:
            response = requests.request('MKCOL', url, auth=self.auth, headers=self.headers)
            response.raise_for_status()
            
            print(f"目录创建成功: {path}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"目录创建失败: {e}")
            return False

    def ensure_directory_exists(self, path):
        """
        确保目录存在，如果不存在则创建
        :param path: 目录路径
        :return: 布尔值，表示最终目录是否存在
        """
        # 移除末尾的斜杠（如果有）
        path = path.rstrip('/')
        
        # 如果目录已经存在，直接返回True
        if self.directory_exists(path):
            print(f"目录已存在: {path}")
            return True
            
        # 递归创建父目录
        parent_dir = os.path.dirname(path)
        if parent_dir and not self.directory_exists(parent_dir):
            # 如果父目录不存在，则先创建父目录
            if not self.ensure_directory_exists(parent_dir):
                print(f"创建父目录失败: {parent_dir}")
                return False
                
        # 创建当前目录
        return self.create_directory(path)
    def upload_file(self, local_path, remote_path):
        """
        上传文件到WebDAV服务器
        :param local_path: 本地文件路径
        :param remote_path: 远程文件路径
        :return: 是否上传成功
        """
        if not os.path.isfile(local_path):
            print(f"本地文件不存在: {local_path}")
            return False

        url = self._get_url(remote_path)
        
        try:
            with open(local_path, 'rb') as f:
                response = requests.put(url, data=f, auth=self.auth, headers=self.headers)
                response.raise_for_status()
            
            print(f"文件上传成功: {local_path} -> {remote_path}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"文件上传失败: {e}")
            return False
    def download_file(self, remote_path, local_path):
        """
        从WebDAV服务器下载文件
        :param remote_path: 远程文件路径
        :param local_path: 本地保存路径
        :return: 是否下载成功
        """
        url = self._get_url(remote_path)
        local_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), local_path)
        self.backup(local_path)
        try:
            response = requests.get(url, auth=self.auth, headers=self.headers, stream=True)
            response.raise_for_status()
            
            # 创建本地目录（如果需要）
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"文件下载成功: {remote_path} -> {local_path}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"文件下载失败: {e}")
            return False
    def backup(self, local_path):
        name_parts = os.path.basename(local_path).rsplit('.', 1)  # 只分割最后一个点
        base_name = name_parts[0]
        extension = name_parts[1]
        timestamp = time.strftime("%Y%m%d%H%M%S")
        if not os.path.exists(os.path.join(os.path.dirname(local_path), "backup")):
            os.makedirs(os.path.join(os.path.dirname(local_path), "backup"))
        backup_file_name = f"{base_name}_{timestamp}_bak.{extension}"
        shutil.copy2(os.path.basename(local_path), os.path.join(os.path.dirname(local_path), "backup", backup_file_name))
</file>

<file path="config.example.json">
{
    "last_interface_format": "OpenAI",
    "last_embedding_interface_format": "OpenAI",
    "llm_configs": {
        "DeepSeek V3": {
            "api_key": "",
            "base_url": "https://api.deepseek.com/v1",
            "model_name": "deepseek-chat",
            "temperature": 0.7,
            "max_tokens": 8192,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "GPT 5": {
            "api_key": "",
            "base_url": "https://api.openai.com/v1",
            "model_name": "gpt-5",
            "temperature": 0.7,
            "max_tokens": 32768,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "Gemini 2.5 Pro": {
            "api_key": "",
            "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
            "model_name": "gemini-2.5-pro",
            "temperature": 0.7,
            "max_tokens": 32768,
            "timeout": 600,
            "interface_format": "OpenAI"
        }
    },
    "embedding_configs": {
        "OpenAI": {
            "api_key": "",
            "base_url": "https://api.openai.com/v1",
            "model_name": "text-embedding-ada-002",
            "retrieval_k": 4,
            "interface_format": "OpenAI"
        }
    },
    "other_params": {
        "topic": "",
        "genre": "",
        "num_chapters": 0,
        "word_number": 0,
        "filepath": "",
        "chapter_num": "120",
        "user_guidance": "",
        "characters_involved": "",
        "key_items": "",
        "scene_location": "",
        "time_constraint": ""
    },
    "choose_configs": {
        "prompt_draft_llm": "DeepSeek V3",
        "chapter_outline_llm": "DeepSeek V3",
        "architecture_llm": "Gemini 2.5 Pro",
        "final_chapter_llm": "GPT 5",
        "consistency_review_llm": "DeepSeek V3"
    },
    "proxy_setting": {
        "proxy_url": "127.0.0.1",
        "proxy_port": "",
        "enabled": false
    },
    "webdav_config": {
        "webdav_url": "",
        "webdav_username": "",
        "webdav_password": ""
    }
}
</file>

<file path="novel_generator/__init__.py">
#novel_generator/__init__.py
from .architecture import Novel_architecture_generate
from .blueprint import Chapter_blueprint_generate
from .chapter import (
    get_last_n_chapters_text,
    summarize_recent_chapters,
    get_filtered_knowledge_context,
    build_chapter_prompt,
    generate_chapter_draft,
    analyze_chapter_logic,
    rewrite_chapter_with_feedback,
    refine_chapter_detail,
)
from .finalization import finalize_chapter, enrich_chapter_text
from .knowledge import import_knowledge_file
from .vectorstore_utils import clear_vector_store
from .qa import answer_novel_question
</file>

<file path="novel_generator/knowledge.py">
#novel_generator/knowledge.py
# -*- coding: utf-8 -*-
"""
知识文件导入至向量库（advanced_split_content、import_knowledge_file）
"""
import os
import logging
import re
import traceback
import nltk
import warnings
from utils import read_file
from novel_generator.vectorstore_utils import load_vector_store, init_vector_store
from langchain.docstore.document import Document

# 禁用特定的Torch警告
warnings.filterwarnings('ignore', message='.*Torch was not compiled with flash attention.*')
os.environ["TOKENIZERS_PARALLELISM"] = "false"
logging.basicConfig(
    filename='app.log',      # 日志文件名
    filemode='a',            # 追加模式（'w' 会覆盖）
    level=logging.INFO,      # 记录 INFO 及以上级别的日志
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
def advanced_split_content(content: str, similarity_threshold: float = 0.7, max_length: int = 500) -> list:
    """使用基本分段策略"""
    # nltk.download('punkt', quiet=True)
    # nltk.download('punkt_tab', quiet=True)
    sentences = nltk.sent_tokenize(content)
    if not sentences:
        return []

    final_segments = []
    current_segment = []
    current_length = 0
    
    for sentence in sentences:
        sentence_length = len(sentence)
        if current_length + sentence_length > max_length:
            if current_segment:
                final_segments.append(" ".join(current_segment))
            current_segment = [sentence]
            current_length = sentence_length
        else:
            current_segment.append(sentence)
            current_length += sentence_length
    
    if current_segment:
        final_segments.append(" ".join(current_segment))
    
    return final_segments

def import_knowledge_file(
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    file_path: str,
    filepath: str
):
    logging.info(f"开始导入知识库文件: {file_path}, 接口格式: {embedding_interface_format}, 模型: {embedding_model_name}")
    if not os.path.exists(file_path):
        logging.warning(f"知识库文件不存在: {file_path}")
        return
    content = read_file(file_path)
    if not content.strip():
        logging.warning("知识库文件内容为空。")
        return
    paragraphs = advanced_split_content(content)
    from embedding_adapters import create_embedding_adapter
    embedding_adapter = create_embedding_adapter(
        embedding_interface_format,
        embedding_api_key,
        embedding_url if embedding_url else "http://localhost:11434/api",
        embedding_model_name
    )
    store = load_vector_store(embedding_adapter, filepath)
    if not store:
        logging.info("Vector store does not exist or load failed. Initializing a new one for knowledge import...")
        store = init_vector_store(embedding_adapter, paragraphs, filepath)
        if store:
            logging.info("知识库文件已成功导入至向量库(新初始化)。")
        else:
            logging.warning("知识库导入失败，跳过。")
    else:
        try:
            docs = [Document(page_content=str(p)) for p in paragraphs]
            store.add_documents(docs)
            logging.info("知识库文件已成功导入至向量库(追加模式)。")
        except Exception as e:
            logging.warning(f"知识库导入失败: {e}")
            traceback.print_exc()
</file>

<file path="ui/character_tab.py">
# ui/character_tab.py
# -*- coding: utf-8 -*-
import os
import customtkinter as ctk
from tkinter import messagebox
from utils import read_file, save_string_to_txt, clear_file_content
from ui.context_menu import TextWidgetContextMenu

def build_character_tab(self):
    self.character_tab = self.tabview.add("Character State")
    self.character_tab.rowconfigure(0, weight=0)
    self.character_tab.rowconfigure(1, weight=1)
    self.character_tab.columnconfigure(0, weight=1)

    load_btn = ctk.CTkButton(self.character_tab, text="加载 character_state.txt", command=self.load_character_state, font=("Microsoft YaHei", 12))
    load_btn.grid(row=0, column=0, padx=5, pady=5, sticky="w")

    self.character_wordcount_label = ctk.CTkLabel(self.character_tab, text="字数：0", font=("Microsoft YaHei", 12))
    self.character_wordcount_label.grid(row=0, column=1, padx=5, pady=5, sticky="w")

    save_btn = ctk.CTkButton(self.character_tab, text="保存修改", command=self.save_character_state, font=("Microsoft YaHei", 12))
    save_btn.grid(row=0, column=2, padx=5, pady=5, sticky="e")

    self.character_text = ctk.CTkTextbox(self.character_tab, wrap="word", font=("Microsoft YaHei", 12))
    
    def update_word_count(event=None):
        text = self.character_text.get("0.0", "end-1c")
        text_length = len(text)
        self.character_wordcount_label.configure(text=f"字数：{text_length}")
    
    self.character_text.bind("<KeyRelease>", update_word_count)
    self.character_text.bind("<ButtonRelease>", update_word_count)
    TextWidgetContextMenu(self.character_text)
    self.character_text.grid(row=1, column=0, sticky="nsew", padx=5, pady=5, columnspan=3)

def load_character_state(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    filename = os.path.join(filepath, "character_state.txt")
    content = read_file(filename)
    self.character_text.delete("0.0", "end")
    self.character_text.insert("0.0", content)
    self.log("已加载 character_state.txt 到编辑区。")

def save_character_state(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    content = self.character_text.get("0.0", "end").strip()
    filename = os.path.join(filepath, "character_state.txt")
    clear_file_content(filename)
    save_string_to_txt(content, filename)
    self.log("已保存对 character_state.txt 的修改。")
</file>

<file path="ui/setting_tab.py">
# ui/setting_tab.py
# -*- coding: utf-8 -*-
import os
import customtkinter as ctk
from tkinter import messagebox
from utils import read_file, save_string_to_txt, clear_file_content
from ui.context_menu import TextWidgetContextMenu

def build_setting_tab(self):
    self.setting_tab = self.tabview.add("Novel Architecture")
    self.setting_tab.rowconfigure(0, weight=0)
    self.setting_tab.rowconfigure(1, weight=1)
    self.setting_tab.columnconfigure(0, weight=1)

    load_btn = ctk.CTkButton(self.setting_tab, text="加载 Novel_architecture.txt", command=self.load_novel_architecture, font=("Microsoft YaHei", 12))
    load_btn.grid(row=0, column=0, padx=5, pady=5, sticky="w")

    self.setting_word_count_label = ctk.CTkLabel(self.setting_tab, text="字数：0", font=("Microsoft YaHei", 12))
    self.setting_word_count_label.grid(row=0, column=1, padx=5, pady=5, sticky="w")

    save_btn = ctk.CTkButton(self.setting_tab, text="保存修改", command=self.save_novel_architecture, font=("Microsoft YaHei", 12))
    save_btn.grid(row=0, column=2, padx=5, pady=5, sticky="e")

    self.setting_text = ctk.CTkTextbox(self.setting_tab, wrap="word", font=("Microsoft YaHei", 12))
    TextWidgetContextMenu(self.setting_text)
    self.setting_text.grid(row=1, column=0, sticky="nsew", padx=5, pady=5, columnspan=3)

    def update_word_count(event=None):
        text = self.setting_text.get("0.0", "end")
        count = len(text) - 1
        self.setting_word_count_label.configure(text=f"字数：{count}")

    self.setting_text.bind("<KeyRelease>", update_word_count)
    self.setting_text.bind("<ButtonRelease>", update_word_count)

def load_novel_architecture(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    filename = os.path.join(filepath, "Novel_architecture.txt")
    content = read_file(filename)
    self.setting_text.delete("0.0", "end")
    self.setting_text.insert("0.0", content)
    self.log("已加载 Novel_architecture.txt 内容到编辑区。")

def save_novel_architecture(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径。")
        return
    content = self.setting_text.get("0.0", "end").strip()
    filename = os.path.join(filepath, "Novel_architecture.txt")
    clear_file_content(filename)
    save_string_to_txt(content, filename)
    self.log("已保存对 Novel_architecture.txt 的修改。")
</file>

<file path="ui/summary_tab.py">
# ui/summary_tab.py
# -*- coding: utf-8 -*-
import os
import customtkinter as ctk
from tkinter import messagebox
from utils import read_file, save_string_to_txt, clear_file_content
from ui.context_menu import TextWidgetContextMenu

def build_summary_tab(self):
    self.summary_tab = self.tabview.add("Global Summary")
    self.summary_tab.rowconfigure(0, weight=0)
    self.summary_tab.rowconfigure(1, weight=1)
    self.summary_tab.columnconfigure(0, weight=1)
    self.summary_tab.columnconfigure(1, weight=0)
    self.summary_tab.columnconfigure(2, weight=0)

    load_btn = ctk.CTkButton(self.summary_tab, text="加载 global_summary.txt", command=self.load_global_summary, font=("Microsoft YaHei", 12))
    load_btn.grid(row=0, column=0, padx=5, pady=5, sticky="w")

    self.word_count_label = ctk.CTkLabel(self.summary_tab, text="字数：0", font=("Microsoft YaHei", 12))
    self.word_count_label.grid(row=0, column=1, padx=5, pady=5, sticky="w")

    save_btn = ctk.CTkButton(self.summary_tab, text="保存修改", command=self.save_global_summary, font=("Microsoft YaHei", 12))
    save_btn.grid(row=0, column=2, padx=5, pady=5, sticky="e")

    self.summary_text = ctk.CTkTextbox(self.summary_tab, wrap="word", font=("Microsoft YaHei", 12))
    TextWidgetContextMenu(self.summary_text)
    self.summary_text.grid(row=1, column=0, sticky="nsew", padx=5, pady=5, columnspan=3)

    def update_word_count(event=None):
        text = self.summary_text.get("0.0", "end")
        count = len(text) - 1
        self.word_count_label.configure(text=f"字数：{count}")

    self.summary_text.bind("<KeyRelease>", update_word_count)
    self.summary_text.bind("<ButtonRelease>", update_word_count)
def load_global_summary(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    filename = os.path.join(filepath, "global_summary.txt")
    content = read_file(filename)
    self.summary_text.delete("0.0", "end")
    self.summary_text.insert("0.0", content)
    self.log("已加载 global_summary.txt 到编辑区。")

def save_global_summary(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    content = self.summary_text.get("0.0", "end").strip()
    filename = os.path.join(filepath, "global_summary.txt")
    clear_file_content(filename)
    save_string_to_txt(content, filename)
    self.log("已保存对 global_summary.txt 的修改。")
</file>

<file path="utils.py">
# utils.py
# -*- coding: utf-8 -*-
import os
import json
import re

def read_file(filename: str) -> str:
    """读取文件的全部内容，若文件不存在或异常则返回空字符串。"""
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            content = file.read()
        return content
    except FileNotFoundError:
        return ""
    except Exception as e:
        print(f"[read_file] 读取文件时发生错误: {e}")
        return ""

def append_text_to_file(text_to_append: str, file_path: str):
    """在文件末尾追加文本(带换行)。若文本非空且无换行，则自动加换行。"""
    if text_to_append and not text_to_append.startswith('\n'):
        text_to_append = '\n' + text_to_append

    try:
        with open(file_path, 'a', encoding='utf-8') as file:
            file.write(text_to_append)
    except IOError as e:
        print(f"[append_text_to_file] 发生错误：{e}")

def clear_file_content(filename: str):
    """清空指定文件内容。"""
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            pass
    except IOError as e:
        print(f"[clear_file_content] 无法清空文件 '{filename}' 的内容：{e}")

def save_string_to_txt(content: str, filename: str):
    """将字符串保存为 txt 文件（覆盖写）。"""
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(content)
    except Exception as e:
        print(f"[save_string_to_txt] 保存文件时发生错误: {e}")

def save_data_to_json(data: dict, file_path: str) -> bool:
    """将数据保存到 JSON 文件。"""
    try:
        with open(file_path, 'w', encoding='utf-8') as json_file:
            json.dump(data, json_file, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        print(f"[save_data_to_json] 保存数据到JSON文件时出错: {e}")
        return False

def extract_relevant_segments(full_text: str, query_keywords: str, window_size: int = 800, step: int = 200) -> str:
    """
    [V2.0 智能版] 基于关键词密度的滑窗截取算法
    优势：容忍语义差异。只要实体词（人名/物名）由于，就能精准定位，不再需要逐字匹配。
    """
    if not full_text or len(full_text) < window_size:
        return full_text

    # 1. 预处理搜索词
    # 将 "百草堂·内部陈设" 拆分为 ["百草堂", "内部", "陈设"]
    # 过滤掉单字，只保留有意义的双字以上词汇
    keywords = [k.strip() for k in re.split(r'[·\s\-_]+', query_keywords) if len(k.strip()) > 1]
    
    if not keywords:
        return full_text[:window_size]  # 降级：返回头部

    # 2. 滑动窗口扫描
    # 我们不求全匹配，只求“得分最高”的窗口
    max_score = -1
    best_window = full_text[:window_size]
    
    # 简单的打分函数：统计窗口内包含多少个关键词
    text_len = len(full_text)
    for start in range(0, text_len, step):
        end = min(start + window_size, text_len)
        window_text = full_text[start:end]
        
        score = 0
        for kw in keywords:
            # 模糊匹配逻辑：
            # 1. 如果关键词直接存在，+10分
            if kw in window_text:
                score += 10
            # 2. (可选) 如果关键词没找到，但它的子序列存在(针对长词)，+3分
            # 比如搜"爆裂火灵果"，原文是"火灵果"，也能加分
            elif len(kw) >= 3 and kw[1:] in window_text:
                score += 3
        
        # 记录最高分的窗口
        if score > max_score:
            max_score = score
            best_window = window_text
            
        # 优化：如果窗口得分极高（命中了一半以上关键词），直接提前返回，节省时间
        if score >= len(keywords) * 10 * 0.8:
            break

    # 3. 结果优化：前后加省略号
    return f"...{best_window}..."
</file>

<file path="main.spec">
# -*- mode: python ; coding: utf-8 -*-
import os
from PyInstaller.utils.hooks import collect_all, collect_submodules

# 收集chromadb所需的数据和二进制文件
datas = []
binaries = []
hiddenimports = [
    'typing_extensions',
    'langchain_openai',
    'langgraph',
    'openai',
    'google.genai',
    'nltk',
    'sentence_transformers',
    'sklearn',
    'langchain_community',
    'pydantic',
    'pydantic.v1',
    'pydantic.deprecated.decorator',
    'tiktoken_ext.openai_public',
    'tiktoken_ext',
    'chromadb.utils.embedding_functions.onnx_mini_lm_l6_v2',
    'customtkinter',
    'tkinter',
    'langchain',
    'langchain_core',
    'langchain_core.tools',
    'langchain_core.agents',
    'langchain_core.messages',
    'langchain_core.prompt_values',
    'langchain_core.prompts',
    'langchain_core.output_parsers',
    'langchain_core.runnables',
    'langchain_core.callbacks',
    'langchain_core.documents',
    'langchain_core.vectorstores',
    'langchain_core.embeddings',
    'langchain_core.language_models',
    'langchain_core.pydantic_v1',
    'langchain.text_splitter',
    'langchain_community.llms',
    'langchain_community.chat_models',
    'langchain_community.embeddings',
    'langchain_community.vectorstores',
    'langchain_community.document_loaders',
    'langchain_community.tools',
    'langchain_community.utilities',
    'langchain_community.vectorstores.chroma',
    'chromadb.api',
    'chromadb.api.models',
    'chromadb.api.types',
    'chromadb.config',
    'chromadb.errors',
    'chromadb.types',
    'chromadb.telemetry',
    'torch',
    'transformers',
    'numpy',
    'requests',
    'yaml',
    'jinja2',
    'fsspec',
    'PIL',
    'pillow',
    'charset_normalizer',
    'idna',
    'certifi',
    'urllib3',
    'click',
    'colorama',
    'filelock',
    'packaging',
    'psutil',
    'pyyaml',
    'six',
    'smart_open',
    'xxhash',
    'regex',
    'tqdm',
    'tokenizers',
    'accelerate',
    'bitsandbytes',
    'peft',
    'trl',
    'datasets',
    'evaluate',
    'rouge_score',
    'seqeval',
    'optimum',
    'diffusers',
    'controlnet_aux',
    'inflect',
    'librosa',
    'sentencepiece',
    'jieba',
    'safetensors',
]

# 收集项目中的数据文件
datas += [
    ('ui', 'ui'),
    ('novel_generator', 'novel_generator'),
    ('config.example.json', '.'),
    ('prompt_definitions.py', '.'),
    ('utils.py', '.'),
    ('config_manager.py', '.'),
    ('embedding_adapters.py', '.'),
    ('llm_adapters.py', '.'),
    ('consistency_checker.py', '.'),
    ('chapter_directory_parser.py', '.'),
    ('tooltips.py', '.'),
]

# 收集chromadb相关资源
tmp_ret = collect_all('chromadb')
datas += tmp_ret[0]
binaries += tmp_ret[1]
hiddenimports += tmp_ret[2]

# 收集customtkinter资源
try:
    import customtkinter
    customtkinter_dir = os.path.dirname(customtkinter.__file__)
    datas.append((customtkinter_dir, 'customtkinter'))
except ImportError:
    pass

# 收集langchain相关模块
for mod in collect_submodules('langchain'):
    if mod not in hiddenimports:
        hiddenimports.append(mod)

for mod in collect_submodules('langchain_community'):
    if mod not in hiddenimports:
        hiddenimports.append(mod)

for mod in collect_submodules('langchain_core'):
    if mod not in hiddenimports:
        hiddenimports.append(mod)

a = Analysis(
    ['main.py'],
    pathex=[],
    binaries=binaries,
    datas=datas,
    hiddenimports=hiddenimports,
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=None,
    noarchive=False,
)

pyz = PYZ(a.pure, a.zipped_data, cipher=None)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='AI_NovelGenerator',
    debug=False,  # 设置为False以减少调试信息
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=False,  # 设置为False以隐藏控制台窗口
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
    icon='icon.ico'  # 如果有图标文件
)

coll = COLLECT(
    exe,
    a.binaries,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='AI_NovelGenerator'
)
</file>

<file path="novel_generator/vectorstore_utils.py">
#novel_generator/vectorstore_utils.py
# -*- coding: utf-8 -*-
"""
向量库相关操作（初始化、更新、检索、清空、文本切分等）
"""
import os
import logging
import traceback
import nltk
import numpy as np
import re
import ssl
import requests
import warnings
from langchain_chroma import Chroma
logging.basicConfig(
    filename='app.log',      # 日志文件名
    filemode='a',            # 追加模式（'w' 会覆盖）
    level=logging.INFO,      # 记录 INFO 及以上级别的日志
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
# 禁用特定的Torch警告
warnings.filterwarnings('ignore', message='.*Torch was not compiled with flash attention.*')
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # 禁用tokenizer并行警告

from chromadb.config import Settings
from langchain.docstore.document import Document
from sklearn.metrics.pairwise import cosine_similarity
from .common import call_with_retry

def get_vectorstore_dir(filepath: str) -> str:
    """获取 vectorstore 路径"""
    return os.path.join(filepath, "vectorstore")

def clear_vector_store(filepath: str) -> bool:
    """清空 清空向量库"""
    import shutil
    store_dir = get_vectorstore_dir(filepath)
    if not os.path.exists(store_dir):
        logging.info("No vector store found to clear.")
        return False
    try:
        shutil.rmtree(store_dir)
        logging.info(f"Vector store directory '{store_dir}' removed.")
        return True
    except Exception as e:
        logging.error(f"无法删除向量库文件夹，请关闭程序后手动删除 {store_dir}。\n {str(e)}")
        traceback.print_exc()
        return False

def init_vector_store(embedding_adapter, texts, filepath: str):
    """
    在 filepath 下创建/加载一个 Chroma 向量库并插入 texts。
    如果Embedding失败，则返回 None，不中断任务。
    """
    from langchain.embeddings.base import Embeddings as LCEmbeddings

    store_dir = get_vectorstore_dir(filepath)
    os.makedirs(store_dir, exist_ok=True)
    documents = [Document(page_content=str(t)) for t in texts]

    try:
        class LCEmbeddingWrapper(LCEmbeddings):
            def embed_documents(self, texts):
                return call_with_retry(
                    func=embedding_adapter.embed_documents,
                    max_retries=3,
                    fallback_return=[],
                    texts=texts
                )
            def embed_query(self, query: str):
                res = call_with_retry(
                    func=embedding_adapter.embed_query,
                    max_retries=3,
                    fallback_return=[],
                    query=query
                )
                return res

        chroma_embedding = LCEmbeddingWrapper()
        vectorstore = Chroma.from_documents(
            documents,
            embedding=chroma_embedding,
            persist_directory=store_dir,
            client_settings=Settings(anonymized_telemetry=False),
            collection_name="novel_collection"
        )
        return vectorstore
    except Exception as e:
        logging.warning(f"Init vector store failed: {e}")
        traceback.print_exc()
        return None

def load_vector_store(embedding_adapter, filepath: str):
    """
    读取已存在的 Chroma 向量库。若不存在则返回 None。
    如果加载失败（embedding 或IO问题），则返回 None。
    """
    from langchain.embeddings.base import Embeddings as LCEmbeddings
    store_dir = get_vectorstore_dir(filepath)
    if not os.path.exists(store_dir):
        logging.info("Vector store not found. Will return None.")
        return None

    try:
        class LCEmbeddingWrapper(LCEmbeddings):
            def embed_documents(self, texts):
                return call_with_retry(
                    func=embedding_adapter.embed_documents,
                    max_retries=3,
                    fallback_return=[],
                    texts=texts
                )
            def embed_query(self, query: str):
                res = call_with_retry(
                    func=embedding_adapter.embed_query,
                    max_retries=3,
                    fallback_return=[],
                    query=query
                )
                return res

        chroma_embedding = LCEmbeddingWrapper()
        return Chroma(
            persist_directory=store_dir,
            embedding_function=chroma_embedding,
            client_settings=Settings(anonymized_telemetry=False),
            collection_name="novel_collection"
        )
    except Exception as e:
        logging.warning(f"Failed to load vector store: {e}")
        traceback.print_exc()
        return None

def split_by_length(text: str, max_length: int = 500):
    """按照 max_length 切分文本"""
    segments = []
    start_idx = 0
    while start_idx < len(text):
        end_idx = min(start_idx + max_length, len(text))
        segment = text[start_idx:end_idx]
        segments.append(segment.strip())
        start_idx = end_idx
    return segments

def split_text_for_vectorstore(chapter_text: str, max_length: int = 500, similarity_threshold: float = 0.7):
    """
    对新的章节文本进行分段后,再用于存入向量库。
    使用 embedding 进行文本相似度计算。
    """
    if not chapter_text.strip():
        return []
    
    # --- 修改开始：自动检查并下载缺失的 NLTK 数据包 ---
    try:
        # 检查是否已存在 punkt_tab，不存在才下载
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        # quiet=True 防止在控制台打印大量下载进度条
        nltk.download('punkt_tab', quiet=True)
        
    try:
        # 保险起见，同时也检查基础的 punkt 包
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    # --- 修改结束 ---

    sentences = nltk.sent_tokenize(chapter_text)
    if not sentences:
        return []
    
    # 直接按长度分段,不做相似度合并
    final_segments = []
    current_segment = []
    current_length = 0
    
    for sentence in sentences:
        sentence_length = len(sentence)
        if current_length + sentence_length > max_length:
            if current_segment:
                final_segments.append(" ".join(current_segment))
            current_segment = [sentence]
            current_length = sentence_length
        else:
            current_segment.append(sentence)
            current_length += sentence_length
    
    if current_segment:
        final_segments.append(" ".join(current_segment))
    
    return final_segments

def update_vector_store(embedding_adapter, new_chapter: str, filepath: str):
    """
    将最新章节文本插入到向量库中。
    若库不存在则初始化；若初始化/更新失败，则跳过。
    """
    from utils import read_file, clear_file_content, save_string_to_txt
    splitted_texts = split_text_for_vectorstore(new_chapter)
    if not splitted_texts:
        logging.warning("No valid text to insert into vector store. Skipping.")
        return

    store = load_vector_store(embedding_adapter, filepath)
    if not store:
        logging.info("Vector store does not exist or failed to load. Initializing a new one for new chapter...")
        store = init_vector_store(embedding_adapter, splitted_texts, filepath)
        if not store:
            logging.warning("Init vector store failed, skip embedding.")
        else:
            logging.info("New vector store created successfully.")
        return

    try:
        docs = [Document(page_content=str(t)) for t in splitted_texts]
        store.add_documents(docs)
        logging.info("Vector store updated with the new chapter splitted segments.")
    except Exception as e:
        logging.warning(f"Failed to update vector store: {e}")
        traceback.print_exc()

def get_relevant_context_from_vector_store(embedding_adapter, query: str, filepath: str, k: int = 2) -> str:
    """
    从向量库中检索与 query 最相关的 k 条文本，拼接后返回。
    如果向量库加载/检索失败，则返回空字符串。
    最终只返回最多2000字符的检索片段。
    """
    store = load_vector_store(embedding_adapter, filepath)
    if not store:
        logging.info("No vector store found or load failed. Returning empty context.")
        return ""

    try:
        docs = store.similarity_search(query, k=k)
        if not docs:
            logging.info(f"No relevant documents found for query '{query}'. Returning empty context.")
            return ""
        combined = "\n".join([d.page_content for d in docs])
        if len(combined) > 2000:
            combined = combined[:2000]
        return combined
    except Exception as e:
        logging.warning(f"Similarity search failed: {e}")
        traceback.print_exc()
        return ""

def _get_sentence_transformer(model_name: str = 'paraphrase-MiniLM-L6-v2'):
    """获取sentence transformer模型，处理SSL问题"""
    try:
        # 设置torch环境变量
        os.environ["TORCH_ALLOW_TF32_CUBLAS_OVERRIDE"] = "0"
        os.environ["TORCH_CUDNN_V8_API_ENABLED"] = "0"
        
        # 禁用SSL验证
        ssl._create_default_https_context = ssl._create_unverified_context
        
        # ...existing code...
    except Exception as e:
        logging.error(f"Failed to load sentence transformer model: {e}")
        traceback.print_exc()
        return None
</file>

<file path="ui/chapters_tab.py">
# ui/chapters_tab.py
# -*- coding: utf-8 -*-
import os
import customtkinter as ctk
from tkinter import messagebox
from ui.context_menu import TextWidgetContextMenu
from utils import read_file, save_string_to_txt, clear_file_content

def build_chapters_tab(self):
    self.chapters_view_tab = self.tabview.add("Chapters Manage")
    self.chapters_view_tab.rowconfigure(0, weight=0)
    self.chapters_view_tab.rowconfigure(1, weight=1)
    self.chapters_view_tab.columnconfigure(0, weight=1)

    top_frame = ctk.CTkFrame(self.chapters_view_tab)
    top_frame.grid(row=0, column=0, sticky="ew", padx=5, pady=5)
    top_frame.columnconfigure(0, weight=0)
    top_frame.columnconfigure(1, weight=0)
    top_frame.columnconfigure(2, weight=0)
    top_frame.columnconfigure(3, weight=0)
    top_frame.columnconfigure(4, weight=1)

    prev_btn = ctk.CTkButton(top_frame, text="<< 上一章", command=self.prev_chapter, font=("Microsoft YaHei", 12))
    prev_btn.grid(row=0, column=0, padx=5, pady=5, sticky="w")

    next_btn = ctk.CTkButton(top_frame, text="下一章 >>", command=self.next_chapter, font=("Microsoft YaHei", 12))
    next_btn.grid(row=0, column=1, padx=5, pady=5, sticky="w")

    self.chapter_select_var = ctk.StringVar(value="")
    self.chapter_select_menu = ctk.CTkOptionMenu(top_frame, values=[], variable=self.chapter_select_var, command=self.on_chapter_selected, font=("Microsoft YaHei", 12))
    self.chapter_select_menu.grid(row=0, column=2, padx=5, pady=5, sticky="w")

    save_btn = ctk.CTkButton(top_frame, text="保存修改", command=self.save_current_chapter, font=("Microsoft YaHei", 12))
    save_btn.grid(row=0, column=3, padx=5, pady=5, sticky="w")

    refresh_btn = ctk.CTkButton(top_frame, text="刷新章节列表", command=self.refresh_chapters_list, font=("Microsoft YaHei", 12))
    refresh_btn.grid(row=0, column=5, padx=5, pady=5, sticky="e")

    self.chapters_word_count_label = ctk.CTkLabel(top_frame, text="字数：0", font=("Microsoft YaHei", 12))
    self.chapters_word_count_label.grid(row=0, column=4, padx=(0,10), sticky="e")

    self.chapter_view_text = ctk.CTkTextbox(self.chapters_view_tab, wrap="word", font=("Microsoft YaHei", 12))
    
    def update_word_count(event=None):
        text = self.chapter_view_text.get("0.0", "end-1c")
        text_length = len(text)
        self.chapters_word_count_label.configure(text=f"字数：{text_length}")
    
    self.chapter_view_text.bind("<KeyRelease>", update_word_count)
    self.chapter_view_text.bind("<ButtonRelease>", update_word_count)
    TextWidgetContextMenu(self.chapter_view_text)
    self.chapter_view_text.grid(row=1, column=0, sticky="nsew", padx=5, pady=5, columnspan=6)

    self.chapters_list = []
    refresh_chapters_list(self)

def refresh_chapters_list(self):
    filepath = self.filepath_var.get().strip()
    chapters_dir = os.path.join(filepath, "chapters")
    if not os.path.exists(chapters_dir):
        self.safe_log("尚未找到 chapters 文件夹，请先生成章节或检查保存路径。")
        self.chapter_select_menu.configure(values=[])
        return

    all_files = os.listdir(chapters_dir)
    chapter_nums = []
    for f in all_files:
        if f.startswith("chapter_") and f.endswith(".txt"):
            number_part = f.replace("chapter_", "").replace(".txt", "")
            if number_part.isdigit():
                chapter_nums.append(number_part)
    chapter_nums.sort(key=lambda x: int(x))
    self.chapters_list = chapter_nums
    self.chapter_select_menu.configure(values=self.chapters_list)
    current_selected = self.chapter_select_var.get()
    if current_selected not in self.chapters_list:
        if self.chapters_list:
            # 默认选择数字最大的章节
            max_chapter = max(self.chapters_list, key=lambda x: int(x))
            self.chapter_select_var.set(max_chapter)
            load_chapter_content(self, max_chapter)
        else:
            self.chapter_select_var.set("")
            self.chapter_view_text.delete("0.0", "end")

def on_chapter_selected(self, value):
    load_chapter_content(self, value)

def load_chapter_content(self, chapter_number_str):
    if not chapter_number_str:
        return
    filepath = self.filepath_var.get().strip()
    chapter_file = os.path.join(filepath, "chapters", f"chapter_{chapter_number_str}.txt")
    if not os.path.exists(chapter_file):
        self.safe_log(f"章节文件 {chapter_file} 不存在！")
        return
    content = read_file(chapter_file)
    self.chapter_view_text.delete("0.0", "end")
    self.chapter_view_text.insert("0.0", content)

def save_current_chapter(self):
    chapter_number_str = self.chapter_select_var.get()
    if not chapter_number_str:
        messagebox.showwarning("警告", "尚未选择章节，无法保存。")
        return
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径")
        return
    chapter_file = os.path.join(filepath, "chapters", f"chapter_{chapter_number_str}.txt")
    content = self.chapter_view_text.get("0.0", "end").strip()
    clear_file_content(chapter_file)
    save_string_to_txt(content, chapter_file)
    self.safe_log(f"已保存对第 {chapter_number_str} 章的修改。")

def prev_chapter(self):
    if not self.chapters_list:
        return
    current = self.chapter_select_var.get()
    if current not in self.chapters_list:
        return
    idx = self.chapters_list.index(current)
    if idx > 0:
        new_idx = idx - 1
        self.chapter_select_var.set(self.chapters_list[new_idx])
        load_chapter_content(self, self.chapters_list[new_idx])
    else:
        messagebox.showinfo("提示", "已经是第一章了。")

def next_chapter(self):
    if not self.chapters_list:
        return
    current = self.chapter_select_var.get()
    if current not in self.chapters_list:
        return
    idx = self.chapters_list.index(current)
    if idx < len(self.chapters_list) - 1:
        new_idx = idx + 1
        self.chapter_select_var.set(self.chapters_list[new_idx])
        load_chapter_content(self, self.chapters_list[new_idx])
    else:
        messagebox.showinfo("提示", "已经是最后一章了。")
</file>

<file path="ui/main_tab.py">
# ui/main_tab.py
# -*- coding: utf-8 -*-
import customtkinter as ctk
from tkinter import messagebox
from ui.context_menu import TextWidgetContextMenu

def build_main_tab(self):
    """
    主Tab包含左侧的"本章内容"编辑框和输出日志，以及右侧的主要操作和参数设置区
    """
    self.main_tab = self.tabview.add("Main Functions")
    self.main_tab.rowconfigure(0, weight=1)
    self.main_tab.columnconfigure(0, weight=1)
    self.main_tab.columnconfigure(1, weight=0)

    self.left_frame = ctk.CTkFrame(self.main_tab)
    self.left_frame.grid(row=0, column=0, sticky="nsew", padx=2, pady=2)

    self.right_frame = ctk.CTkFrame(self.main_tab)
    self.right_frame.grid(row=0, column=1, sticky="nsew", padx=2, pady=2)

    build_left_layout(self)
    build_right_layout(self)

def build_left_layout(self):
    """
    左侧区域：本章内容(可编辑) + Step流程按钮 + 输出日志(只读)
    """
    self.left_frame.grid_rowconfigure(0, weight=0)
    self.left_frame.grid_rowconfigure(1, weight=2)
    self.left_frame.grid_rowconfigure(2, weight=0)
    self.left_frame.grid_rowconfigure(3, weight=0)
    self.left_frame.grid_rowconfigure(4, weight=1)
    self.left_frame.columnconfigure(0, weight=1)

    self.chapter_label = ctk.CTkLabel(self.left_frame, text="本章内容（可编辑）  字数：0", font=("Microsoft YaHei", 12))
    self.chapter_label.grid(row=0, column=0, padx=5, pady=(5, 0), sticky="w")

    # 章节文本编辑框
    self.chapter_result = ctk.CTkTextbox(self.left_frame, wrap="word", font=("Microsoft YaHei", 14))
    TextWidgetContextMenu(self.chapter_result)
    self.chapter_result.grid(row=1, column=0, sticky="nsew", padx=5, pady=(0, 5))



    def update_word_count(event=None):
        text = self.chapter_result.get("0.0", "end")
        count = len(text) - 1  # 减去最后一个换行符
        self.chapter_label.configure(text=f"本章内容（可编辑）  字数：{count}")

    self.chapter_result.bind("<KeyRelease>", update_word_count)
    self.chapter_result.bind("<ButtonRelease>", update_word_count)

    # Step 按钮区域
    self.step_buttons_frame = ctk.CTkFrame(self.left_frame)
    self.step_buttons_frame.grid(row=2, column=0, sticky="ew", padx=5, pady=5)
    self.step_buttons_frame.columnconfigure((0, 1, 2, 3, 4), weight=1)


    self.btn_generate_architecture = ctk.CTkButton(
        self.step_buttons_frame,
        text="Step1. 生成架构",
        command=self.generate_novel_architecture_ui,
        font=("Microsoft YaHei", 12)
    )
    self.btn_generate_architecture.grid(row=0, column=0, padx=5, pady=2, sticky="ew")

    self.btn_generate_directory = ctk.CTkButton(
        self.step_buttons_frame,
        text="Step2. 生成目录",
        command=self.generate_chapter_blueprint_ui,
        font=("Microsoft YaHei", 12)
    )
    self.btn_generate_directory.grid(row=0, column=1, padx=5, pady=2, sticky="ew")

    self.btn_generate_chapter = ctk.CTkButton(
        self.step_buttons_frame,
        text="Step3. 生成草稿",
        command=self.generate_chapter_draft_ui,
        font=("Microsoft YaHei", 12)
    )
    self.btn_generate_chapter.grid(row=0, column=2, padx=5, pady=2, sticky="ew")

    self.btn_finalize_chapter = ctk.CTkButton(
        self.step_buttons_frame,
        text="Step4. 定稿章节",
        command=self.finalize_chapter_ui,
        font=("Microsoft YaHei", 12)
    )
    self.btn_finalize_chapter.grid(row=0, column=3, padx=5, pady=2, sticky="ew")

    self.btn_batch_generate = ctk.CTkButton(
        self.step_buttons_frame,
        text="批量生成",
        command=self.generate_batch_ui,
        font=("Microsoft YaHei", 12)
    )
    self.btn_batch_generate.grid(row=0, column=4, padx=5, pady=2, sticky="ew")


    # 日志文本框
    log_label = ctk.CTkLabel(self.left_frame, text="输出日志 (只读)", font=("Microsoft YaHei", 12))
    log_label.grid(row=3, column=0, padx=5, pady=(5, 0), sticky="w")

    self.log_text = ctk.CTkTextbox(self.left_frame, wrap="word", font=("Microsoft YaHei", 12))
    TextWidgetContextMenu(self.log_text)
    self.log_text.grid(row=4, column=0, sticky="nsew", padx=5, pady=(0, 5))
    self.log_text.configure(state="disabled")

def build_right_layout(self):
    """
    右侧区域：配置区(tabview) + 小说主参数 + 可选功能按钮
    """
    self.right_frame.grid_rowconfigure(0, weight=0)
    self.right_frame.grid_rowconfigure(1, weight=1)
    self.right_frame.grid_rowconfigure(2, weight=0)
    self.right_frame.columnconfigure(0, weight=1)

    # 配置区（AI/Embedding）
    self.config_frame = ctk.CTkFrame(self.right_frame, corner_radius=10, border_width=2, border_color="gray")
    self.config_frame.grid(row=0, column=0, sticky="ew", padx=5, pady=5)
    self.config_frame.columnconfigure(0, weight=1)
    # 其余部分将在 config_tab.py 与 novel_params_tab.py 中构建
</file>

<file path="config_manager.py">
# config_manager.py
# -*- coding: utf-8 -*-
import json
import os
import threading
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter


def load_config(config_file: str) -> dict:
    """从指定的 config_file 加载配置，若不存在则创建一个默认配置文件。"""

    # PenBo 修改代码，增加配置文件不存在则创建一个默认配置文件
    if not os.path.exists(config_file):
        create_config(config_file)

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
            return {}


# PenBo 增加了创建默认配置文件函数
def create_config(config_file: str) -> dict:
    """创建一个创建默认配置文件。"""
    config = {
    "last_interface_format": "OpenAI",
    "last_embedding_interface_format": "OpenAI",
    "llm_configs": {
        "DeepSeek V3": {
            "api_key": "",
            "base_url": "https://api.deepseek.com/v1",
            "model_name": "deepseek-chat",
            "temperature": 0.7,
            "max_tokens": 8192,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "GPT 5": {
            "api_key": "",
            "base_url": "https://api.openai.com/v1",
            "model_name": "gpt-5",
            "temperature": 0.7,
            "max_tokens": 32768,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "Gemini 2.5 Pro": {
            "api_key": "",
            "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
            "model_name": "gemini-2.5-pro",
            "temperature": 0.7,
            "max_tokens": 32768,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
        "SiliconFlow/DeepSeek V3": {
            "api_key": "sk-your-siliconflow-key",
            "base_url": "https://api.siliconflow.cn/v1",
            "model_name": "deepseek-ai/DeepSeek-V3",
            "temperature": 0.7,
            "max_tokens": 4096,
            "timeout": 600,
            "interface_format": "OpenAI"
        },
    },
    "embedding_configs": {
        "OpenAI": {
            "api_key": "",
            "base_url": "https://api.openai.com/v1",
            "model_name": "text-embedding-ada-002",
            "retrieval_k": 4,
            "interface_format": "OpenAI"
        }
    },
    "other_params": {
        "topic": "",
        "genre": "",
        "num_chapters": 0,
        "word_number": 0,
        "filepath": "",
        "chapter_num": "120",
        "user_guidance": "",
        "characters_involved": "",
        "key_items": "",
        "scene_location": "",
        "time_constraint": ""
    },
    "choose_configs": {
        "prompt_draft_llm": "DeepSeek V3",
        "chapter_outline_llm": "DeepSeek V3",
        "architecture_llm": "Gemini 2.5 Pro",
        "final_chapter_llm": "GPT 5",
        "consistency_review_llm": "DeepSeek V3",
        "refine_logic_llm": "Gemini 2.5 Pro"
    },
    "proxy_setting": {
        "proxy_url": "127.0.0.1",
        "proxy_port": "",
        "enabled": False
    },
    "webdav_config": {
        "webdav_url": "",
        "webdav_username": "",
        "webdav_password": ""
    }
}
    save_config(config, config_file)



def save_config(config_data: dict, config_file: str) -> bool:
    """将 config_data 保存到 config_file 中，返回 True/False 表示是否成功。"""
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=4)
        return True
    except:
        return False

def test_llm_config(interface_format, api_key, base_url, model_name, temperature, max_tokens, timeout, log_func, handle_exception_func):
    """测试当前的LLM配置是否可用"""
    def task():
        try:
            log_func("开始测试LLM配置...")
            llm_adapter = create_llm_adapter(
                interface_format=interface_format,
                base_url=base_url,
                model_name=model_name,
                api_key=api_key,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=timeout
            )

            test_prompt = "Please reply 'OK'"
            response = llm_adapter.invoke(test_prompt)
            if response:
                log_func("✅ LLM配置测试成功！")
                log_func(f"测试回复: {response}")
            else:
                log_func("❌ LLM配置测试失败：未获取到响应")
        except Exception as e:
            log_func(f"❌ LLM配置测试出错: {str(e)}")
            handle_exception_func("测试LLM配置时出错")

    threading.Thread(target=task, daemon=True).start()

def test_embedding_config(api_key, base_url, interface_format, model_name, log_func, handle_exception_func):
    """测试当前的Embedding配置是否可用"""
    def task():
        try:
            log_func("开始测试Embedding配置...")
            embedding_adapter = create_embedding_adapter(
                interface_format=interface_format,
                api_key=api_key,
                base_url=base_url,
                model_name=model_name
            )

            test_text = "测试文本"
            embeddings = embedding_adapter.embed_query(test_text)
            if embeddings and len(embeddings) > 0:
                log_func("✅ Embedding配置测试成功！")
                log_func(f"生成的向量维度: {len(embeddings)}")
            else:
                log_func("❌ Embedding配置测试失败：未获取到向量")
        except Exception as e:
            log_func(f"❌ Embedding配置测试出错: {str(e)}")
            handle_exception_func("测试Embedding配置时出错")

    threading.Thread(target=task, daemon=True).start()
</file>

<file path="embedding_adapters.py">
# embedding_adapters.py
# -*- coding: utf-8 -*-
import logging
import traceback
from typing import List
import requests
from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings

def ensure_openai_base_url_has_v1(url: str) -> str:
    """
    若用户输入的 url 不包含 '/v1'，则在末尾追加 '/v1'。
    """
    import re
    url = url.strip()
    if not url:
        return url
    if not re.search(r'/v\d+$', url):
        if '/v1' not in url:
            url = url.rstrip('/') + '/v1'
    return url

class BaseEmbeddingAdapter:
    """
    Embedding 接口统一基类
    """
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        raise NotImplementedError

    def embed_query(self, query: str) -> List[float]:
        raise NotImplementedError

class OpenAIEmbeddingAdapter(BaseEmbeddingAdapter):
    """
    基于 OpenAIEmbeddings（或兼容接口）的适配器
    """
    def __init__(self, api_key: str, base_url: str, model_name: str):
        self._embedding = OpenAIEmbeddings(
            openai_api_key=api_key,
            openai_api_base=ensure_openai_base_url_has_v1(base_url),
            model=model_name
        )

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embedding.embed_documents(texts)

    def embed_query(self, query: str) -> List[float]:
        return self._embedding.embed_query(query)

class AzureOpenAIEmbeddingAdapter(BaseEmbeddingAdapter):
    """
    基于 AzureOpenAIEmbeddings（或兼容接口）的适配器
    """
    def __init__(self, api_key: str, base_url: str, model_name: str):
        import re
        match = re.match(r'https://(.+?)/openai/deployments/(.+?)/embeddings\?api-version=(.+)', base_url)
        if match:
            self.azure_endpoint = f"https://{match.group(1)}"
            self.azure_deployment = match.group(2)
            self.api_version = match.group(3)
        else:
            raise ValueError("Invalid Azure OpenAI base_url format")
        
        self._embedding = AzureOpenAIEmbeddings(
            azure_endpoint=self.azure_endpoint,
            azure_deployment=self.azure_deployment,
            openai_api_key=api_key,
            api_version=self.api_version,
        )

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embedding.embed_documents(texts)

    def embed_query(self, query: str) -> List[float]:
        return self._embedding.embed_query(query)

class OllamaEmbeddingAdapter(BaseEmbeddingAdapter):
    """
    其接口路径为 /api/embeddings
    """
    def __init__(self, model_name: str, base_url: str):
        self.model_name = model_name
        self.base_url = base_url.rstrip("/")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = []
        for text in texts:
            vec = self._embed_single(text)
            embeddings.append(vec)
        return embeddings

    def embed_query(self, query: str) -> List[float]:
        return self._embed_single(query)

    def _embed_single(self, text: str) -> List[float]:
        """
        调用 Ollama 本地服务 /api/embeddings 接口，获取文本 embedding
        """
        url = self.base_url.rstrip("/")
        if "/api/embeddings" not in url:
            if "/api" in url:
                url = f"{url}/embeddings"
            else:
                if "/v1" in url:
                    url = url[:url.index("/v1")]
                url = f"{url}/api/embeddings"

        data = {
            "model": self.model_name,
            "prompt": text
        }
        try:
            response = requests.post(url, json=data)
            response.raise_for_status()
            result = response.json()
            if "embedding" not in result:
                raise ValueError("No 'embedding' field in Ollama response.")
            return result["embedding"]
        except requests.exceptions.RequestException as e:
            logging.error(f"Ollama embeddings request error: {e}\n{traceback.format_exc()}")
            return []

class MLStudioEmbeddingAdapter(BaseEmbeddingAdapter):
    """
    基于 LM Studio 的 embedding 适配器
    """
    def __init__(self, api_key: str, base_url: str, model_name: str):
        self.url = ensure_openai_base_url_has_v1(base_url)
        if not self.url.endswith('/embeddings'):
            self.url = f"{self.url}/embeddings"
        
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.model_name = model_name

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            payload = {
                "input": texts,
                "model": self.model_name
            }
            response = requests.post(self.url, json=payload, headers=self.headers)
            response.raise_for_status()
            result = response.json()
            if "data" not in result:
                logging.error(f"Invalid response format from LM Studio API: {result}")
                return [[]] * len(texts)
            return [item.get("embedding", []) for item in result["data"]]
        except requests.exceptions.RequestException as e:
            logging.error(f"LM Studio API request failed: {str(e)}")
            return [[]] * len(texts)
        except (KeyError, IndexError, ValueError, TypeError) as e:
            logging.error(f"Error parsing LM Studio API response: {str(e)}")
            return [[]] * len(texts)

    def embed_query(self, query: str) -> List[float]:
        try:
            payload = {
                "input": query,
                "model": self.model_name
            }
            response = requests.post(self.url, json=payload, headers=self.headers)
            response.raise_for_status()
            result = response.json()
            if "data" not in result or not result["data"]:
                logging.error(f"Invalid response format from LM Studio API: {result}")
                return []
            return result["data"][0].get("embedding", [])
        except requests.exceptions.RequestException as e:
            logging.error(f"LM Studio API request failed: {str(e)}")
            return []
        except (KeyError, IndexError, ValueError, TypeError) as e:
            logging.error(f"Error parsing LM Studio API response: {str(e)}")
            return []

class GeminiEmbeddingAdapter(BaseEmbeddingAdapter):
    """
    基于 Google Generative AI (Gemini) 接口的 Embedding 适配器
    使用直接 POST 请求方式，URL 示例：
    https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedContent?key=YOUR_API_KEY
    """
    def __init__(self, api_key: str, model_name: str, base_url: str):
        """
        :param api_key: 传入的 Google API Key
        :param model_name: 这里一般是 "text-embedding-004"
        :param base_url: e.g. https://generativelanguage.googleapis.com/v1beta/models
        """
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url.rstrip("/")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = []
        for text in texts:
            vec = self._embed_single(text)
            embeddings.append(vec)
        return embeddings

    def embed_query(self, query: str) -> List[float]:
        return self._embed_single(query)

    def _embed_single(self, text: str) -> List[float]:
        """
        直接调用 Google Generative Language API (Gemini) 接口，获取文本 embedding
        """
        url = f"{self.base_url}/{self.model_name}:embedContent?key={self.api_key}"
        payload = {
            "model": self.model_name,
            "content": {
                "parts": [
                    {"text": text}
                ]
            }
        }

        try:
            response = requests.post(url, json=payload)
            print(response.text)
            response.raise_for_status()
            result = response.json()
            embedding_data = result.get("embedding", {})
            return embedding_data.get("values", [])
        except requests.exceptions.RequestException as e:
            logging.error(f"Gemini embed_content request error: {e}\n{traceback.format_exc()}")
            return []
        except Exception as e:
            logging.error(f"Gemini embed_content parse error: {e}\n{traceback.format_exc()}")
            return []

class SiliconFlowEmbeddingAdapter(BaseEmbeddingAdapter):
    """
    基于 SiliconFlow 的 embedding 适配器
    """
    def __init__(self, api_key: str, base_url: str, model_name: str):
        # 自动为 base_url 添加 scheme（如果缺失）
        if not base_url.startswith("http://") and not base_url.startswith("https://"):
            base_url = "https://" + base_url
        self.url = base_url if base_url else "https://api.siliconflow.cn/v1/embeddings"

        self.payload = {
            "model": model_name,
            "input": "Silicon flow embedding online: fast, affordable, and high-quality embedding services. come try it out!",
            "encoding_format": "float"
        }
        self.headers = {
            "Authorization": "Bearer {api_key}".format(api_key=api_key),
            "Content-Type": "application/json"
        }

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = []
        for text in texts:
            try:
                self.payload["input"] = text
                response = requests.post(self.url, json=self.payload, headers=self.headers)
                response.raise_for_status()
                result = response.json()
                if not result or "data" not in result or not result["data"]:
                    logging.error(f"Invalid response format from SiliconFlow API: {result}")
                    embeddings.append([])
                    continue
                emb = result["data"][0].get("embedding", [])
                embeddings.append(emb)
            except requests.exceptions.RequestException as e:
                logging.error(f"SiliconFlow API request failed: {str(e)}")
                embeddings.append([])
            except (KeyError, IndexError, ValueError, TypeError) as e:
                logging.error(f"Error parsing SiliconFlow API response: {str(e)}")
                embeddings.append([])
        return embeddings

    def embed_query(self, query: str) -> List[float]:
        try:
            self.payload["input"] = query
            response = requests.post(self.url, json=self.payload, headers=self.headers)
            response.raise_for_status()
            result = response.json()
            if not result or "data" not in result or not result["data"]:
                logging.error(f"Invalid response format from SiliconFlow API: {result}")
                return []
            return result["data"][0].get("embedding", [])
        except requests.exceptions.RequestException as e:
            logging.error(f"SiliconFlow API request failed: {str(e)}")
            return []
        except (KeyError, IndexError, ValueError, TypeError) as e:
            logging.error(f"Error parsing SiliconFlow API response: {str(e)}")
            return []

def create_embedding_adapter(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str
) -> BaseEmbeddingAdapter:
    """
    工厂函数：根据 interface_format 返回不同的 embedding 适配器实例
    """
    fmt = interface_format.strip().lower()
    if fmt == "openai":
        return OpenAIEmbeddingAdapter(api_key, base_url, model_name)
    elif fmt == "azure openai":
        return AzureOpenAIEmbeddingAdapter(api_key, base_url, model_name)
    elif fmt == "ollama":
        return OllamaEmbeddingAdapter(model_name, base_url)
    elif fmt == "ml studio":
        return MLStudioEmbeddingAdapter(api_key, base_url, model_name)
    elif fmt == "gemini":
        return GeminiEmbeddingAdapter(api_key, model_name, base_url)
    elif fmt == "siliconflow":
        return SiliconFlowEmbeddingAdapter(api_key, base_url, model_name)
    else:
        raise ValueError(f"Unknown embedding interface_format: {interface_format}")
</file>

<file path="novel_generator/architecture.py">
#novel_generator/architecture.py
# -*- coding: utf-8 -*-
"""
小说总体架构生成（Novel_architecture_generate 及相关辅助函数）
"""
import os
import json
import logging
import traceback
from novel_generator.common import invoke_with_cleaning
from llm_adapters import create_llm_adapter
from prompt_definitions import (
    core_seed_prompt,
    character_dynamics_prompt,
    world_building_prompt,
    plot_architecture_prompt,
    create_character_state_prompt
)
logging.basicConfig(
    filename='app.log',      # 日志文件名
    filemode='a',            # 追加模式（'w' 会覆盖）
    level=logging.INFO,      # 记录 INFO 及以上级别的日志
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
from utils import clear_file_content, save_string_to_txt

def load_partial_architecture_data(filepath: str) -> dict:
    """
    从 filepath 下的 partial_architecture.json 读取已有的阶段性数据。
    如果文件不存在或无法解析，返回空 dict。
    """
    partial_file = os.path.join(filepath, "partial_architecture.json")
    if not os.path.exists(partial_file):
        return {}
    try:
        with open(partial_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except Exception as e:
        logging.warning(f"Failed to load partial_architecture.json: {e}")
        return {}

def save_partial_architecture_data(filepath: str, data: dict):
    """
    将阶段性数据写入 partial_architecture.json。
    """
    partial_file = os.path.join(filepath, "partial_architecture.json")
    try:
        with open(partial_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.warning(f"Failed to save partial_architecture.json: {e}")

def Novel_architecture_generate(
    interface_format: str,
    api_key: str,
    base_url: str,
    llm_model: str,
    topic: str,
    genre: str,
    number_of_chapters: int,
    word_number: int,
    filepath: str,
    user_guidance: str = "",  # 新增参数
    temperature: float = 0.7,
    max_tokens: int = 2048,
    timeout: int = 600
) -> None:
    """
    依次调用:
      1. core_seed_prompt
      2. character_dynamics_prompt
      3. world_building_prompt
      4. plot_architecture_prompt
    若在中间任何一步报错且重试多次失败，则将已经生成的内容写入 partial_architecture.json 并退出；
    下次调用时可从该步骤继续。
    最终输出 Novel_architecture.txt

    新增：
    - 在完成角色动力学设定后，依据该角色体系，使用 create_character_state_prompt 生成初始角色状态表，
      并存储到 character_state.txt，后续维护更新。
    """
    os.makedirs(filepath, exist_ok=True)
    partial_data = load_partial_architecture_data(filepath)
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=llm_model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    # Step1: 核心种子
    if "core_seed_result" not in partial_data:
        logging.info("Step1: Generating core_seed_prompt (核心种子) ...")
        prompt_core = core_seed_prompt.format(
            topic=topic,
            genre=genre,
            number_of_chapters=number_of_chapters,
            word_number=word_number,
            user_guidance=user_guidance  # 修复：添加内容指导
        )
        core_seed_result = invoke_with_cleaning(llm_adapter, prompt_core)
        if not core_seed_result.strip():
            logging.warning("core_seed_prompt generation failed and returned empty.")
            save_partial_architecture_data(filepath, partial_data)
            return
        partial_data["core_seed_result"] = core_seed_result
        save_partial_architecture_data(filepath, partial_data)
    else:
        logging.info("Step1 already done. Skipping...")
    # Step2: 角色动力学
    if "character_dynamics_result" not in partial_data:
        logging.info("Step2: Generating character_dynamics_prompt ...")
        prompt_character = character_dynamics_prompt.format(
            core_seed=partial_data["core_seed_result"].strip(),
            user_guidance=user_guidance
        )
        character_dynamics_result = invoke_with_cleaning(llm_adapter, prompt_character)
        if not character_dynamics_result.strip():
            logging.warning("character_dynamics_prompt generation failed.")
            save_partial_architecture_data(filepath, partial_data)
            return
        partial_data["character_dynamics_result"] = character_dynamics_result
        save_partial_architecture_data(filepath, partial_data)
    else:
        logging.info("Step2 already done. Skipping...")
    # 生成初始角色状态
    if "character_dynamics_result" in partial_data and "character_state_result" not in partial_data:
        logging.info("Generating initial character state from character dynamics ...")
        prompt_char_state_init = create_character_state_prompt.format(
            character_dynamics=partial_data["character_dynamics_result"].strip()
        )
        character_state_init = invoke_with_cleaning(llm_adapter, prompt_char_state_init)
        if not character_state_init.strip():
            logging.warning("create_character_state_prompt generation failed.")
            save_partial_architecture_data(filepath, partial_data)
            return
        partial_data["character_state_result"] = character_state_init
        character_state_file = os.path.join(filepath, "character_state.txt")
        clear_file_content(character_state_file)
        save_string_to_txt(character_state_init, character_state_file)
        save_partial_architecture_data(filepath, partial_data)
        logging.info("Initial character state created and saved.")
    # Step3: 世界观
    if "world_building_result" not in partial_data:
        logging.info("Step3: Generating world_building_prompt ...")
        prompt_world = world_building_prompt.format(
            core_seed=partial_data["core_seed_result"].strip(),
            user_guidance=user_guidance  # 修复：添加用户指导
        )
        world_building_result = invoke_with_cleaning(llm_adapter, prompt_world)
        if not world_building_result.strip():
            logging.warning("world_building_prompt generation failed.")
            save_partial_architecture_data(filepath, partial_data)
            return
        partial_data["world_building_result"] = world_building_result
        save_partial_architecture_data(filepath, partial_data)
    else:
        logging.info("Step3 already done. Skipping...")
    # Step4: 三幕式情节
    if "plot_arch_result" not in partial_data:
        logging.info("Step4: Generating plot_architecture_prompt ...")
        prompt_plot = plot_architecture_prompt.format(
            core_seed=partial_data["core_seed_result"].strip(),
            character_dynamics=partial_data["character_dynamics_result"].strip(),
            world_building=partial_data["world_building_result"].strip(),
            user_guidance=user_guidance  # 修复：添加用户指导
        )
        plot_arch_result = invoke_with_cleaning(llm_adapter, prompt_plot)
        if not plot_arch_result.strip():
            logging.warning("plot_architecture_prompt generation failed.")
            save_partial_architecture_data(filepath, partial_data)
            return
        partial_data["plot_arch_result"] = plot_arch_result
        save_partial_architecture_data(filepath, partial_data)
    else:
        logging.info("Step4 already done. Skipping...")

    core_seed_result = partial_data["core_seed_result"]
    character_dynamics_result = partial_data["character_dynamics_result"]
    world_building_result = partial_data["world_building_result"]
    plot_arch_result = partial_data["plot_arch_result"]

    final_content = (
        "#=== 0) 小说设定 ===\n"
        f"主题：{topic},类型：{genre},篇幅：约{number_of_chapters}章（每章{word_number}字）\n\n"
        "#=== 1) 核心种子 ===\n"
        f"{core_seed_result}\n\n"
        "#=== 2) 角色动力学 ===\n"
        f"{character_dynamics_result}\n\n"
        "#=== 3) 世界观 ===\n"
        f"{world_building_result}\n\n"
        "#=== 4) 三幕式情节架构 ===\n"
        f"{plot_arch_result}\n"
    )

    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    clear_file_content(arch_file)
    save_string_to_txt(final_content, arch_file)
    logging.info("Novel_architecture.txt has been generated successfully.")

    partial_arch_file = os.path.join(filepath, "partial_architecture.json")
    if os.path.exists(partial_arch_file):
        os.remove(partial_arch_file)
        logging.info("partial_architecture.json removed (all steps completed).")
</file>

<file path="novel_generator/blueprint.py">
#novel_generator/blueprint.py
# -*- coding: utf-8 -*-
"""
章节蓝图生成（Chapter_blueprint_generate 及辅助函数）
"""
import os
import re
import logging
from novel_generator.common import invoke_with_cleaning
from llm_adapters import create_llm_adapter
from prompt_definitions import chapter_blueprint_prompt, chunked_chapter_blueprint_prompt, continue_chapter_blueprint_prompt
from utils import read_file, clear_file_content, save_string_to_txt
logging.basicConfig(
    filename='app.log',      # 日志文件名
    filemode='a',            # 追加模式（'w' 会覆盖）
    level=logging.INFO,      # 记录 INFO 及以上级别的日志
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
def compute_chunk_size(number_of_chapters: int, max_tokens: int) -> int:
    """
    基于“每章约100 tokens”的粗略估算，
    再结合当前max_tokens，计算分块大小：
      chunk_size = (floor(max_tokens/100/10)*10) - 10
    并确保 chunk_size 不会小于1或大于实际章节数。
    """
    tokens_per_chapter = 200.0
    ratio = max_tokens / tokens_per_chapter
    ratio_rounded_to_10 = int(ratio // 10) * 10
    chunk_size = ratio_rounded_to_10 - 10
    if chunk_size < 1:
        chunk_size = 1
    if chunk_size > number_of_chapters:
        chunk_size = number_of_chapters
    return chunk_size

def limit_chapter_blueprint(blueprint_text: str, limit_chapters: int = 100) -> str:
    """
    从已有章节目录中只取最近的 limit_chapters 章，以避免 prompt 超长。
    """
    pattern = r"(第\s*\d+\s*章.*?)(?=第\s*\d+\s*章|$)"
    chapters = re.findall(pattern, blueprint_text, flags=re.DOTALL)
    if not chapters:
        return blueprint_text
    if len(chapters) <= limit_chapters:
        return blueprint_text
    selected = chapters[-limit_chapters:]
    return "\n\n".join(selected).strip()

def Chapter_blueprint_generate(
    interface_format: str,
    api_key: str,
    base_url: str,
    llm_model: str,
    filepath: str,
    number_of_chapters: int,
    user_guidance: str = "",  # 新增参数
    temperature: float = 0.7,
    max_tokens: int = 4096,
    timeout: int = 600
) -> None:
    """
    若 Novel_directory.txt 已存在且内容非空，则表示可能是之前的部分生成结果；
      解析其中已有的章节数，从下一个章节继续分块生成；
      对于已有章节目录，传入时仅保留最近100章目录，避免prompt过长。
    否则：
      - 若章节数 <= chunk_size，直接一次性生成
      - 若章节数 > chunk_size，进行分块生成
    生成完成后输出至 Novel_directory.txt。
    """
    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    if not os.path.exists(arch_file):
        logging.warning("Novel_architecture.txt not found. Please generate architecture first.")
        return

    architecture_text = read_file(arch_file).strip()
    if not architecture_text:
        logging.warning("Novel_architecture.txt is empty.")
        return

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=llm_model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )

    filename_dir = os.path.join(filepath, "Novel_directory.txt")
    if not os.path.exists(filename_dir):
        open(filename_dir, "w", encoding="utf-8").close()

    existing_blueprint = read_file(filename_dir).strip()
    chunk_size = compute_chunk_size(number_of_chapters, max_tokens)
    logging.info(f"Number of chapters = {number_of_chapters}, computed chunk_size = {chunk_size}.")

    if existing_blueprint:
        logging.info("Detected existing blueprint content. Will resume chunked generation from that point.")
        pattern = r"第\s*(\d+)\s*章"
        existing_chapter_numbers = re.findall(pattern, existing_blueprint)
        existing_chapter_numbers = [int(x) for x in existing_chapter_numbers if x.isdigit()]
        max_existing_chap = max(existing_chapter_numbers) if existing_chapter_numbers else 0
        logging.info(f"Existing blueprint indicates up to chapter {max_existing_chap} has been generated.")
        final_blueprint = existing_blueprint
        current_start = max_existing_chap + 1
        while current_start <= number_of_chapters:
            current_end = min(current_start + chunk_size - 1, number_of_chapters)
            limited_blueprint = limit_chapter_blueprint(final_blueprint, 100)
            chunk_prompt = chunked_chapter_blueprint_prompt.format(
                novel_architecture=architecture_text,
                chapter_list=limited_blueprint,
                number_of_chapters=number_of_chapters,
                n=current_start,
                m=current_end,
                user_guidance=user_guidance  # 新增参数
            )
            logging.info(f"Generating chapters [{current_start}..{current_end}] in a chunk...")
            chunk_result = invoke_with_cleaning(llm_adapter, chunk_prompt)
            if not chunk_result.strip():
                logging.warning(f"Chunk generation for chapters [{current_start}..{current_end}] is empty.")
                clear_file_content(filename_dir)
                save_string_to_txt(final_blueprint.strip(), filename_dir)
                return
            final_blueprint += "\n\n" + chunk_result.strip()
            clear_file_content(filename_dir)
            save_string_to_txt(final_blueprint.strip(), filename_dir)
            current_start = current_end + 1

        logging.info("All chapters blueprint have been generated (resumed chunked).")
        return

    if chunk_size >= number_of_chapters:
        prompt = chapter_blueprint_prompt.format(
            novel_architecture=architecture_text,
            number_of_chapters=number_of_chapters,
            user_guidance=user_guidance  # 新增参数
        )
        blueprint_text = invoke_with_cleaning(llm_adapter, prompt)
        if not blueprint_text.strip():
            logging.warning("Chapter blueprint generation result is empty.")
            return

        clear_file_content(filename_dir)
        save_string_to_txt(blueprint_text, filename_dir)
        logging.info("Novel_directory.txt (chapter blueprint) has been generated successfully (single-shot).")
        return

    logging.info("Will generate chapter blueprint in chunked mode from scratch.")
    final_blueprint = ""
    current_start = 1
    while current_start <= number_of_chapters:
        current_end = min(current_start + chunk_size - 1, number_of_chapters)
        limited_blueprint = limit_chapter_blueprint(final_blueprint, 100)
        chunk_prompt = chunked_chapter_blueprint_prompt.format(
            novel_architecture=architecture_text,
            chapter_list=limited_blueprint,
            number_of_chapters=number_of_chapters,
            n=current_start,
            m=current_end,
            user_guidance=user_guidance  # 新增参数
        )
        logging.info(f"Generating chapters [{current_start}..{current_end}] in a chunk...")
        chunk_result = invoke_with_cleaning(llm_adapter, chunk_prompt)
        if not chunk_result.strip():
            logging.warning(f"Chunk generation for chapters [{current_start}..{current_end}] is empty.")
            clear_file_content(filename_dir)
            save_string_to_txt(final_blueprint.strip(), filename_dir)
            return
        if final_blueprint.strip():
            final_blueprint += "\n\n" + chunk_result.strip()
        else:
            final_blueprint = chunk_result.strip()
        clear_file_content(filename_dir)
        save_string_to_txt(final_blueprint.strip(), filename_dir)
        current_start = current_end + 1

    logging.info("Novel_directory.txt (chapter blueprint) has been generated successfully (chunked).")


# 新增函数：根据现有目录信息生成后续章节目录
def continue_chapter_blueprint(
    interface_format: str,
    api_key: str,
    base_url: str,
    llm_model: str,
    filepath: str,
    start_chapter: int,
    end_chapter: int,
    user_guidance: str = "",
    temperature: float = 0.7,
    max_tokens: int = 4096,
    timeout: int = 600
) -> str:
    """
    根据现有架构和目录信息生成后续章节的目录
    """
    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    if not os.path.exists(arch_file):
        logging.warning("Novel_architecture.txt not found. Please generate architecture first.")
        return ""

    architecture_text = read_file(arch_file).strip()
    if not architecture_text:
        logging.warning("Novel_architecture.txt is empty.")
        return ""

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=llm_model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )

    # 读取现有目录
    filename_dir = os.path.join(filepath, "Novel_directory.txt")
    existing_blueprint = ""
    if os.path.exists(filename_dir):
        existing_blueprint = read_file(filename_dir).strip()

    # 限制现有目录长度，避免prompt过长
    limited_blueprint = limit_chapter_blueprint(existing_blueprint, 100)

    # 计算需要生成的章节数
    number_of_chapters = end_chapter - start_chapter + 1

    # 构建提示词
    prompt = continue_chapter_blueprint_prompt.format(
        novel_architecture=architecture_text,
        existing_chapters=limited_blueprint,
        start_chapter=start_chapter,
        end_chapter=end_chapter,
        number_of_chapters=number_of_chapters,
        user_guidance=user_guidance
    )

    logging.info(f"Generating chapters {start_chapter} to {end_chapter} based on existing blueprint...")
    result = invoke_with_cleaning(llm_adapter, prompt)
    
    if not result.strip():
        logging.warning(f"Generation for chapters {start_chapter} to {end_chapter} returned empty.")
        return ""

    # 如果已有目录，将新生成的内容附加到现有目录之后
    if existing_blueprint:
        final_blueprint = existing_blueprint + "\n\n" + result.strip()
    else:
        final_blueprint = result.strip()

    # 保存到文件
    clear_file_content(filename_dir)
    save_string_to_txt(final_blueprint.strip(), filename_dir)

    return result.strip()
</file>

<file path="tooltips.py">
# tooltips.py
# -*- coding: utf-8 -*-

tooltips = {
    "api_key": "在这里填写你的API Key。如果使用OpenAI官方接口，请在 https://platform.openai.com/account/api-keys 获取。",
    "base_url": "模型的接口地址。若使用OpenAI官方：https://api.openai.com/v1。若使用Ollama本地部署，则类似 http://localhost:11434/v1。调用Gemini模型则无需填写。",
    "interface_format": "指定LLM接口兼容格式，可选DeepSeek、OpenAI、Ollama、ML Studio、Gemini等。\n\n注意："+
                        "OpenAI 兼容是指的可以通过该标准请求的任何接口，不是只允许使用api.openai.com接口\n"+
                        "例如Ollama接口格式也兼容OpenAI，可以无需修改直接使用\n"+
                        "ML Studio接口格式与OpenAI接口格式也一致。",
    "model_name": "要使用的模型名称，例如deepseek-reasoner、gpt-4o等。如果是Ollama等，请填写你下载好的本地模型名。",
    "temperature": "生成文本的随机度。数值越大越具有发散性，越小越严谨。",
    "max_tokens": "限制单次生成的最大Token数。范围1~100000，请根据模型上下文及需求填写合适值。\n"+
                  "以下是一些常见模型的最大值：\n"+
                  "o1：100,000\n"+
                  "o1-mini：65,536\n"+
                  "gpt-4o：16384\n"+
                  "gpt-4o-mini：16384\n"+
                  "deepseek-reasoner：8192\n"+
                  "deepseek-chat：4096\n",
    "embedding_api_key": "调用Embedding模型时所需的API Key。",
    "embedding_interface_format": "Embedding模型接口风格，比如OpenAI或Ollama。",
    "embedding_url": "Embedding模型接口地址。",
    "embedding_model_name": "Embedding模型名称，如text-embedding-ada-002。",
    "embedding_retrieval_k": "向量检索时返回的Top-K结果数量。",
    "topic": "小说的大致主题或主要故事背景描述。",
    "genre": "小说的题材类型，如玄幻、都市、科幻等。",
    "num_chapters": "小说期望的章节总数。",
    "word_number": "每章的目标字数。",
    "filepath": "生成文件存储的根目录路径。所有txt文件、向量库等放在该目录下。",
    "chapter_num": "当前正在处理的章节号，用于生成草稿或定稿操作。",
    "user_guidance": "为本章提供额外指令或写作引导。\n\n特别建议在此填写【作者隐式设定】——你脑中已有但未写入任何文件的设定，例如：\n· 身份映射：前文「被杀的那兽人」实为血煞\n· 符号含义：兽人身上的纹路代表古尔氏族派系，非实验导致\n· 人物关系或动机的补充说明\n这些内容会注入知识库过滤与正文写作，防止大模型自行推断或胡编。",
    "characters_involved": "本章需要重点描写或影响剧情的角色名单。",
    "key_items": "本章中涉及的重要物品、道具或线索。",
    "scene_location": "本章的主要场景和地点。",
    "time_constraint": "本章的时间限制或紧迫感。",
    "api_key_desc": "填写API Key",
    "base_url_desc": "填写接口地址",
    "model_name_desc": "填写模型名称",
    "temperature_desc": "填写温度值（0-1）",
    "max_tokens_desc": "填写最大Token数",
    "timeout_desc": "填写超时时间（秒）",
    "top_p_desc": "填写Top-P值（0-1）",
    "frequency_penalty_desc": "填写频率惩罚值（-2到2）",
    "presence_penalty_desc": "填写存在惩罚值（-2到2）",
    "num_chapters_desc": "填写章节数",
    "word_number_desc": "填写每章字数",
    "filepath_desc": "填写保存路径",
    "chapter_num_desc": "填写章节号",
    "retrieval_k_desc": "填写检索数量K值",
    "embedding_api_key_desc": "填写Embedding API Key",
    "embedding_base_url_desc": "填写Embedding接口地址",
    "embedding_model_name_desc": "填写Embedding模型名",
    "topic_desc": "填写主题",
    "genre_desc": "填写类型",
    "user_guidance_desc": "填写内容指导",
    "characters_involved_desc": "填写涉及角色",
    "key_items_desc": "填写关键物品",
    "scene_location_desc": "填写场景地点",
    "time_constraint_desc": "填写时间限制",
    "proxy_enabled": "是否启用代理",
    "proxy_url": "代理服务器地址，例如：http://127.0.0.1:7890",
    "proxy_desc": "填写代理地址",
    "interface_format_desc": "选择接口格式",
    "embedding_interface_format_desc": "选择Embedding接口格式",
    # 配置选择相关
    "architecture_llm_config": "用于生成小说架构的大模型",
    "chapter_outline_llm_config": "用于生成章节目录的大模型",
    "prompt_draft_llm_config": "用于生成章节草稿的大模型",
    "refine_logic_llm_config": "用于逻辑分析和选角的大模型",
    "final_chapter_llm_config": "用于定稿章节的大模型",
    "consistency_review_llm_config": "用于一致性审校的大模型",
    "directory_continuation_llm_config": "用于续写目录的大模型",  # 新增
    "directory_refinement_llm_config": "用于微调目录的大模型"     # 新增
}
</file>

<file path="ui/directory_tab.py">
# ui/directory_tab.py
# -*- coding: utf-8 -*-
import os
import customtkinter as ctk
from tkinter import messagebox
from utils import read_file, save_string_to_txt, clear_file_content
from ui.context_menu import TextWidgetContextMenu

def build_directory_tab(self):
    self.directory_tab = self.tabview.add("Chapter Blueprint")
    self.directory_tab.rowconfigure(0, weight=0)
    self.directory_tab.rowconfigure(1, weight=1)
    self.directory_tab.columnconfigure(0, weight=1)

    # 创建顶部按钮容器框架，方便管理
    top_frame = ctk.CTkFrame(self.directory_tab, fg_color="transparent")
    top_frame.grid(row=0, column=0, padx=5, pady=5, sticky="ew")
    
    # 加载按钮
    load_btn = ctk.CTkButton(top_frame, text="加载 Novel_directory.txt", command=self.load_chapter_blueprint, font=("Microsoft YaHei", 12))
    load_btn.pack(side="left", padx=5)

    # 字数统计
    self.directory_word_count_label = ctk.CTkLabel(top_frame, text="字数：0", font=("Microsoft YaHei", 12))
    self.directory_word_count_label.pack(side="left", padx=10)

    # 保存按钮
    save_btn = ctk.CTkButton(top_frame, text="保存修改", command=self.save_chapter_blueprint, font=("Microsoft YaHei", 12))
    save_btn.pack(side="right", padx=5)

    # === 新增：生成后续目录按钮 ===
    continue_btn = ctk.CTkButton(top_frame, text="📚 续写目录 (AI)", command=self.continue_directory_ui, font=("Microsoft YaHei", 12), fg_color="#3498DB")
    continue_btn.pack(side="right", padx=5)
    # ========================

    # === 新增：微调目录按钮 ===
    refine_btn = ctk.CTkButton(top_frame, text="✨ 微调目录 (AI)", command=self.refine_directory_card_ui, font=("Microsoft YaHei", 12), fg_color="#E67E22")
    refine_btn.pack(side="right", padx=5)
    # ========================

    self.directory_text = ctk.CTkTextbox(self.directory_tab, wrap="word", font=("Microsoft YaHei", 12))
    
    def update_word_count(event=None):
        text = self.directory_text.get("0.0", "end")
        count = len(text) - 1
        self.directory_word_count_label.configure(text=f"字数：{count}")
    
    self.directory_text.bind("<KeyRelease>", update_word_count)
    self.directory_text.bind("<ButtonRelease>", update_word_count)
    TextWidgetContextMenu(self.directory_text)
    self.directory_text.grid(row=1, column=0, sticky="nsew", padx=5, pady=5)

def load_chapter_blueprint(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    filename = os.path.join(filepath, "Novel_directory.txt")
    content = read_file(filename)
    self.directory_text.delete("0.0", "end")
    self.directory_text.insert("0.0", content)
    self.log("已加载 Novel_directory.txt 内容到编辑区。")

def save_chapter_blueprint(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先设置保存文件路径")
        return
    content = self.directory_text.get("0.0", "end").strip()
    filename = os.path.join(filepath, "Novel_directory.txt")
    clear_file_content(filename)
    save_string_to_txt(content, filename)
    self.log("已保存对 Novel_directory.txt 的修改。")
</file>

<file path="novel_generator/common.py">
#novel_generator/common.py
# -*- coding: utf-8 -*-
"""
通用重试、清洗、日志工具
"""
import logging
import re
import time
import traceback
logging.basicConfig(
    filename='app.log',      # 日志文件名
    filemode='a',            # 追加模式（'w' 会覆盖）
    level=logging.INFO,      # 记录 INFO 及以上级别的日志
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
def call_with_retry(func, max_retries=3, sleep_time=2, fallback_return=None, **kwargs):
    """
    通用的重试机制封装。
    :param func: 要执行的函数
    :param max_retries: 最大重试次数
    :param sleep_time: 重试前的等待秒数
    :param fallback_return: 如果多次重试仍失败时的返回值
    :param kwargs: 传给func的命名参数
    :return: func的结果，若失败则返回 fallback_return
    """
    for attempt in range(1, max_retries + 1):
        try:
            return func(**kwargs)
        except Exception as e:
            logging.warning(f"[call_with_retry] Attempt {attempt} failed with error: {e}")
            traceback.print_exc()
            if attempt < max_retries:
                time.sleep(sleep_time)
            else:
                logging.error("Max retries reached, returning fallback_return.")
                return fallback_return

def remove_think_tags(text: str) -> str:
    """移除 <think>...</think> 包裹的内容"""
    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

def debug_log(prompt: str, response_content: str):
    logging.info(
        f"\n[#########################################  Prompt  #########################################]\n{prompt}\n"
    )
    logging.info(
        f"\n[######################################### Response #########################################]\n{response_content}\n"
    )

def _is_connection_error(exc: Exception) -> bool:
    """判断是否为网络/SSL 连接类错误，这类错误适合重试"""
    err_str = str(exc).lower()
    err_type = type(exc).__name__
    connection_indicators = [
        "ssl", "connection", "connect", "eof", "protocol",
        "timeout", "unreachable", "refused", "proxy"
    ]
    return (
        "connection" in err_type.lower()
        or "ssl" in err_str
        or "connect" in err_str
        or "eof" in err_str
        or any(ind in err_str for ind in connection_indicators)
    )


def invoke_with_cleaning(llm_adapter, prompt: str, max_retries: int = 3) -> str:
    """调用 LLM 并清理返回结果，对网络/SSL 错误进行指数退避重试"""
    print("\n" + "="*50)
    print("发送到 LLM 的提示词:")
    print("-"*50)
    print(prompt)
    print("="*50 + "\n")

    result = ""
    retry_count = 0
    # 网络/SSL 错误时增加重试次数和等待
    effective_retries = max_retries

    while retry_count < effective_retries:
        try:
            result = llm_adapter.invoke(prompt)
            print("\n" + "="*50)
            print("LLM 返回的内容:")
            print("-"*50)
            print(result)
            print("="*50 + "\n")

            result = result.replace("```", "").strip()
            if result:
                return result
            retry_count += 1
        except Exception as e:
            is_conn = _is_connection_error(e)
            if is_conn:
                effective_retries = max(5, max_retries + 2)  # 连接错误多试几次
                wait = min(60, 3 * (2 ** retry_count))  # 指数退避：3, 6, 12, 24, 48 秒
                print(f"网络/SSL 连接失败 ({retry_count + 1}/{effective_retries})，{wait} 秒后重试...")
                if retry_count + 1 < effective_retries:
                    time.sleep(wait)
            else:
                print(f"调用失败 ({retry_count + 1}/{effective_retries}): {str(e)}")

            retry_count += 1
            if retry_count >= effective_retries:
                if is_conn:
                    raise ConnectionError(
                        f"多次连接失败: {e}\n\n"
                        "可能原因：代理/防火墙、SSL 证书、网络不稳定。\n"
                        "建议：检查代理设置、关闭 VPN 后重试，或稍后再试。"
                    ) from e
                raise e

    return result
</file>

<file path="requirements.txt">
aiohappyeyeballs==2.6.1
aiohttp==3.12.15
aiosignal==1.4.0
annotated-types==0.7.0
anyio==4.10.0
attrs==25.3.0
azure-ai-inference==1.0.0b9
azure-core==1.35.0
backoff==2.2.1
bcrypt==4.3.0
build==1.3.0
cachetools==5.5.2
certifi==2025.8.3
charset-normalizer==3.4.3
chromadb==1.0.20
click==8.2.1
colorama==0.4.6
coloredlogs==15.0.1
customtkinter==5.2.2
darkdetect==0.8.0
dataclasses-json==0.6.7
distro==1.9.0
durationpy==0.10
filelock==3.20.1
flatbuffers==25.2.10
frozenlist==1.7.0
fsspec==2025.9.0
google-ai-generativelanguage==0.6.15
google-api-core==2.25.1
google-api-python-client==2.181.0
google-auth==2.40.3
google-auth-httplib2==0.2.0
google-generativeai==0.8.5
googleapis-common-protos==1.70.0
greenlet==3.2.4
grpcio==1.74.0
grpcio-status==1.71.2
h11==0.16.0
httpcore==1.0.9
httplib2==0.30.0
httptools==0.6.4
httpx==0.28.1
httpx-sse==0.4.1
huggingface-hub==0.34.4
humanfriendly==10.0
idna==3.10
importlib_metadata==8.7.0
importlib_resources==6.5.2
isodate==0.7.2
Jinja2==3.1.6
jiter==0.10.0
joblib==1.5.2
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.25.1
jsonschema-specifications==2025.4.1
keybert==0.9.0
kubernetes==33.1.0
langchain==0.3.27
langchain-chroma==0.2.5
langchain-community==0.3.29
langchain-core==0.3.80
langchain-openai==0.3.32
langchain-text-splitters==0.3.11
langsmith==0.4.25
markdown-it-py==4.0.0
MarkupSafe==3.0.2
marshmallow==3.26.1
mdurl==0.1.2
mmh3==5.2.0
mpmath==1.3.0
multidict==6.6.4
mypy_extensions==1.1.0
networkx==3.5
nltk==3.9.1
numpy==2.3.2
oauthlib==3.3.1
onnxruntime==1.22.1
openai==1.106.1
opentelemetry-api==1.36.0
opentelemetry-exporter-otlp-proto-common==1.36.0
opentelemetry-exporter-otlp-proto-grpc==1.36.0
opentelemetry-proto==1.36.0
opentelemetry-sdk==1.36.0
opentelemetry-semantic-conventions==0.57b0
orjson==3.11.3
overrides==7.7.0
packaging==25.0
pillow==11.3.0
posthog==5.4.0
propcache==0.3.2
proto-plus==1.26.1
protobuf==5.29.5
pyasn1==0.6.1
pyasn1_modules==0.4.2
pybase64==1.4.2
pydantic==2.11.7
pydantic-settings==2.10.1
pydantic_core==2.33.2
Pygments==2.19.2
pyparsing==3.2.3
PyPika==0.48.9
pyproject_hooks==1.2.0
pyreadline3==3.5.4
python-dateutil==2.9.0.post0
python-dotenv==1.1.1
PyYAML==6.0.2
referencing==0.36.2
regex==2025.9.1
requests==2.32.5
requests-oauthlib==2.0.0
requests-toolbelt==1.0.0
rich==14.1.0
rpds-py==0.27.1
rsa==4.9.1
safetensors==0.6.2
scikit-learn==1.7.1
scipy==1.16.1
sentence-transformers==5.1.0
setuptools==80.9.0
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.43
sympy==1.14.0
tenacity==9.1.2
threadpoolctl==3.6.0
tiktoken==0.11.0
tokenizers==0.22.0
torch==2.8.0
tqdm==4.67.1
transformers==4.56.1
typer==0.17.4
typing-inspect==0.9.0
typing-inspection==0.4.1
typing_extensions==4.15.0
uritemplate==4.2.0
urllib3==2.6.0
uvicorn==0.35.0
watchfiles==1.1.0
websocket-client==1.8.0
websockets==15.0.1
wheel==0.45.1
yarl==1.20.1
zipp==3.23.0
zstandard==0.24.0
</file>

<file path="ui/novel_params_tab.py">
# ui/novel_params_tab.py
# -*- coding: utf-8 -*-
import customtkinter as ctk
from tkinter import filedialog, messagebox
from ui.context_menu import TextWidgetContextMenu
from tooltips import tooltips

def build_novel_params_area(self, start_row=1):
    self.params_frame = ctk.CTkScrollableFrame(self.right_frame, orientation="vertical")
    self.params_frame.grid(row=start_row, column=0, sticky="nsew", padx=5, pady=5)
    self.params_frame.columnconfigure(1, weight=1)

    # 1) 主题(Topic)
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="主题(Topic):", tooltip_key="topic", row=0, column=0, font=("Microsoft YaHei", 12), sticky="ne")
    self.topic_text = ctk.CTkTextbox(self.params_frame, height=80, wrap="word", font=("Microsoft YaHei", 12))
    TextWidgetContextMenu(self.topic_text)
    self.topic_text.grid(row=0, column=1, padx=5, pady=5, sticky="nsew")
    if hasattr(self, 'topic_default') and self.topic_default:
        self.topic_text.insert("0.0", self.topic_default)

    # 2) 类型(Genre)
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="类型(Genre):", tooltip_key="genre", row=1, column=0, font=("Microsoft YaHei", 12))
    genre_entry = ctk.CTkEntry(self.params_frame, textvariable=self.genre_var, font=("Microsoft YaHei", 12))
    genre_entry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")

    # 3) 章节数 & 每章字数
    row_for_chapter_and_word = 2
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="章节数 & 每章字数:", tooltip_key="num_chapters", row=row_for_chapter_and_word, column=0, font=("Microsoft YaHei", 12))
    chapter_word_frame = ctk.CTkFrame(self.params_frame)
    chapter_word_frame.grid(row=row_for_chapter_and_word, column=1, padx=5, pady=5, sticky="ew")
    chapter_word_frame.columnconfigure((0, 1, 2, 3), weight=0)
    num_chapters_label = ctk.CTkLabel(chapter_word_frame, text="章节数:", font=("Microsoft YaHei", 12))
    num_chapters_label.grid(row=0, column=0, padx=5, pady=5, sticky="e")
    num_chapters_entry = ctk.CTkEntry(chapter_word_frame, textvariable=self.num_chapters_var, width=60, font=("Microsoft YaHei", 12))
    num_chapters_entry.grid(row=0, column=1, padx=5, pady=5, sticky="w")
    word_number_label = ctk.CTkLabel(chapter_word_frame, text="每章字数:", font=("Microsoft YaHei", 12))
    word_number_label.grid(row=0, column=2, padx=(15, 5), pady=5, sticky="e")
    word_number_entry = ctk.CTkEntry(chapter_word_frame, textvariable=self.word_number_var, width=60, font=("Microsoft YaHei", 12))
    word_number_entry.grid(row=0, column=3, padx=5, pady=5, sticky="w")

    # 4) 保存路径
    row_fp = 3
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="保存路径:", tooltip_key="filepath", row=row_fp, column=0, font=("Microsoft YaHei", 12))
    self.filepath_frame = ctk.CTkFrame(self.params_frame)
    self.filepath_frame.grid(row=row_fp, column=1, padx=5, pady=5, sticky="nsew")
    self.filepath_frame.columnconfigure(0, weight=1)
    filepath_entry = ctk.CTkEntry(self.filepath_frame, textvariable=self.filepath_var, font=("Microsoft YaHei", 12))
    filepath_entry.grid(row=0, column=0, padx=5, pady=5, sticky="ew")
    browse_btn = ctk.CTkButton(self.filepath_frame, text="浏览...", command=self.browse_folder, width=60, font=("Microsoft YaHei", 12))
    browse_btn.grid(row=0, column=1, padx=5, pady=5, sticky="e")

    # 5) 章节号
    row_chap_num = 4
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="章节号:", tooltip_key="chapter_num", row=row_chap_num, column=0, font=("Microsoft YaHei", 12))
    chapter_num_entry = ctk.CTkEntry(self.params_frame, textvariable=self.chapter_num_var, width=80, font=("Microsoft YaHei", 12))
    chapter_num_entry.grid(row=row_chap_num, column=1, padx=5, pady=5, sticky="w")

    # 6) 内容指导
    row_user_guide = 5
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="内容指导:", tooltip_key="user_guidance", row=row_user_guide, column=0, font=("Microsoft YaHei", 12), sticky="ne")
    self.user_guide_text = ctk.CTkTextbox(self.params_frame, height=80, wrap="word", font=("Microsoft YaHei", 12))
    TextWidgetContextMenu(self.user_guide_text)
    self.user_guide_text.grid(row=row_user_guide, column=1, padx=5, pady=5, sticky="nsew")
    if hasattr(self, 'user_guidance_default') and self.user_guidance_default:
        self.user_guide_text.insert("0.0", self.user_guidance_default)

    # 7) 可选元素：核心人物/关键道具/空间坐标/时间压力
    row_idx = 6
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="核心人物:", tooltip_key="characters_involved", row=row_idx, column=0, font=("Microsoft YaHei", 12))
    
    # 核心人物输入框+按钮容器
    char_inv_frame = ctk.CTkFrame(self.params_frame)
    char_inv_frame.grid(row=row_idx, column=1, padx=5, pady=5, sticky="nsew")
    char_inv_frame.columnconfigure(0, weight=1)
    char_inv_frame.rowconfigure(0, weight=1)
    
    # 三行文本输入框
    self.char_inv_text = ctk.CTkTextbox(char_inv_frame, height=60, wrap="word", font=("Microsoft YaHei", 12))
    self.char_inv_text.grid(row=0, column=0, padx=(0,5), pady=5, sticky="nsew")
    if hasattr(self, 'characters_involved_var'):
        self.char_inv_text.insert("0.0", self.characters_involved_var.get())
    
    # 导入按钮
    import_btn = ctk.CTkButton(char_inv_frame, text="导入", width=60, 
                             command=self.show_character_import_window,
                             font=("Microsoft YaHei", 12))
    import_btn.grid(row=0, column=1, padx=(0,5), pady=5, sticky="e")
    row_idx += 1
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="关键道具:", tooltip_key="key_items", row=row_idx, column=0, font=("Microsoft YaHei", 12))
    key_items_entry = ctk.CTkEntry(self.params_frame, textvariable=self.key_items_var, font=("Microsoft YaHei", 12))
    key_items_entry.grid(row=row_idx, column=1, padx=5, pady=5, sticky="ew")
    row_idx += 1
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="空间坐标:", tooltip_key="scene_location", row=row_idx, column=0, font=("Microsoft YaHei", 12))
    scene_loc_entry = ctk.CTkEntry(self.params_frame, textvariable=self.scene_location_var, font=("Microsoft YaHei", 12))
    scene_loc_entry.grid(row=row_idx, column=1, padx=5, pady=5, sticky="ew")
    row_idx += 1
    create_label_with_help_for_novel_params(self, parent=self.params_frame, label_text="时间压力:", tooltip_key="time_constraint", row=row_idx, column=0, font=("Microsoft YaHei", 12))
    time_const_entry = ctk.CTkEntry(self.params_frame, textvariable=self.time_constraint_var, font=("Microsoft YaHei", 12))
    time_const_entry.grid(row=row_idx, column=1, padx=5, pady=5, sticky="ew")

def build_optional_buttons_area(self, start_row=2):
    self.optional_btn_frame = ctk.CTkFrame(self.right_frame)
    self.optional_btn_frame.grid(row=start_row, column=0, sticky="ew", padx=5, pady=5)
    # 配置列权重，确保按钮均匀分布
    self.optional_btn_frame.columnconfigure((0, 1, 2, 3, 4), weight=1)

    # 第一行现有的按钮 (保持不变)
    self.btn_check_consistency = ctk.CTkButton(
        self.optional_btn_frame, text="一致性审校", command=self.do_consistency_check, 
        font=("Microsoft YaHei", 12), width=100
    )
    self.btn_check_consistency.grid(row=0, column=0, padx=5, pady=5, sticky="ew")

    self.btn_import_knowledge = ctk.CTkButton(
        self.optional_btn_frame, text="导入知识库", command=self.import_knowledge_handler,
        font=("Microsoft YaHei", 12), width=100
    )
    self.btn_import_knowledge.grid(row=0, column=1, padx=5, pady=5, sticky="ew")

    self.btn_clear_vectorstore = ctk.CTkButton(
        self.optional_btn_frame, text="清空向量库", fg_color="red", 
        command=self.clear_vectorstore_handler, font=("Microsoft YaHei", 12), width=100
    )
    self.btn_clear_vectorstore.grid(row=0, column=2, padx=5, pady=5, sticky="ew")

    self.plot_arcs_btn = ctk.CTkButton(
        self.optional_btn_frame, text="查看剧情要点", command=self.show_plot_arcs_ui,
        font=("Microsoft YaHei", 12), width=100
    )
    self.plot_arcs_btn.grid(row=0, column=3, padx=5, pady=5, sticky="ew")

    self.role_library_btn = ctk.CTkButton(
        self.optional_btn_frame, text="角色库", command=self.show_role_library,
        font=("Microsoft YaHei", 12), width=100
    )
    self.role_library_btn.grid(row=0, column=4, padx=5, pady=5, sticky="ew")

    # 伏笔库按钮 (改为 grid 布局，位置 1,0)
    self.btn_foreshadow = ctk.CTkButton(
        self.optional_btn_frame, 
        text="🔍 查看伏笔库", 
        command=self.show_foreshadowing_records_ui,
        fg_color="#8E44AD", 
        font=("Microsoft YaHei", 12)
    )
    # 将 columnspan 改为 2 或 3，留出空间给新按钮
    self.btn_foreshadow.grid(row=1, column=0, columnspan=2, padx=5, pady=5, sticky="ew")

    # === [新增] 全书问答按钮 ===
    self.btn_qa = ctk.CTkButton(
        self.optional_btn_frame, 
        text="📚 全书智能问答", 
        command=self.show_novel_qa_ui, # 绑定刚才写的函数
        fg_color="#16A085", # 青绿色，区分度高
        font=("Microsoft YaHei", 12)
    )
    # 放在伏笔库旁边
    self.btn_qa.grid(row=1, column=2, columnspan=3, padx=5, pady=5, sticky="ew")

def create_label_with_help_for_novel_params(self, parent, label_text, tooltip_key, row, column, font=None, sticky="e", padx=5, pady=5):
    frame = ctk.CTkFrame(parent)
    frame.grid(row=row, column=column, padx=padx, pady=pady, sticky=sticky)
    frame.columnconfigure(0, weight=0)
    label = ctk.CTkLabel(frame, text=label_text, font=font)
    label.pack(side="left")
    btn = ctk.CTkButton(frame, text="?", width=22, height=22, font=("Microsoft YaHei", 10),
                        command=lambda: messagebox.showinfo("参数说明", tooltips.get(tooltip_key, "暂无说明")))
    btn.pack(side="left", padx=3)
    return frame
</file>

<file path="ui/role_library.py">
# ui/role_library.py
import os
import tkinter as tk
from tkinter import filedialog
import shutil
import re
import customtkinter as ctk
from tkinter import messagebox, BooleanVar
from customtkinter import CTkScrollableFrame, CTkTextbox, END
from utils import read_file, save_string_to_txt  # 导入 utils 中的函数
from novel_generator.common import invoke_with_cleaning  # 新增导入
from prompt_definitions import Character_Import_Prompt

DEFAULT_FONT = ("Microsoft YaHei", 12)

class RoleLibrary:
    def __init__(self, master, save_path, llm_adapter):  # 新增llm_adapter参数
        self.master = master
        self.save_path = os.path.join(save_path, "角色库")
        self.selected_category = None
        self.current_roles = []
        self.selected_del = []
        self.llm_adapter = llm_adapter  # 保存LLM适配器实例

        # 初始化窗口
        self.window = ctk.CTkToplevel(master)
        self.window.title("角色库管理")
        self.window.geometry("1200x800")
        self.window.protocol("WM_DELETE_WINDOW", self.on_close)

        # 创建目录结构
        self.create_library_structure()
        # 构建UI
        self.create_ui()
        # 窗口居中
        self.center_window()
        # 窗口模态设置
        self.window.grab_set()
        self.window.attributes('-topmost', 1)
        self.window.after(200, lambda: self.window.attributes('-topmost', 0))

    def create_library_structure(self):
        """创建必要的目录结构"""
        os.makedirs(self.save_path, exist_ok=True)
        all_dir = os.path.join(self.save_path, "全部")
        os.makedirs(all_dir, exist_ok=True)

    def create_ui(self):
        """创建主界面"""
        # 分类按钮区
        self.create_category_bar()

        # 主内容区
        main_frame = ctk.CTkFrame(self.window)
        main_frame.pack(fill="both", expand=True, padx=10, pady=10)

        # 左侧面板（保持不变）
        left_panel = ctk.CTkFrame(main_frame, width=300)
        left_panel.pack(side="left", fill="both", padx=5, pady=5)

        # 上部角色列表区（保持不变）
        role_list_container = ctk.CTkFrame(left_panel)
        role_list_container.pack(fill="both", expand=True, pady=(0, 5))

        self.role_list_frame = ctk.CTkScrollableFrame(role_list_container)
        self.role_list_frame.pack(fill="both", expand=True)

        # 下部内容预览区（保持不变）
        preview_container = ctk.CTkFrame(left_panel)
        preview_container.pack(fill="both", expand=True, pady=(5, 0))

        self.preview_text = ctk.CTkTextbox(preview_container, wrap="word",
                                            font=("Microsoft YaHei", 12))
        scrollbar = ctk.CTkScrollbar(
            preview_container, command=self.preview_text.yview)
        self.preview_text.configure(yscrollcommand=scrollbar.set)

        self.preview_text.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        # 右侧面板（信息编辑区）
        right_panel = ctk.CTkFrame(main_frame)
        right_panel.pack(side="right", fill="both", expand=True, padx=5, pady=5)

        # 分类选择行
        category_frame = ctk.CTkFrame(right_panel)
        category_frame.pack(fill="x", padx=5, pady=5)

        # 分类选择标签
        ctk.CTkLabel(category_frame, text="分类选择", font=DEFAULT_FONT).pack(side="left", padx=(0, 5))

        # 分类选择框
        self.category_combobox = ctk.CTkComboBox(
            category_frame,
            values=self._get_all_categories(),
            width=200,
            font=DEFAULT_FONT
        )
        self.category_combobox.pack(side="left", padx=0)

        # 分类保存按钮
        self.save_category_btn = ctk.CTkButton(
            category_frame,
            text="保存分类",
            width=80,
            command=self._move_to_category,
            font=DEFAULT_FONT
        )
        self.save_category_btn.pack(side="left", padx=(0, 5))

        # 打开文件夹按钮
        ctk.CTkButton(
            category_frame,
            text="打开文件夹",
            width=80,
            command=lambda: os.startfile(
                os.path.join(self.save_path, self.category_combobox.get())),
            font=DEFAULT_FONT
        ).pack(side="left", padx=0)

        # 角色名编辑行
        name_frame = ctk.CTkFrame(right_panel)
        name_frame.pack(fill="x", padx=5, pady=5)

        # 角色名称标签
        ctk.CTkLabel(name_frame, text="角色名称", font=DEFAULT_FONT).pack(side="left", padx=(0, 5))

        self.role_name_var = tk.StringVar()
        self.role_name_entry = ctk.CTkEntry(
            name_frame,
            textvariable=self.role_name_var,
            placeholder_text="角色名称",
            width=200,
            font=DEFAULT_FONT
        )
        self.role_name_entry.pack(side="left", padx=0)

        ctk.CTkButton(
            name_frame,
            text="修改",
            width=60,
            command=self._rename_role_file,
            font=DEFAULT_FONT
        ).pack(side="left", padx=(0, 5))

        ctk.CTkButton(
            name_frame,
            text="新增",
            width=60,
            command=lambda: self._create_new_role("全部"),
            font=DEFAULT_FONT
        ).pack(side="left", padx=0)

        # 属性编辑区（基础框架）
        self.attributes_frame = ctk.CTkScrollableFrame(right_panel)
        self.attributes_frame.pack(fill="both", expand=True, padx=5, pady=5)
        # 设置统一的列权重
        self.attributes_frame.grid_columnconfigure(1, weight=1)

        button_frame = ctk.CTkFrame(right_panel)
        button_frame.pack(fill="x", padx=5, pady=5)

        ctk.CTkButton(button_frame, text="导入角色",
                      command=self.import_roles, font=DEFAULT_FONT).pack(side="left", padx=5)
        ctk.CTkButton(button_frame, text="删除",
                      command=self.delete_current_role, font=DEFAULT_FONT).pack(side="left", padx=5)
        ctk.CTkButton(button_frame, text="保存",
                      command=self.save_current_role, font=DEFAULT_FONT).pack(side="left", padx=5)

    def _get_all_categories(self):
        """获取所有有效分类（包括动态更新）"""
        categories = ["全部"]
        for d in os.listdir(self.save_path):
            if os.path.isdir(os.path.join(self.save_path, d)) and d != "全部":
                categories.append(d)
        return categories

    def _move_to_category(self):
        """分类转移功能"""
        if not hasattr(self, 'current_role') or not self.current_role:
            self._show_message("warning", "警告", "请先选择一个角色")
            return

        new_category = self.category_combobox.get()
        
        # 如果当前在"全部"分类下，需要找到角色实际所在分类
        if self.selected_category == "全部":
            # 遍历所有分类查找实际存储位置（包含全部目录）
            actual_category = None
            for category in os.listdir(self.save_path):
                test_path = os.path.join(
                    self.save_path, category, f"{self.current_role}.txt")
                if os.path.exists(test_path):
                    actual_category = category
                    break

            if not actual_category:
                self._show_message("error", "错误", f"找不到角色 {self.current_role} 的实际存储位置")
                return

            old_path = os.path.join(
                self.save_path, actual_category, f"{self.current_role}.txt")
        else:
            old_path = os.path.join(
                self.save_path, self.selected_category, f"{self.current_role}.txt")

        # 如果目标分类是"全部"，则实际移动到"全部"分类
        if new_category == "全部":
            new_path = os.path.join(
                self.save_path, "全部", f"{self.current_role}.txt")
        else:
            new_path = os.path.join(
                self.save_path, new_category, f"{self.current_role}.txt")

        # 检查是否已经在目标分类
        if os.path.exists(new_path):
            self._show_message("info", "提示", "角色已在目标分类中")
            return

        confirm = messagebox.askyesno(
            "确认", f"确定要将角色 {self.current_role} 移动到 {new_category} 分类吗？", parent=self.window)
        if not confirm:
            return

        try:
            # 确保目标目录存在
            os.makedirs(os.path.dirname(new_path), exist_ok=True)
            
            try:
                # 执行移动操作
                shutil.move(old_path, new_path)
                
                # 更新显示
                self.selected_category = new_category if new_category != "全部" else "全部"
                self.show_category(self.selected_category)
                self.category_combobox.set(new_category)
                
                # 成功提示
                messagebox.showinfo("成功", "分类已更新", parent=self.window)
                return  # 成功时直接返回
                
            except Exception as e:
                # 失败时恢复原分类显示
                self.category_combobox.set(self.selected_category)
                raise e
        except Exception as e:
            self._show_message("error", "错误", f"分类转移失败：{str(e)}")
            self.category_combobox.set(self.selected_category)

    def import_roles(self):
        """导入角色窗口"""
        import_window = ctk.CTkToplevel(self.window)
        import_window.title("角色导入")
        import_window.geometry("800x600")
        import_window.transient(self.window)  # 设置为子窗口
        import_window.grab_set()  # 模态窗口
        import_window.lift()  # 置于父窗口前面

        # 窗口居中计算
        import_window.update_idletasks()
        i_width = import_window.winfo_width()
        i_height = import_window.winfo_height()
        x = self.window.winfo_x() + (self.window.winfo_width() - i_width) // 2
        y = self.window.winfo_y() + (self.window.winfo_height() - i_height) // 2
        import_window.geometry(f"+{x}+{y}")

        # 主内容区
        main_frame = ctk.CTkFrame(import_window)
        main_frame.pack(fill="both", expand=True, padx=10, pady=10)

        # 左右面板容器
        content_frame = ctk.CTkFrame(main_frame)
        content_frame.pack(fill="both", expand=True, pady=(0, 10))
        content_frame.grid_columnconfigure(0, weight=1)  # 左侧面板权重
        content_frame.grid_columnconfigure(1, weight=1)  # 右侧面板权重

        # 左侧面板 - 使用权重让控件占满空间
        left_panel = ctk.CTkFrame(content_frame)
        left_panel.grid(row=0, column=0, sticky="nsew", padx=(0, 5), pady=5)
        left_panel.grid_rowconfigure(0, weight=1)
        left_panel.grid_columnconfigure(0, weight=1)
        left_panel.grid_propagate(False)  # 防止子控件改变父容器大小

        # 右侧面板（2份宽度） - 添加初始可编辑文本框
        right_panel = ctk.CTkFrame(content_frame)
        right_panel.grid(row=0, column=1, sticky="nsew", padx=(5, 0), pady=5)
        right_panel.grid_rowconfigure(0, weight=1)
        right_panel.grid_columnconfigure(0, weight=1)
        
        # 创建初始可编辑文本框
        text_box = ctk.CTkTextbox(right_panel, wrap="word", font=DEFAULT_FONT)
        text_box.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)
        text_box.configure(state="normal")  # 保持可编辑状态

        # 初始化角色列表
        self.import_roles_list = []

        # 底部按钮区
        btn_frame = ctk.CTkFrame(main_frame)
        btn_frame.pack(fill="x", pady=(0, 10))

        # 导入按钮
        ctk.CTkButton(
            btn_frame,
            text="导入临时角色库",
            width=120,
            command=lambda: self.confirm_import(import_window),
            font=DEFAULT_FONT
        ).pack(side="left", padx=10)

        # 分析文件按钮
        ctk.CTkButton(
            btn_frame,
            text="分析文件",
            width=100,
            command=lambda: self.analyze_character_state(right_panel, left_panel),
            font=DEFAULT_FONT
        ).pack(side="left", padx=10)

        # 加载character_state.txt按钮
        ctk.CTkButton(
            btn_frame,
            text="加载character_state.txt",
            width=160,
            command=lambda: self.load_default_character_state(right_panel),
            font=DEFAULT_FONT
        ).pack(side="right", padx=10)

        # 从文件导入按钮
        ctk.CTkButton(
            btn_frame,
            text="从文件导入",
            width=100,
            command=lambda: self.import_from_file(right_panel),
            font=DEFAULT_FONT
        ).pack(side="right", padx=10)

        # 设置内容区权重
        content_frame.grid_rowconfigure(0, weight=1)

    def analyze_character_state(self, right_panel, left_panel):
        """分析角色状态文件，使用LLM提取角色信息并保存到临时角色库"""
        content = ""
        for widget in right_panel.winfo_children():
            if isinstance(widget, ctk.CTkTextbox):
                content = widget.get("1.0", "end").strip()
                break
        
        if not content:
            messagebox.showwarning("警告", "未找到可分析的内容", parent=self.window)
            return

        try:
            # 创建临时角色库目录
            target_dir = os.path.join(self.save_path, "临时角色库")
            # 清空现有临时角色库
            if os.path.exists(target_dir):
                for filename in os.listdir(target_dir):
                    file_path = os.path.join(target_dir, filename)
                    try:
                        if os.path.isfile(file_path):
                            os.unlink(file_path)
                    except Exception as e:
                        print(f"删除文件{file_path}时出错: {e}")
            os.makedirs(target_dir, exist_ok=True)

            # 调用LLM进行分析
            prompt = f"{Character_Import_Prompt}\n<<待分析小说文本开始>>\n{content}\n<<待分析小说文本结束>>"
            response = invoke_with_cleaning(
                self.llm_adapter,
                prompt
            )
            
            # 解析LLM响应
            roles = self._parse_llm_response(response)
            
            if not roles:
                messagebox.showwarning("警告", "未解析到有效角色信息", parent=self.window)
                return

            # 直接显示分析结果而不保存到文件
            self._display_analyzed_roles(left_panel, roles)

        except Exception as e:
            messagebox.showerror("分析失败", f"LLM分析出错：{str(e)}", parent=self.window)

    def _display_temp_roles(self, parent, temp_dir):
        """显示临时角色库中的角色"""
        # 清空左侧面板
        for widget in parent.winfo_children():
            widget.destroy()

        # 创建滚动容器
        scroll_frame = ctk.CTkScrollableFrame(parent)
        scroll_frame.pack(fill="both", expand=True)

        # 读取所有临时角色文件
        self.character_checkboxes = {}
        for file_name in os.listdir(temp_dir):
            if file_name.endswith(".txt"):
                role_name = os.path.splitext(file_name)[0]
                file_path = os.path.join(temp_dir, file_name)
                
                # 解析角色属性
                attributes = self._parse_temp_role_file(file_path)
                
                # 创建带勾选框的条目
                frame = ctk.CTkFrame(scroll_frame)
                frame.pack(fill="x", pady=2, padx=5)
                
                # 勾选框
                var = BooleanVar(value=True)
                cb = ctk.CTkCheckBox(frame, text="", variable=var, width=20, font=DEFAULT_FONT)
                cb.pack(side="left", padx=5)
                
                # 角色名称
                lbl = ctk.CTkLabel(frame, text=role_name, 
                                 font=("Microsoft YaHei", 12))
                lbl.pack(side="left", padx=5)
                
                # 属性摘要
                attrs = [f"{k}({len(v)})" for k,v in attributes.items()]
                summary = ctk.CTkLabel(frame, text=" | ".join(attrs), 
                                     font=("Microsoft YaHei", 12),
                                     text_color="gray")
                summary.pack(side="right", padx=10)
                
                self.character_checkboxes[role_name] = {
                    'var': var,
                    'data': {'name': role_name, 'attributes': attributes}
                }

        # 添加操作按钮
        btn_frame = ctk.CTkFrame(scroll_frame)
        btn_frame.pack(fill="x", pady=5)
        ctk.CTkButton(btn_frame, text="全选", 
                     command=lambda: self._toggle_all(True), font=DEFAULT_FONT).pack(side="left")
        ctk.CTkButton(btn_frame, text="取消选择", 
                     command=lambda: self._toggle_all(False), font=DEFAULT_FONT).pack(side="left")

    def _parse_temp_role_file(self, file_path):
        """解析临时角色文件"""
        attributes = {}
        current_attr = None
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # 统一解析├──和└──两种前缀
                    if any(prefix in line for prefix in ['├──', '└──']) and '：' in line:
                        prefix = '├──' if '├──' in line else '└──'
                        current_attr = line.split(prefix)[1].split('：')[0].strip()
                        attributes[current_attr] = []
                    elif any(prefix in line for prefix in ['│  ├──', '│  └──']):
                        prefix = '│  ├──' if '│  ├──' in line else '│  └──'
                        if current_attr:
                            item = line.split(prefix)[1].strip()
                            attributes[current_attr].append(item)
        except Exception as e:
            messagebox.showerror("解析错误", f"解析临时文件失败：{str(e)}", parent=self.window)
        return attributes

    def _parse_llm_response(self, response):
        """解析LLM返回的角色数据"""
        roles = []
        current_role = None
        current_attr = None
        current_subattr = None
        
        attribute_pattern = re.compile(r'^([├└]──)([\w\u4e00-\u9fa5]+)\s*[:：]')
        item_pattern = re.compile(r'^│\s+([├└]──)\s*(.*)')
        
        for line in response.split('\n'):
            line = line.rstrip()
            
            # 检测角色名称行（兼容中英文冒号和前后空格）
            role_match = re.match(r'^\s*([\u4e00-\u9fa5a-zA-Z0-9]+)\s*[:：]\s*$', line)
            if role_match:
                current_role = role_match.group(1).strip()
                roles.append({'name': current_role, 'attributes': {}})
                continue
                
            if not current_role:
                continue
                
            # 解析属性（支持子属性）
            attr_match = attribute_pattern.match(line)
            if attr_match:
                prefix, attr_name = attr_match.groups()
                current_attr = attr_name.strip()
                roles[-1]['attributes'][current_attr] = []
                current_subattr = None
                continue
                
            # 解析属性条目（支持多级结构）
            item_match = item_pattern.match(line)
            if item_match and current_attr:
                prefix, content = item_match.groups()
                content = content.strip()
                
                # 解析子属性（例如"身体状态: xxx"）
                if ':' in content or '：' in content:
                    subattr_match = re.split(r'[:：]', content, 1)
                    if len(subattr_match) > 1:
                        current_subattr = subattr_match[0].strip()
                        value = subattr_match[1].strip()
                        if value:  # 值不为空时才添加
                            roles[-1]['attributes'][current_attr].append(
                                f"{current_subattr}: {value}"
                            )
                        continue
                
                # 普通条目处理
                if content:
                    if current_subattr:
                        # 子属性的延续条目
                        roles[-1]['attributes'][current_attr][-1] += f"，{content}"
                    else:
                        roles[-1]['attributes'][current_attr].append(content)
        return roles

    def _display_analyzed_roles(self, parent, roles):
        """显示分析后的角色列表"""
        self.character_checkboxes = {}
        
        # 创建带滚动条的容器
        scroll_frame = ctk.CTkScrollableFrame(parent)
        scroll_frame.pack(fill="both", expand=True, padx=5, pady=5)
        scroll_frame.grid_rowconfigure(0, weight=1)
        scroll_frame.grid_columnconfigure(0, weight=1)

        # 为每个角色创建带勾选框的条目
        for role in roles:
            frame = ctk.CTkFrame(scroll_frame)
            frame.pack(fill="x", pady=2, padx=5)
            
            # 勾选框
            var = BooleanVar(value=True)
            cb = ctk.CTkCheckBox(frame, text="", variable=var, width=20, font=DEFAULT_FONT)
            cb.pack(side="left", padx=5)
            
            # 角色名称标签
            lbl = ctk.CTkLabel(frame, text=role['name'], 
                             font=("Microsoft YaHei", 12))
            lbl.pack(side="left", padx=5)
            
            # 属性摘要
            attrs = [f"{k}({len(v)})" for k,v in role['attributes'].items()]
            summary = ctk.CTkLabel(frame, text=" | ".join(attrs), 
                                 font=("Microsoft YaHei", 12),
                                 text_color="gray")
            summary.pack(side="right", padx=10)
            
            self.character_checkboxes[role['name']] = {
                'var': var,
                'data': role
            }

        # 添加全选/反选按钮
        btn_frame = ctk.CTkFrame(scroll_frame)
        btn_frame.pack(fill="x", pady=5)
        
        ctk.CTkButton(btn_frame, text="全选", 
                     command=lambda: self._toggle_all(True), font=DEFAULT_FONT).pack(side="left")
        ctk.CTkButton(btn_frame, text="反选", 
                     command=lambda: self._toggle_all(False), font=DEFAULT_FONT).pack(side="left")

    def _toggle_all(self, select):
        """全选/反选操作"""
        for role in self.character_checkboxes.values():
            current_state = role['var'].get()
            # 如果是反选操作，则设置相反状态
            if isinstance(select, bool):
                role['var'].set(select)
            else:
                role['var'].set(not current_state)


    def import_from_file(self, right_panel):
        """从文件导入内容到右侧窗口"""
        filetypes = (
            ('文本文件', '*.txt'),
            ('Word文档', '*.docx'),
            ('所有文件', '*.*')
        )
        
        file_path = filedialog.askopenfilename(
            title="选择要导入的文件",
            initialdir=os.path.expanduser("~"),
            filetypes=filetypes
        )
        
        if not file_path:
            return

        try:
            content = ""
            if file_path.endswith('.docx'):
                # 处理Word文档
                from docx import Document
                doc = Document(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
            else:
                # 处理普通文本文件
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

            # 更新右侧文本框中
            for widget in right_panel.winfo_children():
                if isinstance(widget, ctk.CTkTextbox):
                    widget.delete("1.0", "end")
                    widget.insert("1.0", content)
                    break

        except Exception as e:
            messagebox.showerror("导入失败", f"无法读取文件：{str(e)}", parent=self.window)

    def load_default_character_state(self, right_panel):
        """加载character_state.txt文件到右侧窗口"""
        # 获取保存路径
        save_path = os.path.dirname(self.save_path)
        file_path = os.path.join(save_path, "character_state.txt")

        if not os.path.exists(file_path):
            messagebox.showwarning("警告", f"未找到文件: {file_path}", parent=self.window)
            return

        try:
            # 读取文件内容
            content = read_file(file_path)

            # 清空右侧面板中可能存在的旧控件
            for widget in right_panel.winfo_children():
                widget.destroy()

            # 查找或创建文本框
            text_box = None
            for widget in right_panel.winfo_children():
                if isinstance(widget, ctk.CTkTextbox):
                    text_box = widget
                    break
            
            if not text_box:
                text_box = ctk.CTkTextbox(right_panel, wrap="word", font=DEFAULT_FONT)
                text_box.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)
            
            text_box.configure(state="normal")
            text_box.delete("1.0", "end")
            text_box.insert("1.0", content)

            # 设置右边面板的布局权重
            right_panel.grid_rowconfigure(0, weight=1)
            right_panel.grid_columnconfigure(0, weight=1)

        except Exception as e:
            messagebox.showerror("错误", f"加载文件失败: {str(e)}", parent=self.window)

    def confirm_import(self, import_window):
        """从临时角色库导入选中的角色"""
        # 创建必要的目录
        target_dir = os.path.join(self.save_path, "临时角色库")
        os.makedirs(target_dir, exist_ok=True)
        
        try:
            # 获取选中的角色
            selected_roles = [role_data['data'] for role_data in self.character_checkboxes.values() 
                            if role_data['var'].get()]
            
            if not selected_roles:
                # 创建错误提示窗口
                error_window = ctk.CTkToplevel(import_window)
                error_window.title("错误")
                error_window.transient(import_window)
                error_window.grab_set()
                
                # 窗口内容
                ctk.CTkLabel(error_window, text="请至少选择一个角色", font=DEFAULT_FONT).pack(padx=20, pady=10)
                ctk.CTkButton(error_window, text="确定", command=error_window.destroy, font=DEFAULT_FONT).pack(pady=10)
                
                # 窗口居中
                error_window.update_idletasks()
                e_width = error_window.winfo_width()
                e_height = error_window.winfo_height()
                x = import_window.winfo_x() + (import_window.winfo_width() - e_width) // 2
                y = import_window.winfo_y() + (import_window.winfo_height() - e_height) // 2
                error_window.geometry(f"+{x}+{y}")
                error_window.attributes('-topmost', 1)
                return

            # 从内存数据直接保存角色
            for role in selected_roles:
                dest_path = os.path.join(target_dir, f"{role['name']}.txt")
                
                # 构建角色内容
                content_lines = [f"{role['name']}："]
                for attr, items in role['attributes'].items():
                    content_lines.append(f"├──{attr}：")
                    for i, item in enumerate(items):
                        prefix = "├──" if i < len(items)-1 else "└──"
                        content_lines.append(f"│  {prefix}{item}")
                
                # 直接写入文件，覆盖已存在的文件
                with open(dest_path, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(content_lines))

            # 刷新分类显示
            self.load_categories()
            import_window.destroy()
            
        except Exception as e:
            # 静默处理错误
            import_window.destroy()



    def delete_current_role(self):
        """删除当前角色"""
        if not hasattr(self, 'current_role') or not self.current_role:
            return

        confirm = messagebox.askyesno(
            "确认删除", f"确定要删除角色 {self.current_role} 吗？", parent=self.window)
        if not confirm:
            return

        role_path = os.path.join(
            self.save_path, self.selected_category, f"{self.current_role}.txt")
        try:
            os.remove(role_path)
            # 从"全部"分类也删除
            all_path = os.path.join(
                self.save_path, "全部", f"{self.current_role}.txt")
            if os.path.exists(all_path):
                os.remove(all_path)
            self.show_category(self.selected_category)
            self.preview_text.delete("1.0", "end")
            self._show_message("info", "成功", "角色已删除")
        except Exception as e:
            self._show_message("error", "错误", f"删除失败：{str(e)}")

    def _build_role_content(self):
        """构建角色文件内容"""
        content = [f"{self.role_name_var.get()}："]
        attributes_order = ["物品", "能力", "状态", "主要角色间关系网", "触发或加深的事件"]

        for attr_name in attributes_order:
            content.append(f"├──{attr_name}：")
            # 找到对应的 attribute_block
            for block in self.attributes_frame.winfo_children():
                if isinstance(block, ctk.CTkFrame) and block.attribute_name == attr_name:
                    # 遍历该 block 中的所有 CTkEntry
                    for child in block.winfo_children():
                        if isinstance(child, ctk.CTkFrame):  # 条目行
                            for item in child.winfo_children():
                                if isinstance(item, ctk.CTkEntry):
                                    entry_text = item.get().strip()
                                    if entry_text:  # 只添加非空条目
                                        content.append(f"│  ├──{entry_text}")
                    break  # 找到对应属性后跳出循环
        return content
    
    def _show_message(self, kind: str, title: str, text: str):
        """
        统一显示 messagebox 并确保消息弹窗位于顶层。
        kind: "info" | "warning" | "error"
        """
        try:
            # 先把父窗口置顶，保证 messagebox 覆盖在上面
            self.window.attributes('-topmost', 1)
        except Exception:
            pass

        if kind == "info":
            messagebox.showinfo(title, text, parent=self.window)
        elif kind == "warning":
            messagebox.showwarning(title, text, parent=self.window)
        elif kind == "error":
            messagebox.showerror(title, text, parent=self.window)
        else:
            messagebox.showinfo(title, text, parent=self.window)

        # 延迟一点再恢复父窗口的 topmost，以确保 messagebox 已显示
        try:
            self.window.after(200, lambda: self.window.attributes('-topmost', 0))
        except Exception:
            pass

    def _save_role_file(self, content, save_path):
        """保存角色文件"""
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(content))

    def _check_role_name_conflict(self, new_name):
        """检查角色名是否重复，遍历整个角色文件夹"""
        conflicts = []
        # 遍历所有分类目录
        for category in os.listdir(self.save_path):
            if os.path.isdir(os.path.join(self.save_path, category)):
                # 检查该分类下是否有同名角色
                role_path = os.path.join(
                    self.save_path, category, f"{new_name}.txt")
                if os.path.exists(role_path):
                    # 如果是"全部"分类，需要进一步检查是否是实际文件
                    if category == "全部":
                        # 检查"全部"目录下的文件是否是实际文件
                        all_path = os.path.join(
                            self.save_path, "全部", f"{new_name}.txt")
                        if os.path.isfile(all_path):
                            # 如果是实际文件，则认为是冲突
                            conflicts.append(category)
                    else:
                        # 普通分类直接记录冲突
                        conflicts.append(category)
        return conflicts

    def save_current_role(self):
        """保存当前编辑的角色"""
        if not hasattr(self, 'current_role') or not self.current_role:
            return

        new_name = self.role_name_var.get().strip()
        if not new_name:
            self._show_message("warning", "警告", "角色名称不能为空")
            return

        # 检查角色名是否重复
        if new_name != self.current_role:
            conflicts = self._check_role_name_conflict(new_name)
            if conflicts:
                messagebox.showerror("错误",       
                                    f"角色名称 '{new_name}' 已存在于以下分类中：\n" +
                                    "\n".join(conflicts) +
                                    "\n请使用不同的角色名称", parent=self.window)
                return

        content = self._build_role_content()
        save_path = os.path.join(self.save_path, self.selected_category,
                                 f"{new_name}.txt")

        try:
            self._save_role_file(content, save_path)
            # 如果修改了角色名，更新文件名
            if new_name != self.current_role:
                old_path = os.path.join(self.save_path, self.selected_category,
                                        f"{self.current_role}.txt")
                os.rename(old_path, save_path)

            # 更新显示
            self.current_role = new_name
            self.show_category(self.selected_category)
            self.show_role(new_name)  # 刷新角色显示
            messagebox.showinfo("成功", "角色已保存", parent=self.window)
        except Exception as e:
            messagebox.showerror("错误", f"保存失败：{str(e)}", parent=self.window)

    def _rename_role_file(self):
        """修改角色名称"""
        old_name = self.current_role
        new_name = self.role_name_var.get().strip()

        if not old_name or not new_name:
            return

        # 处理中英文冒号
        for colon in [":", "："]:
            old_name = old_name.split(colon)[0]
            new_name = new_name.split(colon)[0]

        # 如果角色名没有改变，直接返回
        if new_name == old_name:
            return

        # 检查角色名是否重复
        conflicts = self._check_role_name_conflict(new_name)
        if conflicts:
            messagebox.showerror("错误",
                                f"角色名称 '{new_name}' 已存在于以下分类中：\n" +
                                "\n".join(conflicts) +
                                "\n请使用不同的角色名称", parent=self.window)
            return

        try:
            # 如果是"全部"分类，需要找到实际存储的分类
            if self.selected_category == "全部":
                # 首先检查"全部"目录下是否有该角色文件
                all_path = os.path.join(
                    self.save_path, "全部", f"{old_name}.txt")
                if os.path.exists(all_path):
                    # 如果"全部"目录下有文件，则直接操作
                    actual_category = "全部"
                else:
                    # 遍历所有分类查找实际存储位置
                    actual_category = None
                    for category in os.listdir(self.save_path):
                        if category == "全部":
                            continue
                        test_path = os.path.join(
                            self.save_path, category, f"{old_name}.txt")
                        if os.path.exists(test_path):
                            actual_category = category
                            break

                    if not actual_category:
                        raise FileNotFoundError(
                            f"找不到角色 {old_name} 的实际存储位置")
            else:
                actual_category = self.selected_category

            # 读取旧文件内容并更新角色名
            old_path = os.path.join(
                self.save_path, actual_category, f"{old_name}.txt")
            with open(old_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # 获取第一行内容
            first_line = content.split('\n')[0].strip()
            # 提取内容中的角色名
            content_role_name = first_line.split('：')[0].split(':')[0].strip()
            # 如果内容中的角色名与旧文件名不同，更新内容
            if content_role_name != old_name:
                content = content.replace(
                    f"{content_role_name}：", f"{new_name}：", 1)
            else:
                content = content.replace(f"{old_name}：", f"{new_name}：", 1)

            # 写入新文件
            new_path = os.path.join(
                self.save_path, actual_category, f"{new_name}.txt")
            with open(new_path, 'w', encoding='utf-8') as f:
                f.write(content)

            # 删除旧文件
            os.remove(old_path)

            # 处理"全部"目录
            all_old_path = os.path.join(
                self.save_path, "全部", f"{old_name}.txt")
            all_new_path = os.path.join(
                self.save_path, "全部", f"{new_name}.txt")

            # 如果"全部"目录存在旧文件
            if os.path.exists(all_old_path):
                try:
                    # 更新"全部"目录中的文件内容
                    with open(all_old_path, 'r', encoding='utf-8') as f:
                        all_content = f.read()
                    updated_all_content = all_content.replace(
                        f"{old_name}：", f"{new_name}：", 1)

                    # 写入新文件
                    with open(all_new_path, 'w', encoding='utf-8') as f:
                        f.write(updated_all_content)

                    # 删除旧文件
                    os.remove(all_old_path)
                except Exception as e:
                    messagebox.showerror("错误", f"更新全部目录失败: {str(e)}", parent=self.window)
                    # 回滚重命名操作
                    os.rename(new_path, old_path)
                    return

            # 刷新显示
            self.current_role = new_name
            self.show_category(self.selected_category)
            self.role_name_var.set(new_name)
            self.show_role(new_name)  # 刷新角色显示区域

        except Exception as e:
            self._show_message("error", "错误", f"重命名失败：{str(e)}")

    def _create_new_role(self, category):
        """在指定分类创建新角色"""
        role_dir = os.path.join(self.save_path, category)
        base_name = "未命名"
        counter = 1

        # 生成唯一文件名
        while os.path.exists(os.path.join(role_dir, f"{base_name}.txt")):
            base_name = f"未命名{counter}"
            counter += 1

        # 创建基础文件结构（包含初始条目）
        content = f"{base_name}：\n" + "\n".join([
            "├──物品：",
            "│  └──待补充",
            "├──能力：",
            "│  └──待补充",
            "├──状态：",
            "│  └──待补充",
            "├──主要角色间关系网：",
            "│  └──待补充",
            "├──触发或加深的事件：",
            "│  └──待补充"
        ])

        with open(os.path.join(role_dir, f"{base_name}.txt"), "w", encoding="utf-8") as f:
            f.write(content)

        # 刷新显示
        self.show_category(category)
        self.role_name_var.set(base_name)
        self.current_role = base_name

    def create_category_bar(self):
        """创建分类按钮区"""
        category_frame = ctk.CTkFrame(self.window)
        category_frame.pack(fill="x", padx=10, pady=5)

        # 操作提示
        ctk.CTkLabel(category_frame,
                     text="右键分类名即可重命名",
                     font=DEFAULT_FONT,
                     text_color="gray").pack(side="top", anchor="w", padx=5)

        # 固定按钮
        ctk.CTkButton(category_frame, text="全部", width=50,
                      font=("Microsoft YaHei", 12),
                      command=lambda: self.show_category("全部")).pack(side="left", padx=2)

        # 滚动分类区
        self.scroll_frame = ctk.CTkScrollableFrame(
            category_frame, orientation="horizontal", height=30)
        self.scroll_frame.pack(side="left", fill="x", expand=True, padx=5)

        # 操作按钮
        ctk.CTkButton(category_frame, text="新增", width=50,
                      command=self.add_category, font=DEFAULT_FONT).pack(side="right", padx=2)
        ctk.CTkButton(category_frame, text="删除", width=50,
                      command=self.delete_category, font=DEFAULT_FONT).pack(side="right", padx=2)

        self.load_categories()

    def center_window(self):
        """窗口居中"""
        self.window.update_idletasks()
        parent_x = self.master.winfo_x()
        parent_y = self.master.winfo_y()
        parent_width = self.master.winfo_width()
        parent_height = self.master.winfo_height()
        win_width = 1200
        win_height = 800
        x = parent_x + (parent_width - win_width) // 2
        y = parent_y + (parent_height - win_height) // 2
        self.window.geometry(f"{win_width}x{win_height}+{x}+{y}")

    def load_categories(self):
        """加载分类按钮"""
        for widget in self.scroll_frame.winfo_children():
            widget.destroy()

        categories = [d for d in os.listdir(self.save_path)
                      if os.path.isdir(os.path.join(self.save_path, d)) and d != "全部"]

        for category in categories:
            btn = ctk.CTkButton(self.scroll_frame, text=category, width=80, font=DEFAULT_FONT)
            btn.bind("<Button-1>", lambda e, c=category: self.show_category(c))
            btn.bind("<Button-3>", lambda e, c=category: self.rename_category(c))
            btn.pack(side="left", padx=2)

    def _create_category_directory(self, category_name):
        """创建分类目录"""
        new_dir = os.path.join(self.save_path, category_name)
        if not os.path.exists(new_dir):
            os.makedirs(new_dir)
        return new_dir

    def add_category(self):
        """新增分类"""
        self._create_category_directory("未命名")
        self.load_categories()
        # 刷新分类选择下拉框
        self.category_combobox.configure(values=self._get_all_categories())

    def delete_category(self):
        """删除分类对话框"""
        if not self.window.winfo_exists():
            return

        del_window = ctk.CTkToplevel(self.window)
        del_window.title("删除分类")
        del_window.transient(self.window)
        del_window.grab_set()
        del_window.attributes('-topmost', 1)

        # 居中计算
        parent_x = self.window.winfo_x()
        parent_y = self.window.winfo_y()
        parent_width = self.window.winfo_width()
        parent_height = self.window.winfo_height()
        del_window.geometry(
            f"300x400+{parent_x + (parent_width-300)//2}+{parent_y + (parent_height-400)//2}")

        scroll_frame = ctk.CTkScrollableFrame(del_window)
        scroll_frame.pack(fill="both", expand=True)

        categories = [d for d in os.listdir(self.save_path)
                      if os.path.isdir(os.path.join(self.save_path, d)) and d != "全部"]
        self.selected_del = []

        for cat in categories:
            var = tk.BooleanVar()
            chk = ctk.CTkCheckBox(scroll_frame, text=cat, variable=var, font=DEFAULT_FONT)
            chk.pack(anchor="w")
            self.selected_del.append((cat, var))

        # 操作按钮
        btn_frame = ctk.CTkFrame(del_window)
        btn_frame.pack(fill="x", pady=5)

        ctk.CTkButton(btn_frame, text="删除选中",
                      command=lambda: self.confirm_delete(del_window), font=DEFAULT_FONT).pack(side="left", padx=5)
        ctk.CTkButton(btn_frame, text="取消",
                      command=del_window.destroy, font=DEFAULT_FONT).pack(side="right", padx=5)

        self.category_combobox.configure(values=self._get_all_categories())
        self.category_combobox.set("全部")

    def confirm_delete(self, original_window):
        """确认删除操作"""
        selected = [item[0] for item in self.selected_del if item[1].get()]
        if not selected:
            self._show_message("warning", "警告", "请至少选择一个分类")
            return

        # 创建选择窗口时添加前置设置
        choice_window = ctk.CTkToplevel(self.window)
        choice_window.transient(self.window)  # 设置为子窗口
        choice_window.grab_set()  # 模态窗口
        choice_window.lift()  # 置顶
        choice_window.attributes('-topmost', 1)  # 强制置顶

        # 添加居中计算
        choice_window.update_idletasks()
        c_width = choice_window.winfo_width()
        c_height = choice_window.winfo_height()
        x = self.window.winfo_x() + (self.window.winfo_width() - c_width) // 2
        y = self.window.winfo_y() + (self.window.winfo_height() - c_height) // 2
        choice_window.geometry(f"+{x}+{y}")

        ctk.CTkLabel(choice_window, text="请选择删除方式：", font=DEFAULT_FONT).pack(pady=10)
        btn_frame = ctk.CTkFrame(choice_window)
        btn_frame.pack(pady=10)

        def perform_delete(mode):
            all_dir = os.path.join(self.save_path, "全部")
            for cat in selected:
                cat_path = os.path.join(self.save_path, cat)
                if mode == "move":
                    for role_file in os.listdir(cat_path):
                        if role_file.endswith(".txt"):
                            src = os.path.join(cat_path, role_file)
                            dst = os.path.join(all_dir, role_file)
                            try:
                                shutil.move(src, dst)
                            except:
                                os.remove(dst)
                                shutil.move(src, dst)
                shutil.rmtree(cat_path)
            self.load_categories()
            # 刷新分类选择下拉框
            self.category_combobox.configure(values=self._get_all_categories())
            original_window.destroy()
            choice_window.destroy()

        ctk.CTkButton(btn_frame, text="全部删除",
                      command=lambda: perform_delete("all"), font=DEFAULT_FONT).pack(side="left", padx=5)
        ctk.CTkButton(btn_frame, text="移动角色",
                      command=lambda: perform_delete("move"), font=DEFAULT_FONT).pack(side="left", padx=5)

    def count_roles(self, categories):
        """统计角色数量"""
        count = 0
        for cat in categories:
            cat_path = os.path.join(self.save_path, cat)
            count += len([f for f in os.listdir(cat_path) if f.endswith(".txt")])
        return count

    def show_category(self, category):
        """显示分类内容"""
        self.selected_category = category
        self.category_combobox.set(category)
        for widget in self.role_list_frame.winfo_children():
            widget.destroy()

        # 如果是"全部"分类，显示所有角色
        if category == "全部":
            # 获取所有分类目录
            categories = [d for d in os.listdir(self.save_path)
                          
                          if os.path.isdir(os.path.join(self.save_path, d))]
            # 用于去重的角色集合
            unique_roles = set()

            for cat in categories:
                role_dir = os.path.join(self.save_path, cat)
                try:
                    for role_file in os.listdir(role_dir):
                        if role_file.endswith(".txt"):
                            role_name = os.path.splitext(role_file)[0]
                            # 去重
                            if role_name not in unique_roles:
                                unique_roles.add(role_name)
                                btn = ctk.CTkButton(
                                    self.role_list_frame,
                                    text=role_name,
                                    command=lambda r=role_name: self.show_role(r),
                                    font=DEFAULT_FONT
                                )
                                btn.pack(fill="x", pady=2)
                except FileNotFoundError:
                    continue
        else:
            # 普通分类显示
            role_dir = os.path.join(self.save_path, category)
            try:
                for role_file in os.listdir(role_dir):
                    if role_file.endswith(".txt"):
                        role_name = os.path.splitext(role_file)[0]
                        btn = ctk.CTkButton(
                            self.role_list_frame,
                            text=role_name,
                            command=lambda r=role_name: self.show_role(r),
                            font=DEFAULT_FONT
                        )
                        btn.pack(fill="x", pady=2)
            except FileNotFoundError:
                messagebox.showerror("错误", "分类目录不存在", parent=self.window)

    def show_role(self, role_name):
        """显示角色详细信息（支持UTF-8/ANSI编码）"""
        try:
            # 清空现有属性控件
            self.preview_text.delete('1.0', tk.END)
            for widget in self.attributes_frame.winfo_children():
                widget.destroy()

            # 更新角色名称显示
            self.current_role = role_name.split(":")[0].split("：")[0]
            self.role_name_var.set(self.current_role)

            # 查找角色实际所在目录
            if self.selected_category == "全部":
                # 首先检查"全部"目录下是否有该角色文件
                all_path = os.path.join(
                    self.save_path, "全部", f"{role_name}.txt")
                if os.path.exists(all_path):
                    file_path = all_path
                    actual_category = "全部"
                else:
                    # 如果"全部"目录下没有，则遍历其他分类查找
                    file_path = None
                    for cat in os.listdir(self.save_path):
                        if cat == "全部":
                            continue
                        test_path = os.path.join(
                            self.save_path, cat, f"{role_name}.txt")
                        if os.path.exists(test_path):
                            file_path = test_path
                            actual_category = cat
                            # 保存实际分类
                            self.actual_category = cat
                            break
                    if file_path is None:
                        raise FileNotFoundError(f"找不到角色文件：{role_name}")

                # 只更新分类选择框的显示值，不改变当前选中的分类
                self.category_combobox.set(actual_category)
            else:
                # 普通分类直接使用当前路径
                file_path = os.path.join(
                    self.save_path, self.selected_category, f"{role_name}.txt")

            content, _ = self._read_file_with_fallback_encoding(file_path)

            # 解析属性结构
            attributes = {
                "物品": [],
                "能力": [],
                "状态": [],
                "主要角色间关系网": [],
                "触发或加深的事件": []
            }
            current_attribute = None
            for line in content[1:]:
                # 改进属性名称识别
                if line.startswith(("├──", "├──")):
                    # 提取属性名称（兼容冒号和空格）
                    attr_part = line.split("──")[1].strip()
                    attr_name = re.split(r'[:：]', attr_part, 1)[0].strip()

                    # 匹配预设属性
                    for preset_attr in attributes:
                        if attr_name == preset_attr:
                            current_attribute = preset_attr
                            indent_level = line.find(
                                "├") if "├" in line else line.find("├")
                            break
                    else:
                        current_attribute = None

                # 改进条目内容提取
                elif current_attribute and line.startswith(("│  ", "   ")):
                    # 提取整个条目内容
                    item_content = line.strip()
                    # 去掉前面的符号和空格
                    item_content = re.sub(r'^[│├└─\s]*', '', item_content)
                    attributes[current_attribute].append(item_content)

            # 显示原始文件内容
            self.preview_text.insert(tk.END, '\n'.join(content))

            # 重构属性编辑区
            for attr_name, items in attributes.items():
                self._create_attribute_section(attr_name, items)

        except FileNotFoundError as e:
            messagebox.showerror("错误", f"文件不存在：{e}", parent=self.window)
        except Exception as e:
            messagebox.showerror("错误", f"读取文件失败：{e}", parent=self.window)

    def _create_attribute_section(self, attr_name, items):
        """创建单个属性的编辑区域"""

        # 属性块 (attribute_block)
        attribute_block = ctk.CTkFrame(self.attributes_frame)
        attribute_block.pack(fill="x", pady=5)
        attribute_block.attribute_name = attr_name  # 存储属性名称
        attribute_block.grid_columnconfigure(1, weight=1)  # 设置第二列权重
        attribute_block.grid_columnconfigure(1, weight=1)  # 设置第二列权重

        # 属性名称标签
        label = ctk.CTkLabel(attribute_block, text=attr_name, font=DEFAULT_FONT)
        label.grid(row=0, column=0, sticky="w", padx=(5, 10), pady=2)

        # 第一个条目和“增加”按钮的容器
        first_item_frame = ctk.CTkFrame(attribute_block)
        first_item_frame.grid(row=0, column=1, sticky="ew", padx=5, pady=2)
        first_item_frame.grid_columnconfigure(0, weight=1)

        # 第一个条目输入框
        first_entry = ctk.CTkEntry(first_item_frame, font=DEFAULT_FONT)
        first_entry.grid(row=0, column=0, sticky="ew", padx=(0, 5), ipadx=5, ipady=3)
        if items:
            first_entry.insert(0, items[0])  # 填充第一个条目的内容

        # “增加”按钮容器
        add_button_frame = ctk.CTkFrame(first_item_frame, fg_color="transparent")
        add_button_frame.grid(row=0, column=1, sticky="e", padx=(5, 0))

        # “增加”按钮
        add_button = ctk.CTkButton(
            add_button_frame,
            text="+",
            width=30,
            command=lambda: self._add_item(attr_name),
            font=DEFAULT_FONT
        )
        add_button.grid(row=0, column=0)

        # 创建剩余的条目（如果有）
        for i, item_text in enumerate(items[1:]):
            self._add_item(attr_name, item_text)  # 传入初始文本

    def _add_item(self, attr_name, initial_text=""):
        """为指定属性添加一个新条目"""

        # 找到对应的 attribute_block
        attribute_block = None
        for block in self.attributes_frame.winfo_children():
            if isinstance(block, ctk.CTkFrame) and block.attribute_name == attr_name:
                attribute_block = block
                break

        if attribute_block is None:
            return

        # 计算新条目的行号
        row_number = 0
        for child in attribute_block.winfo_children():
            if isinstance(child, ctk.CTkFrame):
                row_number += 1

        # 条目容器
        item_frame = ctk.CTkFrame(attribute_block)
        item_frame.grid(row=row_number, column=1, sticky="ew", padx=5, pady=2)
        item_frame.grid_columnconfigure(0, weight=1)

        # 条目输入框
        new_entry = ctk.CTkEntry(item_frame, font=DEFAULT_FONT)
        new_entry.grid(row=0, column=0, sticky="ew", padx=(0, 5), ipadx=5, ipady=3)
        new_entry.insert(0, initial_text)  # 设置初始文本

        # 删除按钮容器
        del_button_frame = ctk.CTkFrame(item_frame, fg_color="transparent")
        del_button_frame.grid(row=0, column=1, sticky="e", padx=(5, 0))
        # “删除”按钮
        del_button = ctk.CTkButton(
            del_button_frame,
            text="-",
            width=30,
            command=lambda f=item_frame: self._remove_item(f, attr_name),
            font=DEFAULT_FONT
        )
        del_button.grid(row=0, column=0)


    def _remove_item(self, item_frame, attr_name):
        """移除指定的条目，并重新调整布局"""

        # 找到对应的 attribute_block
        attribute_block = None
        for block in self.attributes_frame.winfo_children():
            if isinstance(block, ctk.CTkFrame) and block.attribute_name == attr_name:
                attribute_block = block
                break

        if attribute_block is None:
            return

        # 确认不是删除带"+"号的原始条目
        for child in item_frame.winfo_children():
            if isinstance(child, ctk.CTkFrame):
                for btn in child.winfo_children():
                    if isinstance(btn, ctk.CTkButton) and btn.cget("text") == "+":
                        self._show_message("info", "提示", "不能删除带'+'号的原始条目")
                        return

        # 移除条目
        item_frame.destroy()

        # 重新调整剩余条目的行号
        current_row = 0
        for child in attribute_block.winfo_children():
            if isinstance(child, ctk.CTkFrame):
                if current_row == 0:  # 找到属性标签
                    current_row += 1
                    continue
                ctk.CTkFrame.grid_configure(child, row=current_row)
                current_row += 1

    def _read_file_with_fallback_encoding(self, file_path):
        """带编码回退的文件读取，支持UTF-8、GBK(ANSI)和BOM"""
        encodings = ['utf-8-sig', 'utf-8', 'gbk', 'latin1']  # 增加更多编码支持

        for encoding in encodings:
            try:
                with open(file_path, "r", encoding=encoding) as f:
                    content = f.read()
                    # 检查内容是否包含乱码
                    if any(ord(char) > 127 and not char.isprintable() for char in content):
                        continue  # 如果包含乱码，尝试下一个编码
                    return content.splitlines(), encoding
            except UnicodeDecodeError:
                continue
            except Exception as e:
                raise

        # 如果所有编码尝试都失败，尝试二进制读取
        try:
            with open(file_path, "rb") as f:
                raw_data = f.read()
                # 尝试UTF-8解码
                try:
                    return raw_data.decode('utf-8').splitlines(), 'utf-8'
                except UnicodeDecodeError:
                    # 尝试GBK解码
                    try:
                        return raw_data.decode('gbk').splitlines(), 'gbk'
                    except UnicodeDecodeError:
                        # 最后尝试latin1解码
                        return raw_data.decode('latin1').splitlines(), 'latin1'
        except Exception as e:
            raise ValueError(f"无法识别的文件编码：{file_path}")

    def rename_category(self, old_name):
        """分类重命名（带居中功能）"""
        new_name = None  # 初始化变量

        # 创建对话框窗口
        dialog = ctk.CTkToplevel(self.window)
        dialog.title("重命名分类")
        dialog.transient(self.window)
        dialog.grab_set()

        # 窗口内容
        content_frame = ctk.CTkFrame(dialog)
        content_frame.pack(fill="both", expand=True, padx=10, pady=10)

        # 顶部提示
        ctk.CTkLabel(content_frame, text=f"当前分类：{old_name}", 
                    font=DEFAULT_FONT).pack(pady=(10, 5))

        # 输入框
        input_frame = ctk.CTkFrame(content_frame)
        input_frame.pack(fill="x", pady=5)
        ctk.CTkLabel(input_frame, text="新名称：", 
                    font=DEFAULT_FONT).pack(side="left", padx=5)
        name_var = tk.StringVar()
        name_entry = ctk.CTkEntry(input_frame, textvariable=name_var, width=150, font=DEFAULT_FONT)
        name_entry.pack(side="left", padx=5)

        # 按钮区
        button_frame = ctk.CTkFrame(content_frame)
        button_frame.pack(fill="x", pady=(10, 5))

        def confirm_rename():
            nonlocal new_name  # 引用外部变量
            new_name = name_var.get().strip()
            if not new_name:
                messagebox.showwarning("警告", "分类名称不能为空", parent=self.window)
                return
            if new_name == old_name:
                dialog.destroy()
                return
            if os.path.exists(os.path.join(self.save_path, new_name)):
                messagebox.showerror("错误", "分类名称已存在", parent=self.window)
                return

            try:
                os.rename(os.path.join(self.save_path, old_name),
                          os.path.join(self.save_path, new_name))
                self.load_categories()
                # 更新分类选择框
                self.category_combobox.configure(
                    values=self._get_all_categories())
                self.category_combobox.set(new_name)
                dialog.destroy()
            except Exception as e:
                messagebox.showerror("错误", f"重命名失败：{str(e)}", parent=self.window)

        ctk.CTkButton(button_frame, text="确认",
                      command=confirm_rename, font=DEFAULT_FONT).pack(side="left", padx=10)
        ctk.CTkButton(button_frame, text="取消",
                      command=dialog.destroy, font=DEFAULT_FONT).pack(side="right", padx=10)

        # 窗口居中
        dialog.update_idletasks()
        d_width = dialog.winfo_width()
        d_height = dialog.winfo_height()
        x = self.window.winfo_x() + (self.window.winfo_width() - d_width) // 2
        y = self.window.winfo_y() + (self.window.winfo_height() - d_height) // 2
        dialog.geometry(f"+{x}+{y}")
        dialog.attributes('-topmost', 1)

    def on_close(self):
        """关闭窗口"""
        self.window.destroy()
</file>

<file path=".gitignore">
/Novel_Src
/.venv
/build
/dist
/.vscode
/__pycache__
__pycache__/
*.pyc
/markdown
/vectorstore
/example
config.json
config_test.json
.idea/
/novel
app.log
test.py
/backup
uv.lock
.python-version
pyproject.toml
</file>

<file path="novel_generator/finalization.py">
# novel_generator/finalization.py
# -*- coding: utf-8 -*-
"""
定稿章节和扩写章节（finalize_chapter、enrich_chapter_text）
[V3.0 分步执行 + 结构化伏笔库版]
"""
import os
import logging
import re
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter
from prompt_definitions import (
    summary_prompt,
    update_character_state_prompt,
    FORESHADOWING_ANALYSIS_PROMPT,
    DETECT_CHANGES_PROMPT,
    UPDATE_PROFILE_PROMPT,
)
from novel_generator.common import invoke_with_cleaning
from utils import read_file, clear_file_content, save_string_to_txt, append_text_to_file
from novel_generator.vectorstore_utils import update_vector_store


def _ensure_role_library_dirs(filepath: str) -> str:
    """确保角色库目录存在，返回 '全部' 目录路径。"""
    role_root = os.path.join(filepath, "角色库")
    all_dir = os.path.join(role_root, "全部")
    os.makedirs(all_dir, exist_ok=True)
    return all_dir


def _role_profile_template(char_name: str) -> str:
    """新角色的档案模板（用于 UPDATE_PROFILE_PROMPT 的 old_profile）。"""
    return "\n".join(
        [
            f"{char_name}：",
            "├──物品：",
            "│  └──（待补充）",
            "├──能力：",
            "│  └──（待补充）",
            "├──状态：",
            "│  └──（待补充）",
            "├──主要角色间关系网：",
            "│  └──（待补充）",
            "├──触发或加深的事件：",
            "│  └──（待补充）",
        ]
    )


def sync_role_library_from_chapter(
    novel_number: int,
    filepath: str,
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    temperature: float = 0.2,
    max_tokens: int = 2048,
    timeout: int = 600,
):
    """
    定稿时：把本章发生变化/新登场的角色，自动写入角色库（角色库/全部/<角色名>.txt）。
    目标：不再依赖手动“导入临时角色库”，确保角色档案可持续积累。
    """
    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    if not os.path.exists(chapter_file):
        return

    chapter_text = read_file(chapter_file).strip()
    if not chapter_text:
        return

    all_dir = _ensure_role_library_dirs(filepath)
    llm_adapter = create_llm_adapter(interface_format, base_url, model_name, api_key, temperature, max_tokens, timeout)

    # 1) 识别发生变化/新登场角色
    names = []
    try:
        raw = invoke_with_cleaning(llm_adapter, DETECT_CHANGES_PROMPT.format(chapter_text=chapter_text))
        # 提取 JSON 数组
        import json, re
        m = re.search(r"\[.*?\]", raw, re.DOTALL)
        if m:
            names = json.loads(m.group(0))
    except Exception as e:
        logging.error(f"角色变化检测失败: {e}")
        names = []

    if not names:
        return

    # 2) 逐个更新角色档案
    for char_name in names:
        try:
            if not isinstance(char_name, str):
                continue
            char_name = char_name.strip()
            if not char_name:
                continue

            profile_path = os.path.join(all_dir, f"{char_name}.txt")
            old_profile = read_file(profile_path).strip() if os.path.exists(profile_path) else _role_profile_template(char_name)

            prompt = UPDATE_PROFILE_PROMPT.format(
                char_name=char_name,
                chapter_text=chapter_text,
                old_profile=old_profile,
            )
            new_profile = invoke_with_cleaning(llm_adapter, prompt)
            if new_profile and new_profile.strip():
                save_string_to_txt(new_profile.strip(), profile_path)
        except Exception as e:
            logging.error(f"更新角色档案失败({char_name}): {e}")

# -----------------------------------------------------------------------------
# 1. 独立功能：更新全局摘要
# -----------------------------------------------------------------------------
def update_global_summary(
    novel_number: int,
    filepath: str,
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    temperature: float = 0.5,
    max_tokens: int = 4096,
    timeout: int = 600
):
    logging.info(f"开始单独更新摘要: 第 {novel_number} 章")
    
    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    
    if not os.path.exists(chapter_file):
        logging.error(f"找不到章节文件: {chapter_file}")
        return

    chapter_text = read_file(chapter_file)
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    old_summary = read_file(global_summary_file)

    summary_word_count = len(old_summary.strip())

    llm_adapter = create_llm_adapter(interface_format, base_url, model_name, api_key, temperature, max_tokens, timeout)
    
    prompt = summary_prompt.format(
        global_summary=old_summary,
        chapter_text=chapter_text,
        summary_word_count=summary_word_count  # 添加字数参数
    )

    try:
        new_summary = invoke_with_cleaning(llm_adapter, prompt)
        if new_summary:
            save_string_to_txt(new_summary, global_summary_file)
            logging.info("全局摘要更新完成。")
    except Exception as e:
        logging.error(f"摘要更新失败: {e}")


# -----------------------------------------------------------------------------
# 2. 独立功能：更新角色状态
# -----------------------------------------------------------------------------
def update_character_state(
    novel_number: int,
    filepath: str,
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    temperature: float = 0.5,
    max_tokens: int = 4096,
    timeout: int = 600
):
    logging.info(f"开始单独更新角色状态: 第 {novel_number} 章")

    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    if not os.path.exists(chapter_file):
        return

    chapter_text = read_file(chapter_file)
    char_state_file = os.path.join(filepath, "character_state.txt")
    old_state = read_file(char_state_file)

    llm_adapter = create_llm_adapter(interface_format, base_url, model_name, api_key, temperature, max_tokens, timeout)

    prompt = update_character_state_prompt.format(
        old_state=old_state,
        chapter_text=chapter_text
    )

    try:
        new_state = invoke_with_cleaning(llm_adapter, prompt)
        if new_state:
            save_string_to_txt(new_state, char_state_file)
            logging.info("角色状态表更新完成。")
    except Exception as e:
        logging.error(f"角色状态更新失败: {e}")


# -----------------------------------------------------------------------------
# 3. 独立功能：更新伏笔 (结构化解析版)
# -----------------------------------------------------------------------------

def save_structured_foreshadowing(filepath, novel_number, short_text, long_text):
    """
    辅助函数：将解析出的长短线伏笔整合到文件中，实现动态管理
    - 短线伏笔在解决后会被删除
    - 长线伏笔会按剧情发展更新
    """
    record_file = os.path.join(filepath, "foreshadowing_records.txt")
    
    # 1. 准备新内容 (如果为空则不写入该章节头)
    new_short_block = ""
    if short_text and "无" not in short_text and len(short_text) > 5:
        new_short_block = f"第{novel_number}章：\n{short_text}\n"

    new_long_block = ""
    if long_text and "无" not in long_text and len(long_text) > 5:
        new_long_block = f"第{novel_number}章：\n{long_text}\n"

    if not new_short_block and not new_long_block:
        logging.info("本章无有效伏笔，跳过写入。")
        return

    # 2. 读取现有文件内容
    if os.path.exists(record_file):
        content = read_file(record_file)
    else:
        content = ""

    header_long = "=== 【长线伏笔】 ==="
    header_short = "=== 【短线伏笔】 ==="

    # 初始化模板
    if header_long not in content or header_short not in content:
        content = f"{header_long}\n\n\n{header_short}\n\n"

    # 3. 分离长线和短线部分
    try:
        split_index = content.find(header_short)
        
        # 分离长线和短线部分
        long_section_raw = content[:split_index].rstrip()
        short_section_raw = content[split_index:].rstrip()

        # 导入正则表达式模块
        import re
        
        # 智能处理短线伏笔：检查是否解决了之前的伏笔并删除它们
        if new_short_block:
            # 查找当前章节是否存在
            chapter_pattern = rf"第{novel_number}章：.*?(?=\n第\d+章|$)"
            existing_chapter_match = re.search(chapter_pattern, short_section_raw, re.DOTALL)
            
            if existing_chapter_match:
                # 如果存在，替换整个章节内容
                old_chapter_block = existing_chapter_match.group(0)
                short_section_raw = short_section_raw.replace(old_chapter_block, new_short_block.strip())
            else:
                # 如果不存在，添加到短线部分
                short_section_raw += f"\n\n{new_short_block}"
                
            # 检查当前章节的短线伏笔内容是否提到了"已解决"、"已结束"等关键词
            # 并相应地从短线伏笔中删除已解决的伏笔
            resolved_patterns = [
                r"已解决[:：]?\s*(.+?)(?:\n|$)",
                r"已结束[:：]?\s*(.+?)(?:\n|$)",
                r"已处理[:：]?\s*(.+?)(?:\n|$)",
                r"已回应[:：]?\s*(.+?)(?:\n|$)",
                r"已解决的伏笔[:：]?\s*(.+?)(?:\n|$)",
                r"解决了[:：]?\s*(.+?)(?:\n|$)",
                r"完成了[:：]?\s*(.+?)(?:\n|$)"
            ]
            
            # 查找已解决的伏笔内容
            resolved_items = []
            for pattern in resolved_patterns:
                matches = re.findall(pattern, new_short_block, re.IGNORECASE | re.MULTILINE)
                resolved_items.extend(matches)
            
            # 从短线伏笔中移除已解决的伏笔
            for item in resolved_items:
                # 清理项目名称，去除多余空白
                clean_item = item.strip()
                # 在短线伏笔中查找并移除对应的条目
                short_section_raw = re.sub(r'.*' + re.escape(clean_item) + r'.*?\n?', '', short_section_raw)
                
        else:
            # 如果新的短线伏笔为空，查找当前章节记录并移除
            chapter_pattern = rf"第{novel_number}章：.*?(?=\n第\d+章|$)"
            existing_chapter_match = re.search(chapter_pattern, short_section_raw, re.DOTALL)
            
            if existing_chapter_match:
                old_chapter_block = existing_chapter_match.group(0)
                short_section_raw = short_section_raw.replace(old_chapter_block, "").strip()
                
                # 清理多余的空白行
                short_section_raw = re.sub(r'\n\s*\n\s*\n', '\n\n', short_section_raw)

        # 处理长线伏笔
        if new_long_block:
            # 查找当前章节是否存在
            chapter_pattern = rf"第{novel_number}章：.*?(?=\n第\d+章|$)"
            existing_chapter_match = re.search(chapter_pattern, long_section_raw, re.DOTALL)
            
            if existing_chapter_match:
                # 如果存在，替换整个章节内容
                old_chapter_block = existing_chapter_match.group(0)
                long_section_raw = long_section_raw.replace(old_chapter_block, new_long_block.strip())
            else:
                # 如果不存在，添加到长线部分
                long_section_raw += f"\n\n{new_long_block}"
                
            # 对于长线伏笔，我们不删除它们，而是更新它们的状态
            # 检查新长线伏笔是否提到了之前的伏笔内容并更新其状态
            update_patterns = [
                r"更新状态[:：]?\s*(.+?)(?:\n|$)",
                r"进展[:：]?\s*(.+?)(?:\n|$)",
                r"发展[:：]?\s*(.+?)(?:\n|$)",
                r"推进[:：]?\s*(.+?)(?:\n|$)"
            ]
            
            for pattern in update_patterns:
                matches = re.findall(pattern, new_long_block, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    # 这里我们可以进一步处理长线伏笔的更新逻辑
                    # 暂时保留这个扩展点
                    pass
        else:
            # 如果新长线伏笔为空，但当前章节有记录，移除它
            chapter_pattern = rf"第{novel_number}章：.*?(?=\n第\d+章|$)"
            existing_chapter_match = re.search(chapter_pattern, long_section_raw, re.DOTALL)
            
            if existing_chapter_match:
                old_chapter_block = existing_chapter_match.group(0)
                long_section_raw = long_section_raw.replace(old_chapter_block, "").strip()
                
                # 清理多余的空白行
                long_section_raw = re.sub(r'\n\s*\n\s*\n', '\n\n', long_section_raw)

        # 重新组合内容
        final_content = f"{long_section_raw}\n\n\n{short_section_raw}\n"
        
        save_string_to_txt(final_content, record_file)
        logging.info(f"伏笔库已更新 (动态管理) - 第{novel_number}章")

    except Exception as e:
        logging.error(f"伏笔文件解析写入错误: {e}")
        # 降级：如果解析坏了，直接追加到末尾防止丢数据
        append_text_to_file(f"\n\n【第{novel_number}章补录】\n{long_text}\n{short_text}", record_file)


def cleanup_foreshadowing_records(filepath):
    """
    清理伏笔记录文件，删除已解决的短线伏笔和不必要的标记
    """
    record_file = os.path.join(filepath, "foreshadowing_records.txt")
    if not os.path.exists(record_file):
        return

    content = read_file(record_file)
    
    header_long = "=== 【长线伏笔】 ==="
    header_short = "=== 【短线伏笔】 ==="

    # 分离长线和短线部分
    try:
        split_index = content.find(header_short)
        
        # 分离长线和短线部分
        long_section = content[:split_index].rstrip()
        short_section = content[split_index:].rstrip()

        # 移除短线伏笔中标记为"已解决"或"已结束"的条目
        import re
        # 查找所有"已解决:"或"已结束:"的条目
        solved_pattern = r'[•\-\d\.]*\s*已解决[:：]\s*[^\n]*\n?|[•\-\d\.]*\s*已结束[:：]\s*[^\n]*\n?'
        cleaned_short = re.sub(solved_pattern, '', short_section)
        
        # 移除多余的空行
        cleaned_short = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_short).strip()
        
        # 长线部分保持不变，只清理空行
        cleaned_long = re.sub(r'\n\s*\n\s*\n+', '\n\n', long_section).strip()

        # 重新组合内容
        final_content = f"{cleaned_long}\n\n\n{cleaned_short}\n"
        
        save_string_to_txt(final_content, record_file)
        logging.info("伏笔库已清理完成。")
        
    except Exception as e:
        logging.error(f"伏笔库清理失败: {e}")


def update_foreshadowing_records(
    novel_number: int,
    filepath: str,
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    temperature: float = 0.5,
    max_tokens: int = 4096,
    timeout: int = 600
):
    logging.info(f"开始单独分析伏笔: 第 {novel_number} 章")

    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    if not os.path.exists(chapter_file):
        return

    chapter_text = read_file(chapter_file)
    
    # 获取已有伏笔记录
    record_file = os.path.join(filepath, "foreshadowing_records.txt")
    existing_foreshadowing_records = ""
    if os.path.exists(record_file):
        existing_foreshadowing_records = read_file(record_file)
    else:
        existing_foreshadowing_records = "（暂无已有伏笔记录）"
    
    # 这里的适配器建议使用逻辑较强的模型
    llm_adapter = create_llm_adapter(interface_format, base_url, model_name, api_key, temperature, max_tokens, timeout)

    # 1. 调用 LLM
    prompt = FORESHADOWING_ANALYSIS_PROMPT.format(
        chapter_text=chapter_text,
        novel_number=novel_number,
        existing_foreshadowing_records=existing_foreshadowing_records
    )

    try:
        result = invoke_with_cleaning(llm_adapter, prompt)
        if not result:
            return

        # 2. 解析文本 (Regex)
        # 您的提示词格式为：
        # 【短线伏笔】：
        # ...
        # 【长线伏笔】：
        # ...
        
        # 提取短线内容
        short_match = re.search(r"【短线伏笔】：\s*(.*?)\s*(?=【长线伏笔】|$)", result, re.DOTALL)
        short_content = short_match.group(1).strip() if short_match else ""

        # 提取长线内容
        long_match = re.search(r"【长线伏笔】：\s*(.*?)\s*(?=---|[-]{3,}|$)", result, re.DOTALL)
        long_content = long_match.group(1).strip() if long_match else ""

        # 3. 结构化保存
        save_structured_foreshadowing(filepath, novel_number, short_content, long_content)
        
        # 4. 清理伏笔库
        cleanup_foreshadowing_records(filepath)

    except Exception as e:
        logging.error(f"伏笔分析失败: {e}")


# -----------------------------------------------------------------------------
# 4. 主入口：章节定稿 (依次调用)
# -----------------------------------------------------------------------------
def finalize_chapter(
    novel_number: int,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    filepath: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    interface_format: str,
    max_tokens: int,
    timeout: int = 600
):
    """
    分步定稿：摘要 -> 角色 -> 伏笔 -> 向量库
    """
    # 1. 更新摘要
    update_global_summary(novel_number, filepath, api_key, base_url, model_name, interface_format, temperature, max_tokens, timeout)
    
    # 2. 更新角色
    update_character_state(novel_number, filepath, api_key, base_url, model_name, interface_format, temperature, max_tokens, timeout)

    # 2.5 定稿后同步角色库（自动写入/更新角色档案）
    try:
        sync_role_library_from_chapter(
            novel_number=novel_number,
            filepath=filepath,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            interface_format=interface_format,
            temperature=0.2,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        logging.info("角色库同步完成。")
    except Exception as e:
        logging.error(f"角色库同步失败: {e}")
    
    # 3. [修改] 更新伏笔 (支持长短线分类)
    update_foreshadowing_records(novel_number, filepath, api_key, base_url, model_name, interface_format, temperature, max_tokens, timeout)

    # 4. 向量入库
    # 这里需要单独创建一个 embedding adapter
    ingest_chapter_to_vector_store(novel_number, filepath, embedding_api_key, embedding_url, embedding_interface_format, embedding_model_name)

    logging.info(f"Chapter {novel_number} finalization process completed.")


def ingest_chapter_to_vector_store(novel_number, filepath, api_key, base_url, interface_format, model_name):
    """
    辅助函数：向量入库
    """
    try:
        chapters_dir = os.path.join(filepath, "chapters")
        chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
        if not os.path.exists(chapter_file):
            return
        chapter_text = read_file(chapter_file)
        
        logging.info(f"正在将第 {novel_number} 章存入向量库...")
        
        emb_adapter = create_embedding_adapter(interface_format, api_key, base_url, model_name)
        update_vector_store(emb_adapter, chapter_text, filepath)
        
        logging.info("向量库更新完成。")
    except Exception as e:
        logging.error(f"向量入库失败: {e}")


# -----------------------------------------------------------------------------
# 5. 扩写功能
# -----------------------------------------------------------------------------
def enrich_chapter_text(
    chapter_text: str,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    interface_format: str,
    max_tokens: int,
    timeout: int=600
) -> str:
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    prompt = f"""以下章节文本较短，请在保持剧情连贯的前提下进行扩写，使其更充实，接近 {word_number} 字左右，仅给出最终文本，不要解释任何内容。：
原内容：
{chapter_text}
"""
    enriched_text = invoke_with_cleaning(llm_adapter, prompt)
    return enriched_text if enriched_text else chapter_text
</file>

<file path="README.md">
# 📖 自动小说生成工具

>- 当前没有什么精力维护该项目，本身该项目并无任何收益，以及临近毕业，有很多内容要忙，如果后面有时间的话，再考虑基于更新的技术去重构吧。——2025/9/24

<div align="center">
  
✨ **核心功能** ✨

| 功能模块          | 关键能力                          |
|-------------------|----------------------------------|
| 🎨 小说设定工坊    | 世界观架构 / 角色设定 / 剧情蓝图   |
| 📖 智能章节生成    | 多阶段生成保障剧情连贯性           |
| 🧠 状态追踪系统    | 角色发展轨迹 / 伏笔管理系统         |
| 🔍 语义检索引擎    | 基于向量的长程上下文一致性维护      |
| 📚 知识库集成      | 支持本地文档参考         |
| ✅ 自动审校机制    | 检测剧情矛盾与逻辑冲突          |
| 🖥 可视化工作台    | 全流程GUI操作，配置/生成/审校一体化 |

</div>

> 一款基于大语言模型的多功能小说生成器，助您高效创作逻辑严谨、设定统一的长篇故事

---

## 📑 目录导航
1. [环境准备](#-环境准备)  
2. [项目架构](#-项目架构)  
3. [配置指南](#⚙️-配置指南)  
4. [运行说明](#🚀-运行说明)  
5. [使用教程](#📘-使用教程)  
6. [疑难解答](#❓-疑难解答)  

---

## 🛠 环境准备
确保满足以下运行条件：
- **Python 3.9+** 运行环境（推荐3.10-3.12之间）
- **pip** 包管理工具
- 有效API密钥：
  - 云端服务：OpenAI / DeepSeek 等
  - 本地服务：Ollama 等兼容 OpenAI 的接口

---


## 📥 安装说明
1. **下载项目**  
   - 通过 [GitHub](https://github.com) 下载项目 ZIP 文件，或使用以下命令克隆本项目：
     ```bash
     git clone https://github.com/YILING0013/AI_NovelGenerator
     ```

2. **安装编译工具（可选）**  
   - 如果对某些包无法正常安装，访问 [Visual Studio Build Tools](https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/) 下载并安装C++编译工具，用于构建部分模块包；
   - 安装时，默认只包含 MSBuild 工具，需手动勾选左上角列表栏中的 **C++ 桌面开发** 选项。

3. **安装依赖并运行**  
   - 打开终端，进入项目源文件目录：
     ```bash
     cd AI_NovelGenerator
     ```
   - 安装项目依赖：
     ```bash
     pip install -r requirements.txt
     ```
   - 安装完成后，运行主程序：
     ```bash
     python main.py
     ```

>如果缺失部分依赖，后续**手动执行**
>```bash
>pip install XXX
>```
>进行安装即可

## 🗂 项目架构
```
novel-generator/
├── main.py                      # 入口文件, 运行 GUI
├── consistency_checker.py       # 一致性检查, 防止剧情冲突
|—— chapter_directory_parser.py  # 目录解析
|—— embedding_adapters.py        # Embedding 接口封装
|—— llm_adapters.py              # LLM 接口封装
├── prompt_definitions.py        # 定义 AI 提示词
├── utils.py                     # 常用工具函数, 文件操作
├── config_manager.py            # 管理配置 (API Key, Base URL)
├── config.json                  # 用户配置文件 (可选)
├── novel_generator/             # 章节生成核心逻辑
├── ui/                          # 图形界面
└── vectorstore/                 # (可选) 本地向量数据库存储
```

---

## ⚙️ 配置指南
### 📌 基础配置（config.json）
```json
{
    "api_key": "sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
    "base_url": "https://api.openai.com/v1",
    "interface_format": "OpenAI",
    "model_name": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 4096,
    "embedding_api_key": "sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
    "embedding_interface_format": "OpenAI",
    "embedding_url": "https://api.openai.com/v1",
    "embedding_model_name": "text-embedding-ada-002",
    "embedding_retrieval_k": 4,
    "topic": "星穹铁道主角星穿越到原神提瓦特大陆，拯救提瓦特大陆，并与其中的角色展开爱恨情仇的小说",
    "genre": "玄幻",
    "num_chapters": 120,
    "word_number": 4000,
    "filepath": "D:/AI_NovelGenerator/filepath"
}
```

### 🔧 配置说明
1. **生成模型配置**
   - `api_key`: 大模型服务的API密钥
   - `base_url`: API终端地址（本地服务填Ollama等地址）
   - `interface_format`: 接口模式
   - `model_name`: 主生成模型名称（如gpt-4, claude-3等）
   - `temperature`: 创意度参数（0-1，越高越有创造性）
   - `max_tokens`: 模型最大回复长度

2. **Embedding模型配置**
   - `embedding_model_name`: 模型名称（如Ollama的nomic-embed-text）
   - `embedding_url`: 服务地址
   - `embedding_retrieval_k`: 

3. **小说参数配置**
   - `topic`: 核心故事主题
   - `genre`: 作品类型
   - `num_chapters`: 总章节数
   - `word_number`: 单章目标字数
   - `filepath`: 生成文件存储路径

---

## 🚀 运行说明
### **方式 1：使用 Python 解释器**
```bash
python main.py
```
执行后，GUI 将会启动，你可以在图形界面中进行各项操作。

### **方式 2：打包为可执行文件**
如果你想在无 Python 环境的机器上使用本工具，可以使用 **PyInstaller** 进行打包：

```bash
pip install pyinstaller
pyinstaller main.spec
```
打包完成后，会在 `dist/` 目录下生成可执行文件（如 Windows 下的 `main.exe`）。

---

## 📘 使用教程
1. **启动后，先完成基本参数设置：**  
   - **API Key & Base URL**（如 `https://api.openai.com/v1`）  
   - **模型名称**（如 `gpt-3.5-turbo`、`gpt-4o` 等）  
   - **Temperature** (0~1，决定文字创意程度)  
   - **主题(Topic)**（如 “废土世界的 AI 叛乱”）  
   - **类型(Genre)**（如 “科幻”/“魔幻”/“都市幻想”）  
   - **章节数**、**每章字数**（如 10 章，每章约 3000 字）  
   - **保存路径**（建议创建一个新的输出文件夹）

2. **点击「Step1. 生成设定」**  
   - 系统将基于主题、类型、章节数等信息，生成：  
     - `Novel_setting.txt`：包含世界观、角色信息、雷点暗线等。  
   - 可以在生成后的 `Novel_setting.txt` 中查看或修改设定内容。

3. **点击「Step2. 生成目录」**  
   - 系统会根据已完成的 `Novel_setting.txt` 内容，为全部章节生成：  
     - `Novel_directory.txt`：包括每章标题和简要提示。  
   - 可以在生成后的文件中查看、修改或补充章节标题和描述。

4. **点击「Step3. 生成章节草稿」**  
   - 在生成章节之前，你可以：  
     - **设置章节号**（如写第 1 章，就填 `1`）  
     - **在“本章指导”输入框**中提供对本章剧情的任何期望或提示  
   - 点击按钮后，系统将：  
     - 自动读取前文设定、`Novel_directory.txt`、以及已定稿章节  
     - 调用向量检索回顾剧情，保证上下文连贯  
     - 生成本章大纲 (`outline_X.txt`) 及正文 (`chapter_X.txt`)  
   - 生成完成后，你可在左侧的文本框查看、编辑本章草稿内容。

5. **点击「Step4. 定稿当前章节」**  
   - 系统将：  
     - **更新全局摘要**（写入 `global_summary.txt`）  
     - **更新角色状态**（写入 `character_state.txt`）  
     - **更新向量检索库**（保证后续章节可以调用最新信息）  
     - **更新剧情要点**（如 `plot_arcs.txt`）  
   - 定稿完成后，你可以在 `chapter_X.txt` 中看到定稿后的文本。

6. **一致性检查（可选）**  
   - 点击「[可选] 一致性审校」按钮，对最新章节进行冲突检测，如角色逻辑、剧情前后矛盾等。  
   - 若有冲突，会在日志区输出详细提示。

7. **重复第 4-6 步** 直到所有章节生成并定稿！

> **向量检索配置提示**  
> 1. embedding模型需要显示指定接口和模型名称；
> 2. 使用**本地Ollama**的**Embedding**时需提前启动Ollama服务：  
>    ```bash
>    ollama serve  # 启动服务
>    ollama pull nomic-embed-text  # 下载/启用模型
>    ```
> 3. 切换不同Embedding模型后建议清空vectorstore目录
> 4. 云端Embedding需确保对应API权限已开通

---

## ❓ 疑难解答
### Q1: Expecting value: line 1 column 1 (char 0)

该问题大概率由于API未正确响应造成，也许响应了一个html？其它内容，导致出现该报错；


### Q2: HTTP/1.1 504 Gateway Timeout？
确认接口是否稳定；

### Q3: 如何切换不同的Embedding提供商？
在GUI界面中对应输入即可。

---

如有更多问题或需求，欢迎在**项目 Issues** 中提出。
</file>

<file path="novel_generator/chapter.py">
# novel_generator/chapter.py
# -*- coding: utf-8 -*-
"""
章节草稿生成及获取历史章节文本、当前章节摘要等
"""
import os
import re
import json
import logging
from llm_adapters import create_llm_adapter
from prompt_definitions import (
    first_chapter_draft_prompt, 
    next_chapter_draft_prompt, 
    summarize_recent_chapters_prompt,
    CHAPTER_CAST_PROMPT,
    knowledge_filter_prompt,
    knowledge_search_prompt,
    LOGIC_CHECK_PROMPT,
    REWRITE_WITH_FEEDBACK_PROMPT,
    REFINE_DIRECTORY_PROMPT,
    ACTIVE_VERIFICATION_PLANNER_PROMPT, # 新增
    ACTIVE_VERIFICATION_RULE_MAKER_PROMPT # 新增
)
from chapter_directory_parser import get_chapter_info_from_blueprint
from novel_generator.common import invoke_with_cleaning
from utils import extract_relevant_segments, read_file, clear_file_content, save_string_to_txt
from novel_generator.vectorstore_utils import (
    get_relevant_context_from_vector_store,
    load_vector_store  # 添加导入
)
logging.basicConfig(
    filename='app.log',      # 日志文件名
    filemode='a',            # 追加模式（'w' 会覆盖）
    level=logging.INFO,      # 记录 INFO 及以上级别的日志
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

def extract_entity_lock_list(
    character_state_text: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    previous_excerpt: str,
    user_guidance: str
) -> str:
    """
    从角色状态、本章要素、前文摘要中提取关键实体列表，供大模型锁定名称使用。
    防止大模型胡编乱造人物名、地名、技能名等。
    """
    entities = []
    # 从角色状态中解析角色名（格式：角色名：）
    if character_state_text:
        for line in character_state_text.split('\n'):
            line = line.strip()
            if line.endswith('：') and not line.startswith('├') and not line.startswith('│') and not line.startswith('='):
                name = line.replace('：', '').strip()
                if name and name not in ['【核心人设】', '【当前状态】'] and len(name) >= 2:
                    entities.append(f"人物：{name}")
    # 从核心人物、道具、场景中补充
    for part, prefix in [(characters_involved, "人物"), (key_items, "道具"), (scene_location, "场景")]:
        if part and part.strip():
            for item in re.split(r'[,，、\s]+', part.strip()):
                item = item.strip()
                if item and len(item) >= 2 and item not in ['未指定', '无']:
                    if prefix == "人物" and not any(f"人物：{item}" in e or e.endswith(item) for e in entities):
                        entities.append(f"人物：{item}")
                    elif prefix == "道具":
                        entities.append(f"道具：{item}")
                    elif prefix == "场景":
                        entities.append(f"场景：{item}")
    # 去重并格式化
    seen = set()
    unique = []
    for e in entities:
        key = e.split('：', 1)[-1]
        if key not in seen:
            seen.add(key)
            unique.append(e)
    if not unique:
        return "（请从前文、角色状态、知识库中提取已出现的名称，严禁编造新的人名、地名、技能名）"
    result = "\n".join(unique)
    result += "\n\n【重要】上述为已确认实体。写作时仅使用上述名称或前文/知识库中明确出现的名称，严禁编造新名字。"
    return result


def extract_character_relationships(character_state_text: str) -> str:
    """
    从角色状态文本中提取角色之间的关系网。
    解析格式：
    角色名：
    【当前状态】
    ├──关系: 角色1（关系描述）、角色2（关系描述）
    """
    if not character_state_text:
        return "（暂无角色关系网信息）"
    
    relationships_dict = {}
    current_char = None
    lines = character_state_text.split('\n')
    
    try:
        for i, line in enumerate(lines):
            # 识别活跃区/潜伏区的角色名（以 "角色名：" 结尾，不包含特殊符号）
            if line.endswith('：') and not line.startswith('├') and not line.startswith('│') and not line.startswith('='):
                potential_char = line.replace('：', '').strip()
                # 排除非角色的标题行
                if potential_char and potential_char not in ['【核心人设】', '【当前状态】']:
                    current_char = potential_char
                    relationships_dict[current_char] = []
            
            # 提取关系行（格式：├──关系: ...）
            if current_char and '├──关系:' in line:
                # 提取冒号后的内容
                rel_part = line.split('├──关系:')[1].strip()
                if rel_part:
                    relationships_dict[current_char].append(rel_part)
    except Exception as e:
        logging.warning(f"解析角色关系网失败: {e}")
        return "（角色关系网解析失败）"
    
    # 过滤空关系
    relationships_dict = {k: v for k, v in relationships_dict.items() if v}
    
    if not relationships_dict:
        return "（暂无角色关系网信息）"
    
    # 格式化输出为易读的关系网
    result_lines = ["【人物关系网概览】"]
    for char_name, relations in relationships_dict.items():
        result_lines.append(f"\n{char_name}：")
        for rel in relations:
            result_lines.append(f"  ├─ {rel}")
    
    return "\n".join(result_lines)

def get_last_n_chapters_text(chapters_dir: str, current_chapter_num: int, n: int = 3) -> list:
    """
    从目录 chapters_dir 中获取最近 n 章的文本内容，返回文本列表。
    """
    texts = []
    start_chap = max(1, current_chapter_num - n)
    for c in range(start_chap, current_chapter_num):
        chap_file = os.path.join(chapters_dir, f"chapter_{c}.txt")
        if os.path.exists(chap_file):
            text = read_file(chap_file).strip()
            texts.append(text)
        else:
            texts.append("")
    return texts

def summarize_recent_chapters(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    chapters_text_list: list,
    novel_number: int,            # 新增参数
    chapter_info: dict,           # 新增参数
    next_chapter_info: dict,      # 新增参数
    filepath: str | None = None,  # 【修复类型注解】添加|None允许可选参数
    global_summary: str = "",    # 新增：传入全局摘要以匹配 prompt 占位符
    character_relationships: str = "",  # 新增：角色关系网
    previous_chapter_excerpt: str = "", # 新增：上一章结尾内容
    user_guidance: str = "",     # 新增：用户指导
    timeout: int = 600
) -> str:  # 修改返回值类型为 str，不再是 tuple
    """
    根据前三章内容生成当前章节的精准摘要。(支持伏笔注入)
    如果解析失败，则返回空字符串。
    """
    try:
        combined_text = "\n".join(chapters_text_list).strip()
        if not combined_text and not global_summary:
            return ""
            
        # 限制组合文本长度
        max_combined_length = 4000
        if len(combined_text) > max_combined_length:
            combined_text = combined_text[-max_combined_length:]

        # === 【新增逻辑】读取伏笔库内容 ===
        foreshadowing_text = "（暂无伏笔记录）"
        if filepath:
            try:
                record_file = os.path.join(filepath, "foreshadowing_records.txt")
                if os.path.exists(record_file):
                    content = read_file(record_file).strip()
                    if content:
                        # 截取最后 3000 字符防止 Token 溢出，或者根据模型窗口决定
                        foreshadowing_text = content[-3000:] if len(content) > 3000 else content
            except Exception as e:
                logging.warning(f"摘要生成时读取伏笔库失败: {e}")
            
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        # 确保所有参数都有默认值
        chapter_info = chapter_info or {}
        next_chapter_info = next_chapter_info or {}
        
        # 使用安全的 dict 来格式化 prompt，避免因为 prompt 占位符变动导致 KeyError
        class _SafeDict(dict):
            def __missing__(self, key):
                return ""

        # 【关键修复】为 summarize_recent_chapters_prompt 创建专用的 prompt_values
        # 仅包含当前章节的必要信息，不包含下一章信息，以防止 LLM 混淆
        summarize_prompt_values = {
            "global_summary": global_summary,
            "previous_chapter_excerpt": previous_chapter_excerpt,
            "user_guidance": user_guidance,  # 新增用户指导参数
            "novel_number": novel_number,
            "chapter_title": chapter_info.get("chapter_title", "未命名"),
            "chapter_role": chapter_info.get("chapter_role", "常规章节"),
            "chapter_purpose": chapter_info.get("chapter_purpose", "内容推进"),
            "suspense_level": chapter_info.get("suspense_level", "中等"),
            "foreshadowing": chapter_info.get("foreshadowing", "无"),
            "plot_twist_level": chapter_info.get("plot_twist_level", "★☆☆☆☆"),
            "chapter_summary": chapter_info.get("chapter_summary", ""),
            "foreshadowing_records": foreshadowing_text,
            "character_relationships": character_relationships,  # 新增关系网
            # 下一章信息（仅用于逻辑检查，不混入生成过程）
            "next_chapter_number": novel_number + 1,
            "next_chapter_title": next_chapter_info.get("chapter_title", "（未命名）"),
            "next_chapter_role": next_chapter_info.get("chapter_role", "过渡章节"),
        }
        
        # 完整的 prompt_values（用于其他 prompt）
        prompt_values = {
            "global_summary": global_summary,
            "previous_chapter_excerpt": previous_chapter_excerpt,
            "user_guidance": user_guidance,  # 传递用户指导
            "novel_number": novel_number,
            "chapter_title": chapter_info.get("chapter_title", "未命名"),
            "chapter_role": chapter_info.get("chapter_role", "常规章节"),
            "chapter_purpose": chapter_info.get("chapter_purpose", "内容推进"),
            "suspense_level": chapter_info.get("suspense_level", "中等"),
            "foreshadowing": chapter_info.get("foreshadowing", "无"),
            "plot_twist_level": chapter_info.get("plot_twist_level", "★☆☆☆☆"),
            "chapter_summary": chapter_info.get("chapter_summary", ""),
            "foreshadowing_records": foreshadowing_text,
            "character_relationships": character_relationships,
            # 下一章信息
            "next_chapter_number": novel_number + 1,
            "next_chapter_title": next_chapter_info.get("chapter_title", "（未命名）"),
            "next_chapter_role": next_chapter_info.get("chapter_role", "过渡章节"),
            "next_chapter_purpose": next_chapter_info.get("chapter_purpose", "承上启下"),
            "next_chapter_summary": next_chapter_info.get("chapter_summary", "衔接过渡内容"),
            "next_chapter_suspense_level": next_chapter_info.get("suspense_level", "中等"),
            "next_chapter_foreshadowing": next_chapter_info.get("foreshadowing", "无特殊伏笔"),
            "next_chapter_plot_twist_level": next_chapter_info.get("plot_twist_level", "★☆☆☆☆")
        }

        # 【关键修复】使用 summarize_prompt_values 而不是 prompt_values
        # 这样可以防止下一章的信息泄露到 summarize_recent_chapters_prompt 中
        prompt = summarize_recent_chapters_prompt.format_map(_SafeDict(summarize_prompt_values))
        
        response_text = invoke_with_cleaning(llm_adapter, prompt)
        
        # 如果您有 extract_summary_from_response 函数，可以使用它
        # 如果没有，直接使用 response_text 也是安全的，因为 Prompt 已经要求直接输出了
        if 'extract_summary_from_response' in globals():
            summary = extract_summary_from_response(response_text)
        else:
            summary = response_text
        
        if not summary:
            logging.warning("Failed to extract summary, using full response")
            return response_text[:2000]  # 限制长度
            
        return summary[:2000]  # 限制摘要长度
        
    except Exception as e:
        logging.error(f"Error in summarize_recent_chapters: {str(e)}")
        return ""

def extract_summary_from_response(response_text: str) -> str:
    """从响应文本中提取摘要部分"""
    if not response_text:
        return ""
        
    # 查找摘要标记
    summary_markers = [
        "当前章节摘要:", 
        "章节摘要:",
        "摘要:",
        "本章摘要:"
    ]
    
    for marker in summary_markers:
        if (marker in response_text):
            parts = response_text.split(marker, 1)
            if len(parts) > 1:
                return parts[1].strip()
    
    return response_text.strip()

def format_chapter_info(chapter_info: dict) -> str:
    """将章节信息字典格式化为文本"""
    template = """
章节编号：第{number}章
章节标题：《{title}》
章节定位：{role}
核心作用：{purpose}
主要人物：{characters}
关键道具：{items}
场景地点：{location}
伏笔设计：{foreshadow}
悬念密度：{suspense}
转折程度：{twist}
章节简述：{summary}
"""
    return template.format(
        number=chapter_info.get('chapter_number', '未知'),
        title=chapter_info.get('chapter_title', '未知'),
        role=chapter_info.get('chapter_role', '未知'),
        purpose=chapter_info.get('chapter_purpose', '未知'),
        characters=chapter_info.get('characters_involved', '未指定'),
        items=chapter_info.get('key_items', '未指定'),
        location=chapter_info.get('scene_location', '未指定'),
        foreshadow=chapter_info.get('foreshadowing', '无'),
        suspense=chapter_info.get('suspense_level', '一般'),
        twist=chapter_info.get('plot_twist_level', '★☆☆☆☆'),
        summary=chapter_info.get('chapter_summary', '未提供')
    )

def parse_search_keywords(response_text: str) -> list:
    """解析新版关键词格式（示例输入：'科技公司·数据泄露\n地下实验室·基因编辑'）"""
    return [
        line.strip().replace('·', ' ')
        for line in response_text.strip().split('\n')
        if '·' in line
    ][:5]  # 最多取5组

def apply_content_rules(texts: list, novel_number: int) -> list:
    """
    [修改版] 移除硬编码的跳过逻辑。
    保留原始内容，只做简单的去重或标记，把判断交给 LLM。
    """
    processed = []
    seen_hashes = set()
    
    for text in texts:
        # 1. 简单去重
        clean_text = text.strip()
        text_hash = hash(clean_text)
        if text_hash in seen_hashes:
            continue
        seen_hashes.add(text_hash)
        
        # 2. 移除所有 [SKIP] / [MOD] 标记逻辑
        # 只要检索到了，就说明向量认为它相关，直接透传给 LLM
        processed.append(clean_text)
            
    return processed

def apply_knowledge_rules(contexts: list, chapter_num: int) -> list:
    """
    [修改版] 废弃基于章节距离的过滤。
    只要是检索出来的，说明向量相似度高，都应该保留给 Prompt 处理。
    """
    # 直接返回，不做任何删减
    return contexts

def get_filtered_knowledge_context(
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    filepath: str,
    chapter_info: dict,
    retrieved_texts: list,
    author_implicit_settings: str = "",
    max_tokens: int = 2048,
    timeout: int = 600
) -> str:
    """优化后的知识过滤处理"""
    # 1. 如果没有检索到内容，直接返回
    if not retrieved_texts:
        return "（无相关知识库内容，请基于前文设定创作）"

    try:
        # 2. 调用修改后的规则（现在只是简单的去重和标记）
        # 注意：这里不再传入 chapter_num，因为新版函数不需要它
        processed_texts = apply_content_rules(retrieved_texts, chapter_info.get('chapter_number', 0))

        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=0.1, # 再次降低温度，强制其“死板”
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        # 3. 格式化检索文本：保留更多长度，不要随意截断
        # 只有当总长度极大时才进行截断
        formatted_texts = []
        for i, text in enumerate(processed_texts, 1):
            # 允许单条检索内容更长，以便保留环境描写的全貌
            clean_text = text.strip()
            formatted_texts.append(f"--- 片段 {i} ---\n{clean_text}")

        all_retrieved_text = "\n".join(formatted_texts)

        # 4. 构造 Prompt
        formatted_chapter_info = (
            f"章节：第{chapter_info.get('chapter_number')}章 {chapter_info.get('chapter_title')}\n"
            f"场景：{chapter_info.get('scene_location', '未知')}\n"
            f"涉及人物：{chapter_info.get('characters_involved', '未知')}\n"
            f"关键道具：{chapter_info.get('key_items', '无')}"
        )

        prompt = knowledge_filter_prompt.format(
            chapter_info=formatted_chapter_info,
            author_implicit_settings=author_implicit_settings or "（无）",
            retrieved_texts=all_retrieved_text
        )
        
        filtered_content = invoke_with_cleaning(llm_adapter, prompt)
        return filtered_content if filtered_content else "（知识内容过滤后为空）"
        
    except Exception as e:
        logging.error(f"Error in knowledge filtering: {str(e)}")
        # 降级策略：如果过滤失败，直接返回前2条原始检索结果，保证至少有东西可用
        fallback = "\n".join(retrieved_texts[:2])
        return f"[过滤失败，显示原始检索]:\n{fallback}"

def build_chapter_prompt(
    api_key: str,
    base_url: str,
    model_name: str,
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    # 选角/逻辑专用模型（可选，不传则复用主模型）
    cast_api_key: str | None = None,
    cast_base_url: str | None = None,
    cast_model_name: str | None = None,
    cast_interface_format: str | None = None,
    cast_temperature: float | None = None,
    cast_max_tokens: int | None = None,
    cast_timeout: int | None = None,
) -> str:
    """
    构造当前章节的请求提示词（完整实现版）
    修改重点：
    1. 优化知识库检索流程
    2. 新增内容重复检测机制
    3. 集成提示词应用规则
    """
    # 读取基础文件
    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    novel_architecture_text = read_file(arch_file)
    directory_file = os.path.join(filepath, "Novel_directory.txt")
    blueprint_text = read_file(directory_file)
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    global_summary_text = read_file(global_summary_file)
    character_state_file = os.path.join(filepath, "character_state.txt")
    character_state_text = read_file(character_state_file)
    
    # 获取章节信息
    chapter_info = get_chapter_info_from_blueprint(blueprint_text, novel_number)
    chapter_title = chapter_info["chapter_title"]
    chapter_role = chapter_info["chapter_role"]
    chapter_purpose = chapter_info["chapter_purpose"]
    suspense_level = chapter_info["suspense_level"]
    foreshadowing = chapter_info["foreshadowing"]
    plot_twist_level = chapter_info["plot_twist_level"]
    chapter_summary = chapter_info["chapter_summary"]

    # 获取下一章节信息
    next_chapter_number = novel_number + 1
    next_chapter_info = get_chapter_info_from_blueprint(blueprint_text, next_chapter_number)
    next_chapter_title = next_chapter_info.get("chapter_title", "（未命名）")
    next_chapter_role = next_chapter_info.get("chapter_role", "过渡章节")
    next_chapter_purpose = next_chapter_info.get("chapter_purpose", "承上启下")
    next_chapter_suspense = next_chapter_info.get("suspense_level", "中等")
    next_chapter_foreshadow = next_chapter_info.get("foreshadowing", "无特殊伏笔")
    next_chapter_twist = next_chapter_info.get("plot_twist_level", "★☆☆☆☆")
    next_chapter_summary = next_chapter_info.get("chapter_summary", "衔接过渡内容")

    # 创建章节目录
    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    # 第一章特殊处理
    if novel_number == 1:
        return first_chapter_draft_prompt.format(
            novel_number=novel_number,
            word_number=word_number,
            chapter_title=chapter_title,
            chapter_role=chapter_role,
            chapter_purpose=chapter_purpose,
            suspense_level=suspense_level,
            foreshadowing=foreshadowing,
            plot_twist_level=plot_twist_level,
            chapter_summary=chapter_summary,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            user_guidance=user_guidance,
            novel_setting=novel_architecture_text
        )

    # 获取前文内容和摘要
    recent_texts = get_last_n_chapters_text(chapters_dir, novel_number, n=3)
    
    # 获取前一章结尾（增加长度以更好衔接上下文）
    previous_excerpt = ""
    for text in reversed(recent_texts):
        if text.strip():
            previous_excerpt = text[-800:] if len(text) > 800 else text
            break
    
    # 提取角色关系网
    character_relationships_summary = extract_character_relationships(character_state_text)
    
    try:
        logging.info("Attempting to generate summary")
        short_summary = summarize_recent_chapters(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            chapters_text_list=recent_texts,
            novel_number=novel_number,
            chapter_info=chapter_info,
            next_chapter_info=next_chapter_info,
            filepath=filepath,  # 【修复】添加filepath参数以支持伏笔库注入
            global_summary=global_summary_text,
            character_relationships=character_relationships_summary,  # 新增关系网参数
            previous_chapter_excerpt=previous_excerpt,  # 新增参数：上一章结尾内容
            user_guidance=user_guidance,  # 新增参数：用户指导
            timeout=timeout
        )
        logging.info("Summary generated successfully")
    except Exception as e:
        logging.error(f"Error in summarize_recent_chapters: {str(e)}")
        short_summary = "（摘要生成失败）"

    # ================= 4. 知识库检索与过滤 =================
    filtered_context = "（无相关知识库内容，请基于前文设定创作）"
    try:
        from embedding_adapters import create_embedding_adapter
        embedding_adapter = create_embedding_adapter(
            embedding_interface_format,
            embedding_api_key,
            embedding_url,
            embedding_model_name
        )
        store = load_vector_store(embedding_adapter, filepath)
        if store and store._collection.count() > 0:
            llm_adapter = create_llm_adapter(
                interface_format=interface_format,
                base_url=base_url,
                model_name=model_name,
                api_key=api_key,
                temperature=0.2,
                max_tokens=max_tokens,
                timeout=timeout
            )
            search_prompt = knowledge_search_prompt.format(
                chapter_number=novel_number,
                chapter_title=chapter_title,
                characters_involved=characters_involved,
                key_items=key_items,
                scene_location=scene_location,
                chapter_role=chapter_role,
                chapter_purpose=chapter_purpose,
                foreshadowing=foreshadowing,
                short_summary=short_summary,
                user_guidance=user_guidance or "（无）",
                time_constraint=time_constraint or "（无）"
            )
            search_response = invoke_with_cleaning(llm_adapter, search_prompt)
            keyword_groups = parse_search_keywords(search_response)
            all_contexts = []
            actual_k = min(embedding_retrieval_k, max(1, store._collection.count()))
            for group in keyword_groups[:6]:
                raw = get_relevant_context_from_vector_store(
                    embedding_adapter, group, filepath, k=max(2, actual_k)
                )
                if raw:
                    all_contexts.append(raw)
            if all_contexts:
                processed = apply_content_rules(all_contexts, novel_number)
                chapter_info_for_filter = {
                    "chapter_number": novel_number,
                    "chapter_title": chapter_title,
                    "chapter_role": chapter_role,
                    "chapter_purpose": chapter_purpose,
                    "characters_involved": characters_involved,
                    "key_items": key_items,
                    "scene_location": scene_location,
                }
                filtered_context = get_filtered_knowledge_context(
                    api_key=api_key,
                    base_url=base_url,
                    model_name=model_name,
                    interface_format=interface_format,
                    filepath=filepath,
                    chapter_info=chapter_info_for_filter,
                    retrieved_texts=processed,
                    author_implicit_settings=user_guidance or "",
                    max_tokens=max_tokens,
                    timeout=timeout
                )
    except Exception as e:
        logging.warning(f"Knowledge retrieval/filter failed: {e}")

    # ================= 5. 主动验证逻辑 (Active Verification) =================
    verification_constraints = "（未启用验证）"
    try:
        verif_info = {
            "chapter_title": chapter_title,
            "chapter_role": chapter_role,
            "short_summary": short_summary,
            "characters_involved": characters_involved,
            "key_items": key_items,
            "scene_location": scene_location
        }
        from embedding_adapters import create_embedding_adapter
        embedding_adapter = create_embedding_adapter(
            embedding_interface_format,
            embedding_api_key,
            embedding_url,
            embedding_model_name
        )
        verification_constraints = perform_active_verification(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            interface_format=interface_format,
            embedding_adapter=embedding_adapter,
            filepath=filepath,
            chapter_info=verif_info,
            # 传入逻辑/选角模型参数
            cast_api_key=cast_api_key,
            cast_base_url=cast_base_url,
            cast_model_name=cast_model_name,
            cast_interface_format=cast_interface_format,
            cast_temperature=cast_temperature,
            cast_max_tokens=cast_max_tokens,
            cast_timeout=cast_timeout,
            timeout=timeout
        )
    except Exception as e:
        logging.error(f"Active Verification failed: {e}")
        verification_constraints = "（验证过程异常，请忽略）"

    # ================= 6. 实体锁定列表 =================
    entity_lock_list = extract_entity_lock_list(
        character_state_text,
        characters_involved,
        key_items,
        scene_location,
        previous_excerpt,
        user_guidance
    )

    # ================= 7. 本章人物卡（出场角色/关系网/特点动机）=================
    chapter_cast = "（人物卡生成失败）"
    try:
        # 优先使用专门的“逻辑/选角模型”配置
        cast_if = (cast_interface_format or interface_format)
        cast_key = (cast_api_key or api_key)
        cast_url = (cast_base_url or base_url)
        cast_model = (cast_model_name or model_name)
        cast_temp = cast_temperature if cast_temperature is not None else 0.2
        cast_tokens = cast_max_tokens if cast_max_tokens is not None else max_tokens
        cast_to = cast_timeout if cast_timeout is not None else timeout

        llm_adapter_cast = create_llm_adapter(
            interface_format=cast_if,
            base_url=cast_url,
            model_name=cast_model,
            api_key=cast_key,
            temperature=cast_temp,
            max_tokens=cast_tokens,
            timeout=cast_to,
        )
        chapter_cast_prompt = CHAPTER_CAST_PROMPT.format(
            global_summary=global_summary_text,
            previous_chapter_excerpt=previous_excerpt,
            character_state=character_state_text,
            short_summary=short_summary,
            user_guidance=user_guidance or "（无）",
            characters_involved=characters_involved or "（未指定）",
            key_items=key_items or "（无）",
            scene_location=scene_location or "（未知）",
        )
        chapter_cast = invoke_with_cleaning(llm_adapter_cast, chapter_cast_prompt, max_retries=3)
    except Exception as e:
        logging.warning(f"Chapter cast generation failed: {e}")
        chapter_cast = "（人物卡生成失败，请以角色状态为准）"

    # 返回最终提示词
    return next_chapter_draft_prompt.format(
        user_guidance=user_guidance if user_guidance else "无特殊指导",
        global_summary=global_summary_text,
        previous_chapter_excerpt=previous_excerpt,
        character_state=character_state_text,
        chapter_cast=chapter_cast,
        short_summary=short_summary,
        novel_number=novel_number,
        chapter_title=chapter_title,
        chapter_role=chapter_role,
        chapter_purpose=chapter_purpose,
        suspense_level=suspense_level,
        foreshadowing=foreshadowing,
        plot_twist_level=plot_twist_level,
        chapter_summary=chapter_summary,
        word_number=word_number,
        characters_involved=characters_involved,
        key_items=key_items,
        scene_location=scene_location,
        time_constraint=time_constraint,
        next_chapter_number=next_chapter_number,
        next_chapter_title=next_chapter_title,
        next_chapter_role=next_chapter_role,
        next_chapter_purpose=next_chapter_purpose,
        next_chapter_suspense_level=next_chapter_suspense,
        next_chapter_foreshadowing=next_chapter_foreshadow,
        next_chapter_plot_twist_level=next_chapter_twist,
        next_chapter_summary=next_chapter_summary,
        verification_constraints=verification_constraints,
        entity_lock_list=entity_lock_list,
        filtered_context=filtered_context,
    )

def generate_chapter_draft(
    api_key: str,
    base_url: str,
    model_name: str, 
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    custom_prompt_text: str | None = None
) -> str:
    """
    生成章节草稿，支持自定义提示词
    """
    if custom_prompt_text is None:
        prompt_text = build_chapter_prompt(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            filepath=filepath,
            novel_number=novel_number,
            word_number=word_number,
            temperature=temperature,
            user_guidance=user_guidance,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            embedding_retrieval_k=embedding_retrieval_k,
            interface_format=interface_format,
            max_tokens=max_tokens,
            timeout=timeout
        )
    else:
        prompt_text = custom_prompt_text

    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )

    chapter_content = invoke_with_cleaning(llm_adapter, prompt_text)
    if not chapter_content.strip():
        logging.warning("Generated chapter draft is empty.")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    clear_file_content(chapter_file)
    save_string_to_txt(chapter_content, chapter_file)
    logging.info(f"[Draft] Chapter {novel_number} generated as a draft.")
    return chapter_content


def analyze_chapter_logic(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str,
    chapter_content: str,
    filepath: str,
    novel_number: int = 0,
    temperature: float = 0.1,  # 逻辑检查需要低温度
    max_tokens: int = 2048,
    timeout: int = 600
) -> str:
    """
    调用大模型对生成的章节进行逻辑自检
    """
    try:
        global_summary = read_file(os.path.join(filepath, "global_summary.txt"))
        character_state = read_file(os.path.join(filepath, "character_state.txt"))
        # 尝试读取章节大纲以获取下一章概要，若不存在则传空字符串
        directory_file = os.path.join(filepath, "Novel_directory.txt")
        next_chapter_outline = "（无后续目录信息）"
        try:
            if os.path.exists(directory_file):
                blueprint_text = read_file(directory_file)
                # 若传入了 novel_number，则取下一章信息
                if novel_number and novel_number > 0:
                    next_info = get_chapter_info_from_blueprint(blueprint_text, novel_number + 1)
                    if next_info:
                        next_chapter_outline = f"第{novel_number+1}章《{next_info.get('chapter_title','（未命名）')}》：定位：{next_info.get('chapter_role','')}; 简述：{next_info.get('chapter_summary','') }"
                else:
                    # 若未传入章节号，尽量摘取前几行作为概要
                    lines = blueprint_text.splitlines()
                    next_chapter_outline = '\n'.join(lines[:10]) if lines else next_chapter_outline
        except Exception:
            next_chapter_outline = "（读取目录失败）"

        prompt = LOGIC_CHECK_PROMPT.format(
            global_summary=global_summary,
            character_state=character_state,
            next_chapter_outline=next_chapter_outline,
            chapter_content=chapter_content
        )

        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        logging.info(f"开始逻辑自检，interface={interface_format}, model={model_name}")
        analysis_result = invoke_with_cleaning(llm_adapter, prompt)
        return analysis_result

    except Exception as e:
        logging.error(f"逻辑自检失败: {str(e)}")
        return f"逻辑自检发生错误: {str(e)}"

def rewrite_chapter_with_feedback(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str,
    original_content: str,
    feedback: str,
    temperature: float = 0.7,
    max_tokens: int = 4096,
    timeout: int = 600
) -> str:
    """
    根据反馈意见重写章节
    """
    try:
        prompt = REWRITE_WITH_FEEDBACK_PROMPT.format(
            original_content=original_content,
            feedback=feedback
        )

        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

        logging.info(f"开始根据反馈重写章节，interface={interface_format}, model={model_name}")
        new_content = invoke_with_cleaning(llm_adapter, prompt)
        return new_content

    except Exception as e:
        logging.error(f"重写失败: {str(e)}")
        return ""
    

def refine_chapter_detail(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str,
    chapter_range: str,         # 修改：传入范围描述，如 "第5-7章"
    novel_architecture: str,
    global_summary: str,
    current_outline: str,
    user_instruction: str,
    temperature: float = 0.7,
    max_tokens: int = 4096,     # 增加 token 限制，因为多章节内容较多
    timeout: int = 600
) -> str:
    """
    根据用户意见微调章节大纲 (支持多章节)
    """
    try:
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        prompt = REFINE_DIRECTORY_PROMPT.format(
            chapter_range=chapter_range,
            novel_architecture=novel_architecture if novel_architecture else "（暂无架构信息）",
            global_summary=global_summary if global_summary else "（暂无剧情摘要）",
            current_outline=current_outline,
            user_instruction=user_instruction
        )
        
        logging.info(f"正在微调大纲范围: {chapter_range} ...")
        refined_content = invoke_with_cleaning(llm_adapter, prompt)
        return refined_content
    except Exception as e:
        logging.error(f"微调章节大纲失败: {str(e)}")
        return ""
    

# =============== [新增函数] 执行主动验证流程 ===================
def perform_active_verification(
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    embedding_adapter,
    filepath: str,
    chapter_info: dict,
    # 新增：逻辑/选角模型专用参数
    cast_api_key: str | None = None,
    cast_base_url: str | None = None,
    cast_model_name: str | None = None,
    cast_interface_format: str | None = None,
    cast_temperature: float | None = None,
    cast_max_tokens: int | None = None,
    cast_timeout: int | None = None,
    max_tokens: int = 2048,
    timeout: int = 600
) -> str:
    """
    执行主动验证 RAG 流程：
    1. 识别风险 (Generate Questions)
    2. 检索证据 (Vector Search)
    3. 制定规则 (Generate Constraints)
    """
    logging.info("Starting Active Verification RAG process...")
    verif_if = (cast_interface_format or interface_format)
    verif_key = (cast_api_key or api_key)
    verif_url = (cast_base_url or base_url)
    verif_model = (cast_model_name or model_name)
    verif_temp = cast_temperature if cast_temperature is not None else 0.3
    verif_tokens = cast_max_tokens if cast_max_tokens is not None else max_tokens
    verif_to = cast_timeout if cast_timeout is not None else timeout

    llm_adapter = create_llm_adapter(
        interface_format=verif_if,
        base_url=verif_url,
        model_name=verif_model,
        api_key=verif_key,
        temperature=verif_temp,
        max_tokens=verif_tokens,
        timeout=verif_to,
    )

    # Step 1: 生成验证问题
    planner_prompt = ACTIVE_VERIFICATION_PLANNER_PROMPT.format(
        chapter_title=chapter_info.get('chapter_title'),
        chapter_role=chapter_info.get('chapter_role'),
        short_summary=chapter_info.get('short_summary'),
        characters_involved=chapter_info.get('characters_involved'),
        key_items=chapter_info.get('key_items'),
        scene_location=chapter_info.get('scene_location')
    )
    
    questions_raw = invoke_with_cleaning(llm_adapter, planner_prompt)
    
    # 解析列表
    questions = []
    try:
        # 尝试匹配列表结构 [ ... ]
        match = re.search(r'\[.*?\]', questions_raw, re.DOTALL)
        if match:
            # 注意：在生产环境中建议使用 json.loads 并确保 LLM 输出标准 JSON
            # 这里为了容错使用了 eval，但要小心安全风险
            try:
                questions = json.loads(match.group(0))
            except:
                questions = eval(match.group(0)) 
        else:
            # 降级策略：如果不是列表，按行分割
            questions = [line.strip('- ').strip() for line in questions_raw.split('\n') if '?' in line]
    except Exception as e:
        logging.warning(f"Failed to parse verification questions: {e}")
        return "（自动验证失败，请参考通用设定）"

    if not questions:
        logging.info("No specific verification questions generated.")
        return "（本章无特殊逻辑风险点）"

    logging.info(f"Verification Questions: {questions}")

    # Step 2 & 3: 检索并制定规则
    constraints = []
    
    # 限制最多验证前 5 个问题，避免耗时过长
    for q in questions[:5]: 
        # 向量检索
        context = get_relevant_context_from_vector_store(embedding_adapter, q, filepath, k=2)
        
        if not context:
            continue

        # 制定规则
        rule_prompt = ACTIVE_VERIFICATION_RULE_MAKER_PROMPT.format(
            question=q,
            retrieved_context=context
        )
        
        rule = invoke_with_cleaning(llm_adapter, rule_prompt)
        
        # 过滤掉无效回答
        if "无特定约束" not in rule and "No specific constraint" not in rule and len(rule) > 5:
            constraints.append(f"● [Query: {q}]\n  {rule}")

    if not constraints:
        return "（检索完成，未发现显著的设定冲突，请自由发挥）"

    return "\n".join(constraints)
</file>

<file path="llm_adapters.py">
# llm_adapters.py
# -*- coding: utf-8 -*-
import logging
from typing import Optional
from langchain_openai import ChatOpenAI, AzureChatOpenAI
from pydantic import SecretStr
from google import genai
from google.genai import types
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference.models import SystemMessage, UserMessage
from openai import OpenAI


def check_base_url(url: str) -> str:
    """
    处理base_url的规则：
    1. 如果url以#结尾，则移除#并直接使用用户提供的url
    2. 否则检查是否需要添加/v1后缀
    """
    import re
    url = url.strip()
    if not url:
        return url
        
    if url.endswith('#'):
        return url.rstrip('#')
        
    if not re.search(r'/v\d+$', url):
        if '/v1' not in url:
            url = url.rstrip('/') + '/v1'
    return url

class BaseLLMAdapter:
    """
    统一的 LLM 接口基类，为不同后端（OpenAI、Ollama、ML Studio、Gemini等）提供一致的方法签名。
    """
    def invoke(self, prompt: str) -> str:
        raise NotImplementedError("Subclasses must implement .invoke(prompt) method.")

class DeepSeekAdapter(BaseLLMAdapter):
    """
    适配官方/OpenAI兼容接口（使用 langchain.ChatOpenAI）
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=SecretStr(self.api_key),  # 修改此处
            base_url=self.base_url,
            max_completion_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from DeepSeekAdapter.")
            return ""
        # 确保 content 是字符串类型
        content = response.content
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return str(content)
        return ""

class OpenAIAdapter(BaseLLMAdapter):
    """
    适配官方/OpenAI兼容接口（使用 langchain.ChatOpenAI）
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=SecretStr(self.api_key),  # 修改此处
            base_url=self.base_url,
            max_completion_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )


    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from OpenAIAdapter.")
            return ""
        # 确保 content 是字符串类型
        content = response.content
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return str(content)
        return ""

class GeminiAdapter(BaseLLMAdapter):
    """
    适配 Google Gemini (Google Generative AI) 接口
    """
    def __init__(self, api_key: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = genai.Client(
            api_key=self.api_key,
            http_options={'timeout': self.timeout * 1000} if self.timeout else None # 新SDK部分版本支持http_options配置超时(毫秒)
        )

    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    max_output_tokens=self.max_tokens,
                    temperature=self.temperature,
                )
            )
            if response and response.text:
                return response.text
            else:
                logging.warning("No text response from Gemini API.")
                return ""
        except Exception as e:
            logging.error(f"Gemini API (google-genai) 调用失败: {e}")
            return ""

class AzureOpenAIAdapter(BaseLLMAdapter):
    """
    适配 Azure OpenAI 接口（使用 langchain.ChatOpenAI）
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        import re
        match = re.match(r'https://(.+?)/openai/deployments/(.+?)/chat/completions\?api-version=(.+)', base_url)
        if match:
            self.azure_endpoint = f"https://{match.group(1)}"
            self.azure_deployment = match.group(2)
            self.api_version = match.group(3)
        else:
            raise ValueError("Invalid Azure OpenAI base_url format")
        
        self.api_key = api_key
        self.model_name = self.azure_deployment
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = AzureChatOpenAI(
            azure_endpoint=self.azure_endpoint,
            azure_deployment=self.azure_deployment,
            api_version=self.api_version,
            api_key=SecretStr(self.api_key),
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )


    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from AzureOpenAIAdapter.")
            return ""
        # 确保 content 是字符串类型
        content = response.content
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return str(content)
        return ""


class OllamaAdapter(BaseLLMAdapter):
    """
    Ollama 同样有一个 OpenAI-like /v1/chat 接口，可直接使用 ChatOpenAI。
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        if self.api_key == '':
            self.api_key= 'ollama'

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=SecretStr(self.api_key),  # 修改此处
            base_url=self.base_url,
            max_completion_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )


    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from OllamaAdapter.")
            return ""
        # 确保 content 是字符串类型
        content = response.content
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return str(content)
        return ""


class MLStudioAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=SecretStr(self.api_key),  # 修改此处
            base_url=self.base_url,
            max_completion_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )


    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.invoke(prompt)
            if not response:
                logging.warning("No response from MLStudioAdapter.")
                return ""
            # 确保 content 是字符串类型
            content = response.content
            if isinstance(content, str):
                return content
            elif isinstance(content, list):
                return str(content)
            return ""
        except Exception as e:
            logging.error(f"ML Studio API 调用超时或失败: {e}")
            return ""


class AzureAIAdapter(BaseLLMAdapter):
    """
    适配 Azure AI Inference 接口，用于访问Azure AI服务部署的模型
    使用 azure-ai-inference 库进行API调用
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        import re
        # 匹配形如 https://xxx.services.ai.azure.com/models/chat/completions?api-version=xxx 的URL
        match = re.match(r'https://(.+?)\.services\.ai\.azure\.com(?:/models)?(?:/chat/completions)?(?:\?api-version=(.+))?', base_url)
        if match:
            # endpoint需要是形如 https://xxx.services.ai.azure.com/models 的格式
            self.endpoint = f"https://{match.group(1)}.services.ai.azure.com/models"
            # 如果URL中包含api-version参数，使用它；否则使用默认值
            self.api_version = match.group(2) if match.group(2) else "2024-05-01-preview"
        else:
            raise ValueError("Invalid Azure AI base_url format. Expected format: https://<endpoint>.services.ai.azure.com/models/chat/completions?api-version=xxx")
        
        self.base_url = self.endpoint  # 存储处理后的endpoint URL
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatCompletionsClient(
            endpoint=self.endpoint,
            credential=AzureKeyCredential(self.api_key),
            model=self.model_name,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.complete(
                messages=[
                    SystemMessage("You are a helpful assistant."),
                    UserMessage(prompt)
                ]
            )
            if response and response.choices:
                return response.choices[0].message.content
            else:
                logging.warning("No response from AzureAIAdapter.")
                return ""
        except Exception as e:
            logging.error(f"Azure AI Inference API 调用失败: {e}")
            return ""

# 火山引擎实现
class VolcanoEngineAIAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout  # 添加超时配置
        )
    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "你是DeepSeek，是一个 AI 人工智能助手"},
                    {"role": "user", "content": prompt},
                ],
                timeout=self.timeout
            )
            if not response:
                logging.warning("No response from VolcanoEngineAIAdapter.")
                return ""
            # 确保 content 是字符串类型，处理 None 的情况
            content = response.choices[0].message.content
            return content if content is not None else ""
        except Exception as e:
            logging.error(f"火山引擎API调用超时或失败: {e}")
            return ""


class SiliconFlowAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout  # 添加超时配置
        )
    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "你是DeepSeek，是一个 AI 人工智能助手"},
                    {"role": "user", "content": prompt},
                ],
                timeout=self.timeout  # 添加超时参数
            )
            if response and response.choices:
                content = response.choices[0].message.content
                # 确保返回字符串，不是 None
                return content if content is not None else ""
            else:
                logging.warning("No response from AzureAIAdapter.")
                return ""
        except Exception as e:
            logging.error(f"硅基流动API调用超时或失败: {e}")
            return ""

def create_llm_adapter(
    interface_format: str,
    base_url: str,
    model_name: str,
    api_key: str,
    temperature: float,
    max_tokens: int,
    timeout: int
) -> BaseLLMAdapter:
    """
    工厂函数：根据 interface_format 返回不同的适配器实例。
    """
    fmt = interface_format.strip().lower()
    if fmt == "deepseek":
        return DeepSeekAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "openai":
        return OpenAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "azure openai":
        return AzureOpenAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "azure ai":
        return AzureAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "ollama":
        return OllamaAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "ml studio":
        return MLStudioAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "gemini":
        # base_url 对 Gemini 暂无用处，可忽略
        return GeminiAdapter(api_key, model_name, max_tokens, temperature, timeout)
    elif fmt == "阿里云百炼":
        return OpenAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "火山引擎":
        return VolcanoEngineAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "硅基流动":
        return SiliconFlowAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    else:
        raise ValueError(f"Unknown interface_format: {interface_format}")
</file>

<file path="ui/main_window.py">
# ui/main_window.py
# -*- coding: utf-8 -*-
import os
import threading
import logging
import traceback
import customtkinter as ctk
import tkinter as tk
from tkinter import filedialog, messagebox
from .role_library import RoleLibrary
from llm_adapters import create_llm_adapter

from config_manager import load_config, save_config, test_llm_config, test_embedding_config
from utils import read_file, save_string_to_txt, clear_file_content
from tooltips import tooltips

from ui.context_menu import TextWidgetContextMenu
from ui.main_tab import build_main_tab, build_left_layout, build_right_layout
from ui.config_tab import build_config_tabview, load_config_btn, save_config_btn
from ui.novel_params_tab import build_novel_params_area, build_optional_buttons_area
from ui.generation_handlers import (
    generate_novel_architecture_ui,
    generate_chapter_blueprint_ui,
    generate_chapter_draft_ui,
    finalize_chapter_ui,
    do_consistency_check,
    import_knowledge_handler,
    clear_vectorstore_handler,
    show_plot_arcs_ui,
    generate_batch_ui,
    refine_directory_card_ui,
    continue_directory_ui,  # 新增续写目录函数
    show_foreshadowing_records_ui,
    show_novel_qa_ui as _show_novel_qa_ui
)
from ui.setting_tab import build_setting_tab, load_novel_architecture, save_novel_architecture
from ui.directory_tab import build_directory_tab, load_chapter_blueprint, save_chapter_blueprint
from ui.character_tab import build_character_tab, load_character_state, save_character_state
from ui.summary_tab import build_summary_tab, load_global_summary, save_global_summary
from ui.chapters_tab import build_chapters_tab, refresh_chapters_list, on_chapter_selected, load_chapter_content, save_current_chapter, prev_chapter, next_chapter
from ui.other_settings import build_other_settings_tab


class NovelGeneratorGUI:
    """
    小说生成器的主GUI类，包含所有的界面布局、事件处理、与后端逻辑的交互等。
    """
    def __init__(self, master):
        self.master = master
        # -- 声明将在 build_* 函数中初始化的属性 --
        self.log_text: ctk.CTkTextbox
        self.char_inv_text: ctk.CTkTextbox
        self.chapter_result: ctk.CTkTextbox
        self.tabview: ctk.CTkTabview
        self.master.title("Novel Generator GUI")
        try:
            if os.path.exists("icon.ico"):
                self.master.iconbitmap("icon.ico")
        except Exception:
            pass
        self.master.geometry("1350x840")

        # --------------- 配置文件路径 ---------------
        self.config_file = "config.json"
        self.loaded_config = load_config(self.config_file)

        if self.loaded_config:
            last_llm = next(iter(self.loaded_config["llm_configs"].values())).get("interface_format", "OpenAI")

            last_embedding = self.loaded_config.get("last_embedding_interface_format", "OpenAI")
        else:
            last_llm = "OpenAI"
            last_embedding = "OpenAI"

        # if self.loaded_config and "llm_configs" in self.loaded_config and last_llm in self.loaded_config["llm_configs"]:
        #     llm_conf = next(iter(self.loaded_config["llm_configs"]))
        # else:
        #     llm_conf = {
        #         "api_key": "",
        #         "base_url": "https://api.openai.com/v1",
        #         "model_name": "gpt-4o-mini",
        #         "temperature": 0.7,
        #         "max_tokens": 8192,
        #         "timeout": 600
        #     }
        llm_conf = next(iter(self.loaded_config["llm_configs"].values()))
        choose_configs = self.loaded_config.get("choose_configs", {})


        if self.loaded_config and "embedding_configs" in self.loaded_config and last_embedding in self.loaded_config["embedding_configs"]:
            emb_conf = self.loaded_config["embedding_configs"][last_embedding]
        else:
            emb_conf = {
                "api_key": "",
                "base_url": "https://api.openai.com/v1",
                "model_name": "text-embedding-ada-002",
                "retrieval_k": 4
            }

        # PenBo 增加代理功能支持
        proxy_url = self.loaded_config["proxy_setting"]["proxy_url"]
        proxy_port = self.loaded_config["proxy_setting"]["proxy_port"]
        if self.loaded_config["proxy_setting"]["enabled"]:
            os.environ['HTTP_PROXY'] = f"http://{proxy_url}:{proxy_port}"
            os.environ['HTTPS_PROXY'] = f"http://{proxy_url}:{proxy_port}"
        else:
            os.environ.pop('HTTP_PROXY', None)  
            os.environ.pop('HTTPS_PROXY', None)



        # -- LLM通用参数 --
        # self.llm_conf_name = next(iter(self.loaded_config["llm_configs"]))
        self.api_key_var = ctk.StringVar(value=llm_conf.get("api_key", ""))
        self.base_url_var = ctk.StringVar(value=llm_conf.get("base_url", "https://api.openai.com/v1"))
        self.interface_format_var = ctk.StringVar(value=llm_conf.get("interface_format", "OpenAI"))
        self.model_name_var = ctk.StringVar(value=llm_conf.get("model_name", "gpt-4o-mini"))
        self.temperature_var = ctk.DoubleVar(value=llm_conf.get("temperature", 0.7))
        self.max_tokens_var = ctk.IntVar(value=llm_conf.get("max_tokens", 8192))
        self.timeout_var = ctk.IntVar(value=llm_conf.get("timeout", 600))
        self.interface_config_var = ctk.StringVar(value=next(iter(self.loaded_config["llm_configs"])))



        # -- Embedding相关 --
        self.embedding_interface_format_var = ctk.StringVar(value=last_embedding)
        self.embedding_api_key_var = ctk.StringVar(value=emb_conf.get("api_key", ""))
        self.embedding_url_var = ctk.StringVar(value=emb_conf.get("base_url", "https://api.openai.com/v1"))
        self.embedding_model_name_var = ctk.StringVar(value=emb_conf.get("model_name", "text-embedding-ada-002"))
        self.embedding_retrieval_k_var = ctk.StringVar(value=str(emb_conf.get("retrieval_k", 4)))


        # -- 生成配置相关 --
        self.architecture_llm_var = ctk.StringVar(value=choose_configs.get("architecture_llm", "DeepSeek"))
        self.chapter_outline_llm_var = ctk.StringVar(value=choose_configs.get("chapter_outline_llm", "DeepSeek"))
        self.final_chapter_llm_var = ctk.StringVar(value=choose_configs.get("final_chapter_llm", "DeepSeek"))
        self.consistency_review_llm_var = ctk.StringVar(value=choose_configs.get("consistency_review_llm", "DeepSeek"))
        self.prompt_draft_llm_var = ctk.StringVar(value=choose_configs.get("prompt_draft_llm", "DeepSeek"))
        self.refine_logic_llm_var = ctk.StringVar(value=choose_configs.get("refine_logic_llm", "DeepSeek"))
        self.logic_rewrite_llm_var = ctk.StringVar(value=choose_configs.get("logic_rewrite_llm", "DeepSeek"))




        # -- 小说参数相关 --
        if self.loaded_config and "other_params" in self.loaded_config:
            op = self.loaded_config["other_params"]
            self.topic_default = op.get("topic", "")
            self.genre_var = ctk.StringVar(value=op.get("genre", "玄幻"))
            self.num_chapters_var = ctk.StringVar(value=str(op.get("num_chapters", 10)))
            self.word_number_var = ctk.StringVar(value=str(op.get("word_number", 3000)))
            self.filepath_var = ctk.StringVar(value=op.get("filepath", ""))
            self.chapter_num_var = ctk.StringVar(value=str(op.get("chapter_num", "1")))
            self.characters_involved_var = ctk.StringVar(value=op.get("characters_involved", ""))
            self.key_items_var = ctk.StringVar(value=op.get("key_items", ""))
            self.scene_location_var = ctk.StringVar(value=op.get("scene_location", ""))
            self.time_constraint_var = ctk.StringVar(value=op.get("time_constraint", ""))
            self.user_guidance_default = op.get("user_guidance", "")
            self.webdav_url_var = ctk.StringVar(value=op.get("webdav_url", ""))
            self.webdav_username_var = ctk.StringVar(value=op.get("webdav_username", ""))
            self.webdav_password_var = ctk.StringVar(value=op.get("webdav_password", ""))

        else:
            self.topic_default = ""
            self.genre_var = ctk.StringVar(value="玄幻")
            self.num_chapters_var = ctk.StringVar(value="10")
            self.word_number_var = ctk.StringVar(value="3000")
            self.filepath_var = ctk.StringVar(value="")
            self.chapter_num_var = ctk.StringVar(value="1")
            self.characters_involved_var = ctk.StringVar(value="")
            self.key_items_var = ctk.StringVar(value="")
            self.scene_location_var = ctk.StringVar(value="")
            self.time_constraint_var = ctk.StringVar(value="")
            self.user_guidance_default = ""

            self.webdav_url_var = ctk.StringVar(value="")
            self.webdav_username_var = ctk.StringVar(value="")
            self.webdav_password_var = ctk.StringVar(value="")

        # --------------- 整体Tab布局 ---------------
        self.tabview = ctk.CTkTabview(self.master)
        self.tabview.pack(fill="both", expand=True)

        # 创建各个标签页
        build_main_tab(self)
        build_config_tabview(self)
        build_novel_params_area(self, start_row=1)
        build_optional_buttons_area(self, start_row=2)
        build_setting_tab(self)
        build_directory_tab(self)
        build_character_tab(self)
        build_summary_tab(self)
        build_chapters_tab(self)
        build_other_settings_tab(self)

        self.show_novel_qa_ui = lambda: _show_novel_qa_ui(self)


    # ----------------- 通用辅助函数 -----------------
    def show_tooltip(self, key: str):
        info_text = tooltips.get(key, "暂无说明")
        messagebox.showinfo("参数说明", info_text)

    def safe_get_int(self, var, default=1):
        try:
            val_str = str(var.get()).strip()
            return int(val_str)
        except:
            var.set(str(default))
            return default

    def log(self, message: str):
        self.log_text.configure(state="normal")
        self.log_text.insert("end", message + "\n")
        self.log_text.see("end")
        self.log_text.configure(state="disabled")

    def safe_log(self, message: str):
        self.master.after(0, lambda: self.log(message))

    def disable_button_safe(self, btn):
        self.master.after(0, lambda: btn.configure(state="disabled"))

    def enable_button_safe(self, btn):
        self.master.after(0, lambda: btn.configure(state="normal"))

    def handle_exception(self, context: str):
        full_message = f"{context}\n{traceback.format_exc()}"
        logging.error(full_message)
        self.safe_log(full_message)

    def show_chapter_in_textbox(self, text: str):
        self.chapter_result.delete("0.0", "end")
        self.chapter_result.insert("0.0", text)
        self.chapter_result.see("end")
    
    def test_llm_config(self):
        """
        测试当前的LLM配置是否可用
        """
        interface_format = self.interface_format_var.get().strip()
        api_key = self.api_key_var.get().strip()
        base_url = self.base_url_var.get().strip()
        model_name = self.model_name_var.get().strip()
        temperature = self.temperature_var.get()
        max_tokens = self.max_tokens_var.get()
        timeout = self.timeout_var.get()

        test_llm_config(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            log_func=self.safe_log,
            handle_exception_func=self.handle_exception
        )

    def test_embedding_config(self):
        """
        测试当前的Embedding配置是否可用
        """
        api_key = self.embedding_api_key_var.get().strip()
        base_url = self.embedding_url_var.get().strip()
        interface_format = self.embedding_interface_format_var.get().strip()
        model_name = self.embedding_model_name_var.get().strip()

        test_embedding_config(
            api_key=api_key,
            base_url=base_url,
            interface_format=interface_format,
            model_name=model_name,
            log_func=self.safe_log,
            handle_exception_func=self.handle_exception
        )
    
    def browse_folder(self):
        selected_dir = filedialog.askdirectory()
        if selected_dir:
            self.filepath_var.set(selected_dir)

    def show_character_import_window(self):
        """显示角色导入窗口"""
        import_window = ctk.CTkToplevel(self.master)
        import_window.title("导入角色信息")
        import_window.geometry("600x500")
        import_window.transient(self.master)  # 设置为父窗口的临时窗口
        import_window.grab_set()  # 保持窗口在顶层
        
        # 主容器
        main_frame = ctk.CTkFrame(import_window)
        main_frame.pack(fill="both", expand=True, padx=10, pady=10)
        
        # 滚动容器
        scroll_frame = ctk.CTkScrollableFrame(main_frame)
        scroll_frame.pack(fill="both", expand=True, padx=5, pady=5)
        
        # 获取角色库路径
        role_lib_path = os.path.join(self.filepath_var.get().strip(), "角色库")
        self.selected_roles = []  # 存储选中的角色名称
        
        # 动态加载角色分类
        if os.path.exists(role_lib_path):
            # 配置网格布局参数
            scroll_frame.columnconfigure(0, weight=1)
            max_roles_per_row = 4
            current_row = 0
            
            for category in os.listdir(role_lib_path):
                category_path = os.path.join(role_lib_path, category)
                if os.path.isdir(category_path):
                    # 创建分类容器
                    category_frame = ctk.CTkFrame(scroll_frame)
                    category_frame.grid(row=current_row, column=0, sticky="w", pady=(10,5), padx=5)
                    
                    # 添加分类标签
                    category_label = ctk.CTkLabel(category_frame, text=f"【{category}】", 
                                                font=("Microsoft YaHei", 12, "bold"))
                    category_label.grid(row=0, column=0, padx=(0,10), sticky="w")
                    
                    # 初始化角色排列参数
                    role_count = 0
                    row_num = 0
                    col_num = 1  # 从第1列开始（第0列是分类标签）
                    
                    # 添加角色复选框
                    for role_file in os.listdir(category_path):
                        if role_file.endswith(".txt"):
                            role_name = os.path.splitext(role_file)[0]
                            if not any(name == role_name for _, name in self.selected_roles):
                                chk = ctk.CTkCheckBox(category_frame, text=role_name)
                                chk.grid(row=row_num, column=col_num, padx=5, pady=2, sticky="w")
                                self.selected_roles.append((chk, role_name))
                                
                                # 更新行列位置
                                role_count += 1
                                col_num += 1
                                if col_num > max_roles_per_row:
                                    col_num = 1
                                    row_num += 1
                    
                    # 如果没有角色，调整分类标签占满整行
                    if role_count == 0:
                        category_label.grid(columnspan=max_roles_per_row+1, sticky="w")
                    
                    # 更新主布局的行号
                    current_row += 1
                    
                    # 添加分隔线
                    separator = ctk.CTkFrame(scroll_frame, height=1, fg_color="gray")
                    separator.grid(row=current_row, column=0, sticky="ew", pady=5)
                    current_row += 1
        
        # 底部按钮框架
        btn_frame = ctk.CTkFrame(main_frame)
        btn_frame.pack(fill="x", pady=10)
        
        # 选择按钮
        def confirm_selection():
            selected = [name for chk, name in self.selected_roles if chk.get() == 1]
            self.char_inv_text.delete("0.0", "end")
            self.char_inv_text.insert("0.0", ", ".join(selected))
            import_window.destroy()
            
        btn_confirm = ctk.CTkButton(btn_frame, text="选择", command=confirm_selection)
        btn_confirm.pack(side="left", padx=20)
        
        # 取消按钮
        btn_cancel = ctk.CTkButton(btn_frame, text="取消", command=import_window.destroy)
        btn_cancel.pack(side="right", padx=20)

    def show_role_library(self):
        save_path = self.filepath_var.get().strip()
        if not save_path:
            messagebox.showwarning("警告", "请先设置保存路径")
            return
        
        # [修改] 初始化LLM适配器：使用 "refine_logic_llm_var"
        try:
            logic_llm_key = self.refine_logic_llm_var.get()
            logic_config = self.loaded_config["llm_configs"][logic_llm_key]
            
            llm_adapter = create_llm_adapter(
                interface_format=logic_config.get("interface_format", "OpenAI"),
                base_url=logic_config.get("base_url", ""),
                model_name=logic_config.get("model_name", ""),
                api_key=logic_config.get("api_key", ""),
                temperature=float(logic_config.get("temperature", 0.7)),
                max_tokens=int(logic_config.get("max_tokens", 4096)),
                timeout=int(logic_config.get("timeout", 600))
            )
        except Exception as e:
            messagebox.showerror("配置错误", f"无法加载逻辑/选角模型配置: {str(e)}")
            return
        
        # 传递LLM适配器实例到角色库
        if hasattr(self, '_role_lib'):
            if self._role_lib.window and self._role_lib.window.winfo_exists():
                self._role_lib.window.destroy()
        
        self._role_lib = RoleLibrary(self.master, save_path, llm_adapter)

    # ----------------- 将导入的各模块函数直接赋给类方法 -----------------
    generate_novel_architecture_ui = generate_novel_architecture_ui
    generate_chapter_blueprint_ui = generate_chapter_blueprint_ui
    generate_chapter_draft_ui = generate_chapter_draft_ui
    finalize_chapter_ui = finalize_chapter_ui
    do_consistency_check = do_consistency_check
    generate_batch_ui = generate_batch_ui
    refine_directory_card_ui = refine_directory_card_ui
    continue_directory_ui = continue_directory_ui
    import_knowledge_handler = import_knowledge_handler
    clear_vectorstore_handler = clear_vectorstore_handler
    show_plot_arcs_ui = show_plot_arcs_ui
    show_foreshadowing_records_ui = show_foreshadowing_records_ui
    show_novel_qa_ui = _show_novel_qa_ui
    load_config_btn = load_config_btn
    save_config_btn = save_config_btn
    load_novel_architecture = load_novel_architecture
    save_novel_architecture = save_novel_architecture
    load_chapter_blueprint = load_chapter_blueprint
    save_chapter_blueprint = save_chapter_blueprint
    load_character_state = load_character_state
    save_character_state = save_character_state
    load_global_summary = load_global_summary
    save_global_summary = save_global_summary
    refresh_chapters_list = refresh_chapters_list
    on_chapter_selected = on_chapter_selected
    save_current_chapter = save_current_chapter
    prev_chapter = prev_chapter
    next_chapter = next_chapter
    test_llm_config = test_llm_config
    test_embedding_config = test_embedding_config
    browse_folder = browse_folder
</file>

<file path="prompt_definitions.py">
# prompt_definitions.py
# -*- coding: utf-8 -*-
"""
集中存放所有提示词 (Prompt)，整合雪花写作法、角色弧光理论、悬念三要素模型等
并包含新增加的前三章摘要/下一章关键字提炼提示词，以及章节正文写作提示词。
"""

# =============== 生成草稿提示词当前章节摘要、知识库提炼 ===============
# 当前章节摘要生成提示词
summarize_recent_chapters_prompt = """\
你是一名辅助小说写作过程的章节摘要生成助手。
当前任务是：为【第{novel_number}章《{chapter_title}》的正文写作】生成一份**精简、克制、聚焦核心**的剧情大纲。
目标：**严格控制内容量**，防止正文扩写时字数失控。只保留核心冲突，砍掉无效的过渡细节。

**⚠️ 极其重要的约束：**
- 你的输出**必须仅涉及第{novel_number}章的内容**。
- **绝对禁止**生成、提及或猜测下一章（第{next_chapter_number}章）的内容。
- 如果感到困惑，请参考下一章信息只是用于检查本章结尾的**逻辑衔接点**，而非生成内容。

已完成章节内容（仅作背景参考）：
{global_summary}

**上一章结尾内容（用于剧情衔接参考）：**
{previous_chapter_excerpt}

**【作者特殊指导（必须遵守）】：**
{user_guidance}

第{novel_number}章《{chapter_title}》：
├── 本章定位：{chapter_role}
├── 核心作用：{chapter_purpose}
├── 悬念密度：{suspense_level}
├── 伏笔操作：{foreshadowing}
├── 认知颠覆：{plot_twist_level}
└── 本章简述：{chapter_summary}

【伏笔线索库 (请在安排剧情时尝试回收短期伏笔并推进长期伏笔)】：
{foreshadowing_records}

【关键人物关系网】：
{character_relationships}

【后续逻辑衔接参考 (仅用于检查本章结尾，不进行生成)】：
下一章为第{next_chapter_number}章《{next_chapter_title}》，定位为【{next_chapter_role}】。
你需要确保本章结尾能够为第{next_chapter_number}章的这个定位提供合理的起点，但你的输出内容必须完全聚焦于第{novel_number}章。

【当前章节摘要生成要求】：
**首先，用一句话概括【本章故事内核】：**
（格式：核心人物+核心目标+核心阻碍/转折。例：姜璃潜入敌营寻找名单，却发现自己人竟是内鬼。）

**然后，按以下结构生成关键节点：**
1.  **结构要求：剧情关键帧（Key Plot Frames）**
    请将【本章】剧情推进浓缩为 **2-4个** 核心事件节点。
    **核心指令**：每个节点下，用"**人物+核心动作**"的极简句式描述事件（例如：姜璃揭露阵法秘密）。不描写环境、神态、心理活动等细节。
    **格式如下**：
    - **【节点一】**[地点/场景]
      - **事件**：[谁] + [做了什么]（主谓宾，指向情节推进）。
      - **看点**：本段的核心冲突、爽点、悬念爆发点或关键信息揭露。
    - **>>【转场】**：（如必要）用几个词简述场景转换，如"受袭后遁走"、"谈话结束，前往"。
    - **【节点二】**[地点/场景] ...

2.  **做减法原则**：
    - **合并琐事**：行走、等待、日常寒暄等过渡情节，必须合并为一句带过或直接省略。
    - **聚焦动作**：每个节点只保留推动【本章】剧情必不可缺的动作和对话结果。

3.  **逻辑一致性检查（最高优先级！）**：
    - **时间连续**：严格检查前文结尾状态（如"受伤"、"深夜"），本章开头必须承接。
    - **人设统一**：确保人物称呼、性别、已知能力与前文设定一致。**人物姓名、称号必须唯一且不可编造**。
    - **身份与指代**：若前文用"那兽人"等指代而未直呼其名，需在摘要中明确其身份（如"即血煞"），避免后文混淆。
    - **状态合理**：受伤角色不能无故痊愈，体力消耗后需体现影响。
    - **未来适配**：检查本章结尾状态是否能为第{next_chapter_number}章（【{next_chapter_role}】）提供合理的起点。

4.  **伏笔操作要求**：
    - **回收旧伏笔**：优先考虑回收【伏笔线索库】中短线伏笔。
    - **埋设新伏笔**：为后续关键事件埋设合理铺垫（但不要过度剧透）。

**5. 作者指导整合**：
    - **严格遵循**【作者特殊指导】中的具体要求。
    - 将作者指导中的要点融入到剧情关键节点中。
    - 确保本章内容体现作者的特殊意图。

**⚠️ 最终检查清单（输出前必读）：**
- [ ] 我的摘要内容**100%只涉及第{novel_number}章**，完全不包含第{next_chapter_number}章的事件吗？
- [ ] 我没有写"接下来"、"下一步"、"第{next_chapter_number}章"这样的下章引导语吗？
- [ ] 我的结尾状态合理地为第{next_chapter_number}章做了铺垫吗？
- [ ] 我是否充分考虑了【作者特殊指导】的要求并将其融入了剧情设计？

请直接 output优化后的章节摘要，无需额外解释。
"""

# 知识库相关性检索提示词
knowledge_search_prompt = """\
请基于以下当前写作需求，生成合适的知识库检索关键词：

章节元数据：
- 准备创作：第{chapter_number}章
- 章节主题：{chapter_title}
- 核心人物：{characters_involved}
- 关键道具：{key_items}
- 场景位置：{scene_location}

写作目标：
- 本章定位：{chapter_role}
- 核心作用：{chapter_purpose}
- 伏笔操作：{foreshadowing}

当前摘要：
{short_summary}

- 作者隐式设定/补充指导（可能有身份映射、派系设定等）：
{user_guidance}

- 核心人物(可能未指定)：{characters_involved}
- 关键道具(可能未指定)：{key_items}
- 空间坐标(可能未指定)：{scene_location}
- 时间压力(可能未指定)：{time_constraint}

生成规则：组件化检索 
请关注以下四类信息的检索：
1.  **人物身份与关系检索**（优先级最高）：
    * 针对【核心人物】，检索其**准确姓名、称号、身份映射**（如前文用"那兽人"指代，需确认其真名）。
    * 检索**人物之间的关系、动机、派系归属**。
    * *Example*: "血煞 兽人 身份", "叶落 姜璃 关系", "古尔氏族 纹路 派系"
2.  **容器检索 (Container Search)**：
    * 针对【场景位置】，检索其**静态设定**（布局、光线、气味、NPC外貌）。
    * *Example*: "百草堂 内部陈设", "百草堂 掌柜外貌"
3.  **载荷检索 (Payload Search)**：
    * 针对【核心人物】，检索其**可携带状态**（随身物品、当前伤势、战利品）。
    * *Example*: "叶落 战利品列表", "叶落 当前伤势"
4.  **逻辑检索 (Logic Search)**：
    * 针对【章节意图】，检索相关的**世界观规则、派系设定、符号含义**。
    * *Example*: "兽人纹路 派系象征", "兽人材料 收购价格"

请生成 4-6 组检索词，按优先级降序排列。优先检索人物身份与关系。
格式：每组用"·"连接 2-3 个关键词，每组占一行。
"""

# 知识库内容过滤提示词
knowledge_filter_prompt = """\
你是一名极其严格的**剧情素材筛选官**。
你的任务是：从一堆杂乱的检索片段中，挑选出**对当前章节写作绝对必要**的信息，
特别是确保人物名称、地点名称、技能名称等关键信息的一致性。
**重要**：若检索片段中有"作者隐式设定"或"身份映射"（如"被杀兽人=血煞"、"纹路=派系象征"），必须优先保留。

【输入数据】
1. **本章写作目标**：
{chapter_info}

2. **作者隐式设定**（若提供，写作时必须遵守，优先于推断）：
{author_implicit_settings}

3. **待筛选的检索片段**（可能包含大量无关噪音）：
{retrieved_texts}

【筛选核心原则：准确性与一致性】
保留信息的标准：
1. **人物名称与身份映射**：必须保留准确的人物姓名、称号；若有隐式指代（如"那兽人"实为"血煞"），必须明确写出
2. **地点名称**：必须保留准确的地点、场景名称
3. **技能/道具名称**：必须保留准确的技能、功法、道具名称
4. **人物关系与动机**：必须保留人物间关系、行动动机、派系归属
5. **符号与设定含义**：若涉及纹路、标记等，保留其准确含义（如"纹路=古尔氏族派系象征"）

【分类筛选规则】

1. **人物信息（优先级最高）**
   - 保留：人物的准确姓名、称号、外貌特征
   - 保留：人物与其他人的确切关系（如"血煞是兽人部落的首领"）
   - 保留：人物的关键属性（如所属派系、地位等）

2. **场景视觉锚点（必须严格匹配当前场景）**
   - 保留：**仅限当前章节发生地**的具体视觉元素（柜台、招牌、特殊装饰）。
   - 保留：场景的独特特征和布局信息
   - 丢弃：所有非当前场景的描述（如"客栈窗棂"如果本章不在客栈）。
   - 要求：必须是**可观察**的静态物体，而不是光影、气味等主观感受。

3. **人物状态（必须是可执行的状态）**
   - 保留：直接影响本章行动的数值/状态（剩余灵石数、具体伤势、携带的关键物品）。
   - 保留：人物的当前状态（身体、心理、能力状态）
   - 丢弃：抽象描述（如"感知清晰"）、已经过期的状态、心理感受（除非是本章冲突核心）。
   - 格式：必须转换为"可执行指令"，如："叶落：灵石余额23块"而非"叶落有23块灵石"。

4. **世界观规则（必须是本章会触发的规则）**
   - 保留：**本章剧情会直接用到**的交易价格、战斗规则、禁忌条款。
   - 保留：世界观设定、派系组织信息、技能体系等
   - 丢弃：所有"背景知识"，除非角色在本章会主动提及或使用。

5. **人物关系（必须是本章会互动的关系）**
   - 保留：**本章出场角色之间**的具体关系状态（如"杨尘欠叶落一个人情"）。
   - 保留：角色之间的准确称呼和关系描述
   - 丢弃：所有历史关系、不在场角色的关系。

【输出格式】
*按以下格式组织，无关类别留空*

1. 【人物信息】（人物姓名、称号、特征、关系）
   - 例：血煞：兽人，部落首领，身上的纹路代表古尔氏族

2. 【场景物件】（具体物体名称+位置）
   - 例：百草堂·柜台：由青金石打造，表面有细微划痕

3. 【数值状态】（人物/物品的具体数值）
   - 例：叶落·灵石余额：23块下品灵石
   - 例：姜璃·伤势：左肩箭伤未愈，移动时隐痛

4. 【可用规则】（本章会触发的规则）
   - 例：兽人材料收购价：狼牙5灵石/颗，熊掌20灵石/对

5. 【本章关系】（出场角色间的具体关系）
   - 例：杨尘→姜璃：称呼"师姐"，信任度高，可托付秘密

6. 【作者隐式设定/身份映射】（若作者隐式设定中有身份映射、符号含义等，必须在此列出）
   - 例：前文"被杀的那兽人"=血煞（本名）
   - 例：兽人身上的纹路=古尔氏族派系象征，非实验导致

【严禁】
- 不输出任何解释性文字
- 不保留"可能有用"的模糊信息
- 环境描写必须具体到物体，不得出现"氛围描写"
- 严禁对人物、地点、技能等名称进行改写或猜测
"""

# =============== 1. 核心种子设定（雪花第1层）===================
core_seed_prompt = """\
作为专业作家，请用"雪花写作法"第一步构建故事核心：
主题：{topic}
类型：{genre}
篇幅：约{number_of_chapters}章（每章{word_number}字）

请用单句公式概括故事本质，例如：
"当[主角]遭遇[核心事件]，必须[关键行动]，否则[灾难后果]；与此同时，[隐藏的更大危机]正在发酵。"

要求：
1. 必须包含显性冲突与潜在危机
2. 体现人物核心驱动力
3. 暗示世界观关键矛盾
4. 使用25-100字精准表达

仅返回故事核心文本，不要解释任何内容。
"""

# =============== 2. 角色动力学设定（角色弧光模型）===================
character_dynamics_prompt = """\
基于以下元素：
- 内容指导：{user_guidance}
- 核心种子：{core_seed}

请设计3-6个具有动态变化潜力的核心角色，每个角色需包含：
特征：
- 背景、外貌、性别、年龄、职业等
- 暗藏的秘密或潜在弱点(可与世界观或其他角色有关)

核心驱动力三角：
- 表面追求（物质目标）
- 深层渴望（情感需求）
- 灵魂需求（哲学层面）

角色弧线设计：
初始状态 → 触发事件 → 认知失调 → 蜕变节点 → 最终状态

关系冲突网：
- 与其他角色的关系或对立点
- 与至少两个其他角色的价值观冲突
- 一个合作纽带
- 一个隐藏的背叛可能性

要求：
仅给出最终文本，不要解释任何内容。
"""

# =============== 3. 世界构建矩阵（三维度交织法）===================
world_building_prompt = """\
基于以下元素：
- 内容指导：{user_guidance}
- 核心冲突："{core_seed}"

为服务上述内容，请构建三维交织的世界观：

1. 物理维度：
- 空间结构（地理×社会阶层分布图）
- 时间轴（关键历史事件年表）
- 法则体系（物理/魔法/社会规则的漏洞点）

2. 社会维度：
- 权力结构断层线（可引发冲突的阶层/种族/组织矛盾）
- 文化禁忌（可被打破的禁忌及其后果）
- 经济命脉（资源争夺焦点）

3. 隐喻维度：
- 贯穿全书的视觉符号系统（如反复出现的意象）
- 气候/环境变化映射的心理状态
- 建筑风格暗示的文明困境

要求：
每个维度至少包含3个可与角色决策产生互动的动态元素。
仅给出最终文本，不要解释任何内容。
"""

# =============== 4. 情节架构（三幕式悬念）===================
plot_architecture_prompt = """\
基于以下元素：
- 内容指导：{user_guidance}
- 核心种子：{core_seed}
- 角色体系：{character_dynamics}
- 世界观：{world_building}

要求按以下结构设计：
第一幕（触发） 
- 日常状态中的异常征兆（3处铺垫）
- 引出故事：展示主线、暗线、副线的开端
- 关键事件：打破平衡的催化剂（需改变至少3个角色的关系）
- 错误抉择：主角的认知局限导致的错误反应

第二幕（对抗）
- 剧情升级：主线+副线的交叉点
- 双重压力：外部障碍升级+内部挫折
- 虚假胜利：看似解决实则深化危机的转折点
- 灵魂黑夜：世界观认知颠覆时刻

第三幕（解决）
- 代价显现：解决危机必须牺牲的核心价值
- 嵌套转折：至少包含三层认知颠覆（表面解→新危机→终极抉择）
- 余波：留下2个开放式悬念因子

每个阶段需包含3个关键转折点及其对应的伏笔回收方案。
仅给出最终文本，不要解释任何内容。
"""

# =============== 5. 章节目录生成（悬念节奏曲线）===================
chapter_blueprint_prompt = """\
# Role: 小说章节目录生成助手

## Profile
- author: LangGPT
- version: 2.0
- language: 中文
- description:
  你是一名辅助小说创作的章节目录生成助手，
  目标是生成“作者可用、系统可解析”的章节目录结构。

## Background
当前任务为【章节目录规划阶段】。
章节目录用于：
- 帮助作者把握整体写作方向
- 为后续章节摘要与正文生成提供参考
而非剧情定稿或策划文档。

## Goals
- 生成若干连续章节的章节目录
- 每一章包含必要字段，但保持描述宽松
- 避免过度设计与写死剧情

## OutputFormat（必须严格遵守）
请严格按以下格式输出，每一章之间空一行：

第n章 - 《章节标题》
本章定位：
核心作用：
悬念密度：
伏笔操作：
认知颠覆：★☆☆☆☆
本章简述：

## Rules
1. 关于章节标题：
   - 标题简洁、直观
   - 可偏事件、对话、人物状态或关系变化
   - 避免空泛或过度文艺化

2. 关于各字段的生成原则：
   - 本章定位：一句话说明本章整体性质（如：日常 / 对话 / 过渡 / 推进）
   - 核心作用：概括本章在整体中的作用，不必写完整逻辑
   - 悬念密度：使用自然语言（低 / 中 / 偏高），不强制制造悬念
   - 伏笔操作：可写“无 / 暗示 / 轻微铺垫 / 信息提示”，允许模糊
   - 认知颠覆：使用星级表示（★☆☆☆☆ 起），大多数章节可偏低
   - 本章简述：2–4 句话，概括本章主要内容即可，不展开细节

3. 节奏与自由度：
   - 允许连续多章为日常或信息推进
   - 不要求每章都有冲突或反转
   - 不提前设计复杂桥段

4. 禁止事项：
   - 不使用策划案或教学语言
   - 不拆解因果链
   - 不在目录阶段写“必然发生”的剧情结论

## Workflows
1. 理解小说当前所处阶段
2. 按自然叙事顺序规划章节推进
3. 为每一章填写上述字段，但保持描述克制

## Init
请根据以下信息生成章节目录：
- 小说背景与当前进度：{novel_architecture}
- 需要生成的章节数量：{number_of_chapters}

请直接 output章节目录，不要额外解释。

"""

chunked_chapter_blueprint_prompt = """\
# Role: 小说章节目录生成助手

## Profile
- author: LangGPT
- version: 2.0
- language: 中文
- description:
  你是一名辅助小说创作的章节目录生成助手，
  目标是生成“作者可用、系统可解析”的章节目录结构。

## Background
当前任务为【章节目录规划阶段】。
章节目录用于：
- 帮助作者把握整体写作方向
- 为后续章节摘要与正文生成提供参考
而非剧情定稿或策划文档。
当前已有章节目录（若为空则说明是初始生成）：\n
{chapter_list}

## Goals
- 生成若干连续章节的章节目录
- 每一章包含必要字段，但保持描述宽松
- 避免过度设计与写死剧情

## OutputFormat（必须严格遵守）
请严格按以下格式输出，每一章之间空一行：

第n章 - 《章节标题》
本章定位：
核心作用：
悬念密度：
伏笔操作：
认知颠覆：★☆☆☆☆
本章简述：

## Rules
1. 关于章节标题：
   - 标题简洁、直观
   - 可偏事件、对话、人物状态或关系变化
   - 避免空泛或过度文艺化

2. 关于各字段的生成原则：
   - 本章定位：一句话说明本章整体性质（如：日常 / 对话 / 过渡 / 推进）
   - 核心作用：概括本章在整体中的作用，不必写完整逻辑
   - 悬念密度：使用自然语言（低 / 中 / 偏高），不强制制造悬念
   - 伏笔操作：可写“无 / 暗示 / 轻微铺垫 / 信息提示”，允许模糊
   - 认知颠覆：使用星级表示（★☆☆☆☆ 起），大多数章节可偏低
   - 本章简述：2–4 句话，概括本章主要内容即可，不展开细节

3. 节奏与自由度：
   - 允许连续多章为日常或信息推进
   - 不要求每章都有冲突或反转
   - 不提前设计复杂桥段

4. 禁止事项：
   - 不使用策划案或教学语言
   - 不拆解因果链
   - 不在目录阶段写“必然发生”的剧情结论

## Workflows
1. 理解小说当前所处阶段
2. 按自然叙事顺序规划章节推进
3. 为每一章填写上述字段，但保持描述克制

## Init
请根据以下信息生成章节目录：
- 小说背景与当前进度：{novel_architecture}
- 需要生成的章节数量：{number_of_chapters}

请直接 output章节目录，不要额外解释。

"""

# =============== 6. 前文摘要更新 ===================
summary_prompt = """\
你是一名严谨的小说剧情档案管理员，任务是基于【旧摘要】和【新章节】生成一份**严格≤1500字**的更新版剧情摘要。

【输入】
1. 【旧摘要】（当前字数：{summary_word_count}）：
{global_summary}

2. 【新章节】：
{chapter_text}

【生成铁律】
⚠️ 严禁幻觉：只整合输入文本中明确出现的信息，禁止：
   - 推测角色心理（除非原文直接描写）
   - 补充原文未提及的因果关系
   - 添加"可能""似乎"等模糊推测
   - 若信息不确定，直接舍弃而非脑补

⚠️ 字数硬上限：最终输出必须 ≤1500 字。超字数时按以下优先级删减：
   ① 已解决的支线副本细节 → ② 环境/外貌描写 → ③ 重复性战斗过程 → ④ 早期剧情的次要人物

【结构化压缩策略】
采用三层记忆结构（总字数分配建议：核心层40% + 近期层40% + 归档层20%）：

▸ 核心层（永久锚点｜必须保留）
  - 未解决的核心伏笔（标注首次出现章节）
  - 主角核心目标/仇恨/身世秘密（仅当有新进展时更新）
  - 关键人物关系变化（仅保留当前状态，删除历史演变过程）

▸ 近期层（高精度｜最近3章）
  - 仅保留：①推动主线的关键事件 ②直接影响下一章的对话要点 ③新出现的重要人物/物品
  - 删除：战斗细节、环境渲染、情绪描写等非必要信息

▸ 归档层（压缩归档｜3章前剧情）
  - 仅用1句话概括一个完整故事弧（例如："主角在青云宗大比中夺冠（第5-12章），获得进入藏经阁资格"）
  - 已解决的伏笔标记为【已闭环】并压缩至10字内（例如："王家追杀【已闭环】"）

【输出格式要求】
- 仅返回纯文本摘要，无标题/解释
- 用符号标记更新类型（便于后续追踪）：
  [+] 新增内容
  [→] 更新已有信息（先写旧状态→新状态）
  [-] 归档/压缩的旧内容（仅当该内容被完全解决时标记）
- 严格控制在 1400-1500 字（预留100字缓冲）

【最后检查清单】
□ 字数 ≤1500？
□ 无任何原文未提及的推测/补充？
□ 已解决的支线是否已压缩或归档？
□ 近期3章是否只保留了影响后续的关键信息？
"""

# =============== 7. 角色状态更新 ===================
create_character_state_prompt = """\
依据当前角色动力学设定：{character_dynamics}

请生成一个角色状态文档，内容格式：
例：
张三：
├──物品:
│  ├──青衫：一件破损的青色长袍，带有暗红色的污渍
│  └──寒铁长剑：一柄断裂的铁剑，剑身上刻有古老的符文
├──能力
│  ├──技能1：强大的精神感知能力：能够察觉到周围人的心中活动
│  └──技能2：无形攻击：能够释放一种无法被视觉捕捉的精神攻击
├──状态
│  ├──身体状态: 身材挺拔，穿着华丽的铠甲，面色冷峻
│  └──心理状态: 目前的心态比较平静，但内心隐藏着对柳溪镇未来掌控的野心和不安
├──主要角色间关系网
│  ├──李四：张三从小就与她有关联，对她的成长一直保持关注
│  └──王二：两人之间有着复杂的过去，最近因一场冲突而让对方感到威胁
├──触发或加深的事件
│  ├──村庄内突然出现不明符号：这个不明符号似乎在暗示柳溪镇即将发生重大事件
│  └──李四被刺穿皮肤：这次事件让两人意识到对方的强大实力，促使他们迅速离开队伍

角色名：
├──物品:
│  ├──某物(道具)：描述
│  └──XX长剑(武器)：描述
│   ...
├──能力
│  ├──技能1：描述
│  └──技能2：描述
│   ...
├──状态
│  ├──身体状态：
│  └──心理状态：描述
│    
├──主要角色间关系网
│  ├──李四：描述
│  └──王二：描述
│   ...
├──触发或加深的事件
│  ├──事件1：描述
│  └──事件2：描述
    ...

新出场角色：
- (此处填写未来任何新增角色或临时出场人物的基本信息)

要求：
仅返回编写好的角色状态文本，不要解释任何内容。
"""

update_character_state_prompt = """\
以下是新完成的章节文本：
{chapter_text}

这是当前的角色状态文档：
{old_state}

请更新主要角色状态。

【核心更新逻辑】：**“日常维护 vs 重大重铸”**

1. **【核心人设（相对稳定区）】—— 仅限重大变故修改**
   - **默认规则**：通常情况下，保持这部分内容不变，以维持人物稳定性。
   - **重铸触发条件（Personality Reforging）**：
     - 如果本章发生了**改变人物命运**的大事（如：遭遇灭门惨案、得知身世真相、修为境界大突破、身份职位变更）。
     - **必须**修改【核心人设】以反映这种质变。

2. **【当前状态（滚动更新区）】—— 动态清洗**
   - **事件栏**：**滚动删除**。只保留**最近 5 件**未完结的、对当下有直接影响的事件。
   - **物品栏**：删除普通生活用品，只留关键道具。
   - **关系网**：只列出**本章在场**或**即将互动**的角色。

3. **新角色处理**
   - 如果有新角色登场，请按上述“动静分离”格式新建条目。
   - 必须写清【核心人设】。

请将所有角色严格划分为以下三个区域进行处理：

1. **【活跃区 (Active Zone)】—— 舞台中心的演员**
   - **收录标准**：本章**在场**，或虽不在场但**直接影响**本章剧情的核心人物（如主角）。
   - **格式**：保持【核心人设】+【当前状态】的全量格式。
   - **事件栏**：只留最近 5 件未完结事件。

2. **【潜伏区 (Dormant Zone)】—— 后台待命的演员**
   - **收录标准**：
     - 重要人物（亲友/大反派），但**本章未出场**且**近期无互动**。
   - **操作（折叠）**：**强制压缩为单行文本**。

3. **【清理区 (Trash Zone)】—— 杀青的龙套**
   - **收录标准**：
     - 已死亡的角色。
     - 完成功能的工具人（如“卖药的伙计”、“带路的路人甲”）。
     - 被击败且**无后续伏笔**的小反派。
   - **操作**：**直接删除**，不要在文档中保留任何痕迹。

【输出格式示例】：
=== 活跃区 ===
叶落：
【核心人设】
- 身份：叶家村遗孤，金灵根拥有者
- 性格：坚毅、重情义、偶尔有些少年心性
- 外貌：清秀少年，眼神明亮
【当前状态】
├──物品: 人形玉佩、下品灵石
├──状态:
│  ├──身体: 背部轻微红肿
│  └──心理: 因被王鹏羞辱而感到愤怒，渴望变强
├──关系: 姜璃（随行导师）、王鹏（新仇敌）
└──近期事件:
   ├── 抵达碧河镇：入住客栈
   └── 冲突爆发：在百草堂被王鹏抢走药材并受辱

=== 潜伏区 ===
- 叶墨：主角爷爷，叶家村村长。在村中留守。(暂离)

（注：清理区的角色直接消失，不要列出）

仅返回更新后的角色状态文本，不要解释任何内容。
"""

# =============== 8. 章节正文写作 ===================

# =============== 8.0 本章出场角色提取（精简实战版）===================
CHAPTER_CAST_PROMPT = """\
你是小说的「人物连续性编辑」，任务是为本章生成一份**极简人物卡**——只保留直接影响本章写作决策的信息，剔除一切背景性/历史性冗余。

【输入】
- 前情提要：{global_summary}
- 上一章结尾：{previous_chapter_excerpt}
- 角色当前状态：{character_state}
- 本章核心摘要：{short_summary}
- 作者指导：{user_guidance}
- 本章要素：人物={characters_involved} / 道具={key_items} / 场景={scene_location}

【角色筛选铁律】（三问过滤，任一否决即剔除）
❌ 本章是否与主角/关键角色有直接对话或肢体互动？
❌ 本章是否做出影响剧情走向的独立行动（非背景板）？
❌ 本章是否触发/暴露其性格底线或关系转折？
→ 仅保留通过全部三问的角色（通常2-5人，宁缺毋滥）

【输出格式】（严格按以下结构，无解释）

1) 【本章核心角色】（按戏份排序，≤5人）
- 角色名：身份关键词（当前状态，≤8字）

2) 【动态关系网】（仅双向/本章将爆发的关系，格式：A ↔ B：关系本质+本章张力点）
- 示例：林凡 ↔ 苏婉：表面师徒→本章将因秘籍归属爆发信任危机
- 禁止列出：单向仰慕、已稳定无变化的关系、仅背景提及的关系

3) 【本章行动动机】（每人仅2条，必须可直接转化为对话/动作）
- 角色名：
  • 核心驱动力：1句话（为何做这件事，关联长期目标）
  • 本章具体目标：1句话（本章必须达成的最小行动单元）

【严禁事项】
⚠️ 禁止列入：回忆中人物、仅被提及名字、无台词龙套、关系网"装饰性"角色
⚠️ 禁止描述：外貌细节、历史背景、已解决的旧恩怨（除非本章重启）
⚠️ 禁止堆砌：性格标签（如"冷静""狡猾"）必须转化为"可写行为"（如"遇险先观察3秒再行动"）
"""

# 8.1 第一章草稿提示
first_chapter_draft_prompt = """\
即将创作：第 {novel_number} 章《{chapter_title}》
本章定位：{chapter_role}
核心作用：{chapter_purpose}
悬念密度：{suspense_level}
伏笔操作：{foreshadowing}
认知颠覆：{plot_twist_level}
本章简述：{chapter_summary}

可用元素：
- 核心人物(可能未指定)：{characters_involved}
- 关键道具(可能未指定)：{key_items}
- 空间坐标(可能未指定)：{scene_location}
- 时间压力(可能未指定)：{time_constraint}

参考文档：
- 小说设定：
{novel_setting}

完成第 {novel_number} 章的正文，字数要求{word_number}字。
【网文开篇特别要求】：
1. **黄金三章法则**：必须在开篇前500字内确立主角的“核心困境”或“金手指/异常点”，迅速抓住读者眼球。
2. **拒绝慢热**：不要进行大段的景物描写或背景设定介绍（Info-dump），所有设定必须通过冲突和事件引出。

请至少包含以下动态场景：
1. **冲突前置**：
   - 开篇即切入危机、争吵、突发事件或巨大的情绪张力点。
   - 展现主角在压力下的本能反应，而非心理独白。

2. **信息极简对话**：
   - 对话要短促有力，隐藏信息差。
   - 避免“你好我好大家好”的寒暄，对话必须推动剧情或揭示矛盾。

3. **视觉化动作**：
   - 用动词带动节奏（如“砸”、“撞”、“扑”），减少形容词堆砌。

格式要求：
- 仅返回章节正文文本；
- **强制短段落**：每段不超过3-4行，对话独占一行；
- 不使用分章节小标题；
- 不要使用```

额外指导(可能未指定)：{user_guidance}
"""

# 8.2 后续章节草稿提示
next_chapter_draft_prompt = """\
**指令：** 你将作为一位专业的网络小说作家进行创作。请严格遵循以下所有信息、风格要求及核心法则，完成指定章节的正文。

---

### **一、核心指令**
1.  **输出要求**：仅生成纯粹的章节正文，不使用任何Markdown格式、章节标题、编号或解释性文字。
2.  **风格遵循**：必须完全遵循下文“三、写作核心法则”进行创作。
3.  **信息整合**：必须有机融合下文“二、创作上下文与蓝图”中的所有信息，确保情节、人物和设定的连贯性。

---

### **二、创作上下文与蓝图**
**1. 创作背景（用于理解故事脉络）：**
- 前情提要：{global_summary}

- **【衔接重点】前一章结尾**（开篇必须无缝承接此处的场景、人物状态、对话氛围，严禁跳跃或忽略）：{previous_chapter_excerpt}

- **作者隐式设定与补充指导**（作者脑中已有但可能未写入文件的设定，写作时必须遵守）：{user_guidance}

- **本章人物卡（写作前必须遵守，禁止擅自改写人物关系与动机）：**
{chapter_cast}

- 本章核心摘要：{short_summary}

**2. 强制性设定约束（必须严格遵守，不得违背）：**
{verification_constraints}

**3. 关键实体锁定（人物名、地名、技能名、称号等，严禁编造，仅使用以下或前文已明确出现的名称）：**
{entity_lock_list}

**4. 本章创作蓝图（你的具体任务清单）：**
- **章节**：第{novel_number}章《{chapter_title}》
- **定位与作用**：本章的定位是「{chapter_role}」，核心作用是「{chapter_purpose}」。需控制「悬念密度」为{suspense_level}、「转折程度」为{plot_twist_level}，并融入伏笔：{foreshadowing}。
- **内容要素**：核心人物为{characters_involved}，关键道具有{key_items}，主要场景在{scene_location}，时间压力为{time_constraint}。
- **节奏与结构**：开篇200字内必须无缝承接上一章结尾（场景/情绪/动作延续），然后出现吸引读者的"钩子"；整体约{word_number}字，在1500字左右需安排一个小高潮；结尾需为下一章制造悬念。
- **下章引导**：下一章《{next_chapter_title}》的定位是「{next_chapter_role}」，这将影响你本章结尾悬念的设计方向。
- **知识参考**：涉及世界设定、人物关系、地名技能、派系设定时，请严格参考（不得自行推断或改写）：{filtered_context}


---

### **三、写作核心法则（你的创作铁律）**
**A. 画面与节奏**
- **五感沉浸**：用视觉（色彩、光影）、听觉（环境音、人声）、触觉（温度、冲击）等细节让读者身临其境。
- **镜头语言**：像摄像机一样运镜，组合特写、远景、跟随等视角，构建空间感。动静结合，用环境烘托氛围。
- **节奏把控**：紧张场景用**短句**（3-10字），舒缓场景可稍长。**严禁流水账**，场景之间直接跳跃。

**B. 叙述与视角**
- **严格POV**：仅描写主角所见、所听、所感，**严禁泄露主角不知道的信息**。
- **旁白极简**：**严禁**“他想”、“他感到”这类解释性心理描写。只通过**动作、对话、环境反应**来暗示心理。可保留“瞳孔一缩”等生理反应或“该死！”等瞬间心声。
- **情绪画面化**：不直接说“他悲伤”，而是通过握紧的拳头、窗外突降的暴雨等细节表现。

**C. 文风与修辞**
- **比喻慎用**：**严禁频繁**使用比喻、拟人等修饰性修辞。行文以**直接、精准的白描**为首要原则。
- **使用前提**：仅在**关键氛围渲染**或**抽象概念具象化**时谨慎考虑使用，且同一段落内不得超过一个。
- **核心原则**：**如无必要，勿增比喻**。优先采用动词、名词和感官细节进行直接有力的刻画。

**D. 对话与动作**
- **对话声纹**：
  - *高位者/智者*：对话从容，用长句、反问，伴随慢动作（如缓缓放下茶杯）。
  - *低位者/急切者*：对话急促，用短句、感叹号，直奔主题。
- **对话节奏**：**严禁**在对话前后加解释（如“因为他认为…”）。长对话必须被微动作（如转身、停顿）自然打断。
- **动作可视化**：每个动作都要写明力度、速度和方向，并体现其对环境的影响（如“他一拳砸在桌上，震翻了茶杯”）。

**E. 文本格式**
- **标点规范**：
  - 引号“”**仅用于人物对话**，对话框内**严禁**使用省略号……、破折号——、单引号‘’。
  - **严禁**为强调某个词而为其加引号。
  - 旁白带强烈情绪、决断或冲击力时，**必须用感叹号！** 替代句号。
- **段落规范**：
  - 单段**不超过2行**。
  - **场景切换**或**视角转换**时，必须换行。
  - **动作描写**与**对话**需分开段落。

---

### **四、你的创作流程**
1.  **构思**：基于“创作蓝图”，明确本章开头、小高潮（约1500字处）和结尾悬念的具体场景。
2.  **执行**：在撰写每一段落时，同步应用 **“写作核心法则”**：
    - 先构建场景（空间、氛围）。
    - 再描写人物动作与反应（可视化、POV）。
    - 然后编织对话（符合声纹、自然节奏）。
    - 最后精炼旁白与段落（极简、换行、标点正确）。
3.  **自检与输出**：完成初稿后，**必须按以下清单检查一遍**，确保无误后再做最终输出：
    - [ ] 是否严格遵守了所有“强制性设定约束”和"关键实体锁定"？
    - [ ] **人物名、地名、技能名、称号是否与前文/知识库一致？是否出现编造的新名字？**
    - [ ] 开篇是否无缝承接了上一章结尾？是否有跳跃或忽略前文状态？
    - [ ] 开篇是否有钩子？结尾是否有悬念？
    - [ ] 人物行动动机是否与角色关系、前文逻辑一致？
    - [ ] 是否全程保持主角POV，无信息泄露？
    - [ ] 是否删除了所有“他想”类解释性心理，改用动作、环境展现？
    - [ ] 对话是否符合人物身份（高位/低位）？长对话是否被动作打断？
    - [ ] 段落是否简短（≤2行）？场景/动作/对话切换是否已换行？
    - [ ] 标点使用是否正确？（对话用“”，感叹号情境正确，无滥用引号强调）
    - [ ] 是否融合了多种感官描写，并避免了流水账式过渡？

**最终，当你确认内容已符合所有要求后，请输出纯净的章节正文。**
"""

Character_Import_Prompt = """\
根据以下文本内容，分析出所有角色及其属性信息，严格按照以下格式要求：

<<角色状态格式要求>>
1. 必须包含以下五个分类（按顺序）：
   ● 物品 ● 能力 ● 状态 ● 主要角色间关系网 ● 触发或加深的事件
2. 每个属性条目必须用【名称: 描述】格式
   例：├──青衫: 一件破损的青色长袍，带有暗红色污渍
3. 状态必须包含：
   ● 身体状态: [当前身体状况]
   ● 心理状态: [当前心理状况]
4. 关系网格式：
   ● [角色名称]: [关系类型，如"竞争对手"/"盟友"]
5. 触发事件格式：
   ● [事件名称]: [简要描述及影响]

<<示例>>
李员外:
├──物品:
│  ├──青衫: 一件破损的青色长袍，带有暗红色污渍
│  └──寒铁长剑: 剑身有裂痕，刻有「青云」符文
├──能力:
│  ├──精神感知: 能感知半径30米内的生命体
│  └──剑气压制: 通过目光释放精神威压
├──状态:
│  ├──身体状态: 右臂有未愈合的刀伤
│  └──心理状态: 对苏明远的实力感到忌惮
├──主要角色间关系网:
│  ├──苏明远: 竞争对手，十年前的同僚
│  └──林婉儿: 暗中培养的继承人
├──触发或加深的事件:
│  ├──兵器库遇袭: 丢失三把传家宝剑，影响战力
│  └──匿名威胁信: 信纸带有檀香味，暗示内部泄密
│

请严格按上述格式分析以下内容：
<<待分析小说文本开始>>
{content}
<<待分析小说文本结束>>
"""

# ============== 9. 逻辑一致性检查 ===================
LOGIC_CHECK_PROMPT = """
请作为专业文学逻辑分析师，使用以下系统化框架检查新章节与已有摘要的逻辑一致性：

### 【第一阶段：基础事实核对】
1. **关键实体状态检查**：
   - 人物生理/心理状态是否一致（伤势、情绪、能力水平）
   - 物品属性/归属是否一致（谁拥有、在哪、状态如何）
   - 环境/地点特征是否一致（已被摧毁、完好、特定布局）

2. **时间线硬性冲突检查**：
   - 事件发生的绝对时间点是否冲突（如“三天前”vs“昨天”）
   - 持续时间是否合理（任务完成时间、恢复时间）
   - 时序关系是否颠倒（A在B之前发生）

### 【第二阶段：因果逻辑深度分析】
3. **明确因果关系验证**：
   - 当章节中出现“因为A所以B”表述时：
     a) 检查摘要中是否已确立此因果关系
     b) 若无，这是否为新信息？若是，是否与已有事实冲突？
     c) 若是角色主观认为，是否标记为“角色观点”而非事实？

4. **角色动机一致性检查**：
   - 角色的行为动机是否与摘要中揭示的性格、目标一致？
   - 角色新产生的动机是否有合理触发事件？
   - 特别注意“为某人做某事”类表述，需验证：
     * 受益方是否确实需要此行动？
     * 行动者是否有能力/意图执行？
     * 摘要中是否暗示过此动机？

5. **未解之谜处理规则**：
   - 摘要中标记为“未解之谜”的内容，章节中是否给出了明确答案？
   - 若给出答案，是否与已有线索矛盾？
   - 若添加新线索，是否破坏了谜题的公平性？

### 【第三阶段：主观认知与客观现实对照】
6. **角色内心活动真实性检查**：
   - 角色的回忆/反思内容是否与已知事实匹配？
   - 角色对他人行为的解读是否有事实依据？
   - 角色自我归因（如“都是我的错”）是否基于已发生事件？

7. **对话信息准确性检查**：
   - 角色陈述的“事实”是否与其所知信息一致？
   - 角色是否有途径获得所述信息？
   - 对话中的谣言/误解是否与叙述者提供的事实区分明确？

### 【第四阶段：设定与规则连续性】
8. **能力/力量体系一致性**：
   - 角色能力使用是否遵循已建立的规则？
   - 新展示的能力是否在之前有伏笔或解释？
   - 能力消耗/代价是否与之前描述一致？

9. **社会关系动态检查**：
   - 角色间的关系状态是否与摘要一致（敌对、友好、未知）？
   - 新出现的互动是否基于已有关系合理发展？
   - 群体态度（如全镇对主角的看法）变化是否有触发事件？

### 【第五阶段：叙述逻辑检查】
10. **视角一致性验证**：
    - 叙述视角（第一人称、第三人称有限/全知）是否保持一致？
    - 角色是否获得了本不应知道的信息？
    - 叙述者的评论/描述是否与客观事实层一致？

11. **信息呈现顺序检查**：
    - 章节中信息的揭示顺序是否破坏了摘要中设定的悬念？
    - 角色是否过早知道了本应在后续揭示的信息？

### 【第六阶段：微妙矛盾捕捉】
12. **隐性假设检查**：
    - 章节是否默认了某些摘要中未确认的假设？
    - 如“大家都知道X”但摘要中X并非公开信息

13. **情感反应合理性**：
    - 角色对事件的情感反应是否与先前性格/经历匹配？
    - 情感强度是否与事件严重程度成比例？

14. **角色知识一致性（重要）**：
   - 检查章节中角色在对话或旁白中提到的其他人物名字或关键信息，验证该角色是否有合理渠道知道这些信息（来自共同场景、直接介绍、回忆、信件等）。
   - 标注所有出现“角色不应知道却说出姓名/秘密”的句子，并给出修正建议（如改为“提到职业/描述而非姓名”或加入提示性回忆/信息来源）。

15. **后文目录/大纲冲突检查**：
   - 对照提供的后续章节概要（下一章/后续章节目录），指出当前章节与后文大纲在事件、人物状态、地点迁移、时间线等方面的冲突。
   - 若发现冲突，标明冲突类型并建议修改（优先级/推荐度）。

### 【输出格式要求】

**【发现的确切逻辑错误】**
（按严重程度排序）

错误编号：[序号]
- **错误类型**：[事实冲突/因果矛盾/时间线错误等]
- **问题位置**：[章节中的具体引用]
- **冲突摘要**：[摘要中的相关描述]
- **错误分析**：[详细解释为何矛盾]
- **修正方案**：[提供1-3种修改建议，标明推荐度]

**【潜在风险点】**
（虽未直接冲突但可能引发后续问题）

风险编号：[序号]
- **风险描述**：
- **可能后果**：
- **预防建议**：

【前文摘要】：
{global_summary}

【角色状态档案】：
{character_state}

【下一章概要（用于检查衔接，不作为生成）】：
{next_chapter_outline}

【章节正文】：
{chapter_content}

请列出你发现的所有严重逻辑漏洞和建议。如果没有明显漏洞，请直接回复“无明显逻辑错误”。
格式要求：
1. [错误类型] 具体描述...
2. [错误类型] 具体描述...

附加要求：
- 在输出中增加一个专门小节 `【知识不一致 & POV 异常】`，列出所有角色知识冲突和 POV 泄露问题（每项包含：问题句子 / 涉及角色 / 建议修正）。
- 在输出中增加一个专门小节 `【后文目录冲突】`，列出与下一章概要的所有不一致之处并给出修改建议。
"""

# 9.2 章节正文重写提示
REWRITE_WITH_FEEDBACK_PROMPT = """请根据用户的修改意见，对章节正文进行修正和重写。
保持原有的文风和核心剧情走向，仅针对指出的问题进行修改。其他未提及部分不要修改！

【原始正文】：
{original_content}

【修改意见/逻辑漏洞】：
{feedback}

写作核心法则
**A. 画面与节奏**
- **五感沉浸**：用视觉（色彩、光影）、听觉（环境音、人声）、触觉（温度、冲击）等细节让读者身临其境。
- **镜头语言**：像摄像机一样运镜，组合特写、远景、跟随等视角，构建空间感。动静结合，用环境烘托氛围。
- **节奏把控**：紧张场景用**短句**（3-10字），舒缓场景可稍长。**严禁流水账**，场景之间直接跳跃。

**B. 叙述与视角**
- **严格POV**：仅描写主角所见、所听、所感，**严禁泄露主角不知道的信息**。
- **旁白极简**：**严禁**“他想”、“他感到”这类解释性心理描写。只通过**动作、对话、环境反应**来暗示心理。可保留“瞳孔一缩”等生理反应或“该死！”等瞬间心声。
- **情绪画面化**：不直接说“他悲伤”，而是通过握紧的拳头、窗外突降的暴雨等细节表现。

**C. 文风与修辞**
- **比喻慎用**：**严禁频繁**使用比喻、拟人等修饰性修辞。行文以**直接、精准的白描**为首要原则。
- **使用前提**：仅在**关键氛围渲染**或**抽象概念具象化**时谨慎考虑使用，且同一段落内不得超过一个。
- **核心原则**：**如无必要，勿增比喻**。优先采用动词、名词和感官细节进行直接有力的刻画。

**D. 对话与动作**
- **对话声纹**：
  - *高位者/智者*：对话从容，用长句、反问，伴随慢动作（如缓缓放下茶杯）。
  - *低位者/急切者*：对话急促，用短句、感叹号，直奔主题。
- **对话节奏**：**严禁**在对话前后加解释（如“因为他认为…”）。长对话必须被微动作（如转身、停顿）自然打断。
- **动作可视化**：每个动作都要写明力度、速度和方向，并体现其对环境的影响（如“他一拳砸在桌上，震翻了茶杯”）。

**E. 文本格式**
- **标点规范**：
  - 引号“”**仅用于人物对话**，对话框内**严禁**使用省略号……、破折号——、单引号‘’。
  - **严禁**为强调某个词而为其加引号。
  - 旁白带强烈情绪、决断或冲击力时，**必须用感叹号！** 替代句号。
- **段落规范**：
  - 单段**不超过2行**。
  - **场景切换**或**视角转换**时，必须换行。
  - **动作描写**与**对话**需分开段落。

请按照以上文风要求进行重写，输出修改后的完整章节正文（不要包含任何解释性语言，直接输出正文）：
"""

# =============== 10. 章节目录微调 ===================
REFINE_DIRECTORY_PROMPT = """你是一名专业的小说主编和架构师。
用户希望微调 **{chapter_range}** 的大纲。为了确保剧情连贯，请结合以下全局背景信息进行修改。

【全局背景资料】：
1. **小说核心架构**：
{novel_architecture}

2. **当前剧情演进**（全局摘要）：
{global_summary}

【待修改章节 - 原始内容】：
{current_outline}

【用户的修改指令】：
{user_instruction}

【任务要求】：
1. **逻辑自洽**：修改后的剧情必须符合设定，且章节之间（如第N章到第N+1章）的衔接必须流畅。
2. **格式保持**：**绝对保持**原有的字段结构（章节编号/标题/定位/作用/简述等）。**不要合并章节**，保持原有的章节数量和编号不变。
3. **精准执行**：根据指令进行增删改，未提及的部分保持原样。
4. **期待感设计（每章必须包含至少一种）：**
   - **当下期待**：本章内即将爆发的矛盾（如“下一场战斗”、“即将揭露的真相”）
   - **中期甜头**：为未来3-5章设置的“大奖预告”（如“若水城的治疗机会”）
   - **宏观悬念**：与世界观核心秘密相关的暗示（如“天道的目的”）
5. **节奏控制四维度（每章评估）：**
   - 信息密度：本章包含多少新信息？（低/中/高）
   - 冲突循环：小冲突→解决→新冲突的循环次数
   - 情绪波段：压抑→释放的转折点位置
   - 断章艺术：章节结尾是否制造强烈追读欲？

请输出修改后的**完整章节大纲段落**（直接输出内容，不要加```等标记）：
"""

# =============== 11. 角色变化检测与档案更新 ===================
DETECT_CHANGES_PROMPT = """
请分析以下小说章节文本，找出所有在剧情中**状态、能力、持有物品或人际关系发生实质性变化**的角色名字，以及新登场的重要角色。
请只返回名字列表，格式为JSON字符串数组，例如：["叶落", "穆先生", "夜灵"]。
如果没有任何角色发生重要变化，返回空数组 []。

【章节文本】：
{chapter_text}
"""

# 11.2 角色档案更新提示
UPDATE_PROFILE_PROMPT = """
你是一个严谨的小说角色档案管理员。请根据【当前章节】的剧情发展，更新该角色的【角色档案】。

【当前章节】：
{chapter_text}

【角色档案（旧）】：
{old_profile}

【任务要求】：
1. **增量更新**：仅根据本章发生的事件更新对应条目（如：获得新物品、受了伤、习得新技能、关系变化）。
2. **保持格式**：严格保持原有的树状图结构（物品/能力/状态/关系网/事件），不要更改标题。
3. **新角色处理**：如果旧档案只有模板，请根据章节内容填入初始信息。
4. **输出纯文本**：直接输出更新后的档案内容，不要包含 ```json 或其他解释性文字。

【标准格式参考】：
{char_name}：
├──物品：
│  ├──[物品名]
├──能力：
│  ├──[能力名]
├──状态：
│  ├──[当前状态]
├──主要角色间关系网：
│  ├──[关系描述]
├──触发或加深的事件：
│  └──[事件名]

请输出更新后的完整档案：
"""

# 11.3 伏笔分析提示
FORESHADOWING_ANALYSIS_PROMPT = """
你是一名拥有"上帝视角"的小说分析师。请仔细阅读【当前章节】，提取其中埋下的所有伏笔，并进行分类。

【已有伏笔记录】：
{existing_foreshadowing_records}

【当前章节文本】：
{chapter_text}

【分析要求】：
1. **区分长短线**：
   - **短线伏笔**：预计在未来3-10章内就会解决或爆发的冲突/悬念（如：反派的临时报复、即将到来的考核）。
   - **长线伏笔**：涉及世界观真相、主角身世、大后期BOSS、核心金手指进化等贯穿全文的线索。
2. **动态管理**：
   - **识别已解决的短线伏笔**：如果当前章节解决了之前的短线伏笔，请在【短线伏笔】部分列出已解决的内容，并标记为"已解决："或"已结束："开头
   - **更新长线伏笔状态**：如果当前章节推进了长线伏笔，请在【长线伏笔】部分列出更新内容，并标记为"进展："或"更新状态："开头
   - **新增伏笔**：仅记录当前章节真正引入的新伏笔
3. **整合与发展**：
   - **避免重复**：检查当前章节是否延续了已有伏笔的发展，而不是创建全新伏笔
   - **发展已有伏笔**：如果当前章节推进了已有伏笔，应标注其进展状态
   - **精简表述**：尽量简洁明了地描述伏笔，避免冗长句子
4. **格式规范**：
   - 必须严格按照下方格式输出。
   - 如果某类伏笔本章没有，请写"无"。

【输出格式】：
第{novel_number}章伏笔记录：
【短线伏笔】：
1. [新伏笔：对象/事件 ......]
2. 已解决：[之前记录的伏笔名称或内容]
3. 已结束：[之前记录的伏笔名称或内容]
【长线伏笔】：
1. [新伏笔：对象/事件 ......]
2. 进展：[之前记录的伏笔名称或内容 - 描述当前进展]
3. 更新状态：[之前记录的伏笔名称或内容 - 描述当前状态]
-------------------
"""

# =============== 12. 主动验证与规则制定 ===================
ACTIVE_VERIFICATION_PLANNER_PROMPT = """\
你是一名小说的**连续性编辑（Continuity Editor）**。
在作者开始写下一章之前，你的任务是识别出可能会导致逻辑漏洞或设定冲突的“高风险叙事元素”，并进行查证。

【本章大纲】
章节标题: {chapter_title}
章节定位: {chapter_role}
剧情摘要: {short_summary}
涉及人物: {characters_involved}
关键道具: {key_items}
场景地点: {scene_location}

【任务】
分析上述大纲，生成 3-5 个具体的**验证性问题（Verification Questions）**。
关注点：
1. **设定一致性**：关于地点或物品的具体细节（例如：“王家家徽的具体颜色是什么？”，“噬魂阵的启动条件是什么？”）。
2. **人物状态**：物理状态或近期经历（例如：“角色A受伤的是左腿还是右腿？”，“角色B对角色C说的最后一句话是什么？”）。
3. **逻辑规则**：与本章冲突相关的世界观规则。

【输出格式】
仅返回一个 Python 字符串列表。不要解释。
例如：
[
    "血灵丹的副作用具体是什么？",
    "藏经阁内部的布局是怎样的？",
    "叶落和穆雪当前的关系进展到了哪一步？"
]
"""

# 12.2 规则制定提示
ACTIVE_VERIFICATION_RULE_MAKER_PROMPT = """\
你是一名**设定守门人（Lore Keeper）**。
我将提供一个验证性问题和从知识库中检索到的原始文本。
你的任务是制定一条**严格的写作约束（Strict Writing Constraint）**，供作者在写作时遵守。

【验证问题】: {question}

【知识库检索结果】:
{retrieved_context}

【指令】
1. 如果检索结果包含答案，请将其总结为一条简明的**“必须做”或“禁止做”的规则**。
2. 如果检索结果为空或不相关，请输出：“无特定约束。”
3. 必须具体。不要说“描述好房间”，要说“约束：房间必须有八根柱子且有硫磺味”。

【输出格式】
约束：[你的规则]
"""

# 13. 续写目录
continue_chapter_blueprint_prompt = """
你是专业网络小说目录规划专家，精通**三线叙事法、期待感设计、情绪节奏控制**。你的任务不仅是延续目录，更是**构建读者追读欲望的叙事框架**。

## Background
当前任务为【章节目录延续规划阶段】。
已有章节目录（用于分析当前状态）：\n{existing_chapters}

## 核心分析框架（你必须先分析再生成）

### 第一步：分析已有目录状态（基于现有文本分析）
1. **主线分析**：主角当前核心目标是什么？最近进展如何？
2. **支线分析**：人物关系处于什么状态？有哪些情感线索？
3. **暗线分析**：有哪些未解之谜？伏笔分布如何？
4. **节奏分析**：最近3章的情绪曲线（压抑/释放）？距离上次高潮多少章？

### 第二步：三线叙事模板（必须每章应用）
- **主线（骨架）**：主角行动与目标推进
- **支线（血肉）**：人物关系、情感发展、次要冲突  
- **暗线（悬念）**：世界观秘密、幕后阴谋、伏笔推进

### 第三步：期待感设计（每章至少包含两类）
- **当下钩子**：本章内即将爆发的矛盾
- **中期甜头**：3-5章后会兑现的“奖励预告”
- **宏观悬念**：与世界观核心相关的长期问题

## OutputFormat（必须严格遵守）
请严格按以下增强格式输出，每一章之间空一行：

第n章 - 《章节标题》
本章定位：【类型】一句话（需体现三线侧重）
核心作用：【读者体验】说明本章带给读者的核心情绪价值
悬念密度：低 / 中 / 高（根据本章结尾悬念强度）
伏笔操作：回收/推进/新埋（具体说明）
认知颠覆：★☆☆☆☆（与前面章节保持节奏）
本章简述：2-4句话，必须包含：
  1. 主角行动+直接结果
  2. 情感/关系变化
  3. 新问题/危机出现（断章悬念）
  4. 【可选】世界观信息揭示
**三线标记**：主-... / 支-... / 暗-...（每线5字内）
**期待锚点**：钩-... / 甜-... / 悬-...（每点5字内）

## Rules（基于现有参数深度优化）

### 1. 标题设计（增强连贯性）
- **情绪暗示**：标题应暗示本章核心情绪（如《绝望突围》《温情时刻》）
- **悬念内嵌**：可包含疑问（《谁在暗处？》《消失的证物》）
- **系列感**：连续章节标题可形成情绪递进（如《黑暗降临》《微光初现》《黎明之前》）

### 2. 各字段生成细则（必须应用分析框架）

#### 本章定位：
- **必须标注三线侧重类型**，格式：【主线推进/支线情感/暗线伏笔】一句话
- 示例：【主线战斗/支线师徒/暗线真相暗示】逃离包围的血战与师徒深情时刻

#### 核心作用：
- **必须明确读者情绪价值**，格式：【情绪体验】具体描述
- 示例：【复仇爽感】主角反杀仇敌的畅快感；【悬疑紧张】发现惊人秘密的窒息感

#### 悬念密度：
- **需与本章结尾的断章悬念强度匹配**
- 低：日常过渡，无紧迫危机
- 中：有明确未解决问题，但无生命危险
- 高：生死危机、倒计时、重大秘密即将揭露

#### 伏笔操作：
- **必须具体到操作类型和大致内容**
- 格式：回收[前文章节]的[什么伏笔]；新埋[什么新线索]（预计[多少章]后回收）
- 示例：回收第38章“实验品”伏笔；新埋“玉佩与天道联系”线索（预计30章后回收）

#### 认知颠覆：
- **需形成波浪节奏**，避免连续高星或低星
- ★☆☆☆☆：信息补充，无认知改变
- ★★☆☆☆：小修正（如角色动机微调）
- ★★★☆☆：中等反转（如盟友变敌人）
- ★★★★☆：大反转（关键设定被推翻）
- ★★★★★：世界观级颠覆（全书转折点）

#### 本章简述（2-4句话）：
- **严格四段式结构**（如无第四段可省略）：
  1. **行动与结果**：主角做了什么，达到什么直接效果
  2. **情感变化**：关键人物的情感或关系进展
  3. **新危机/问题**：结尾留下的悬念（必须具体）
  4. **世界观信息**：揭示的规则/设定/历史（如有）

#### 新增字段：三线标记（必须每章都有）
- 格式：主-[5字内主线进展] / 支-[5字内支线发展] / 暗-[5字内暗线推进]
- 示例：主-突破包围 / 支-师徒情深 / 暗-玉佩秘密

#### 新增字段：期待锚点（必须每章都有）
- 格式：钩-[当下钩子] / 甜-[中期甜头] / 悬-[宏观悬念]
- 至少包含两个，用“/”分隔
- 示例：钩-兽人追兵将至 / 甜-三天后抵达安全区 / 悬-天道真相

### 3. 连贯性要求（基于已有目录分析）

#### 情绪节奏控制：
- 分析已有目录的**最近3章情绪状态**
- 如果连续压抑≥2章，本章必须设计释放点
- 如果最近有高潮，本章应设计缓冲或新危机铺垫

#### 信息释放分层：
- 关键秘密分3次释放：暗示→部分真相→完整真相
- 重大设定需有“展示→应用→深化”三阶段

#### 角色聚焦轮换：
- 连续主角视角不超过5章
- 每3-5章可有一章配角视角（但必须推进主线）

### 4. 禁止事项（强化）
- ❌ 标题空泛（如《修炼》《谈话》）
- ❌ 核心作用写“推进剧情”等空洞描述
- ❌ 悬念密度与内容不匹配
- ❌ 伏笔操作写“无”或模糊描述
- ❌ 本章简述不包含断章悬念
- ❌ 三线标记或期待锚点缺失

## Workflows（具体执行步骤）

### 第一步：深度分析已有目录
仔细阅读以下已有目录，提取关键状态：
{existing_chapters}

分析要点：
1. **最近3章**的情绪状态、核心事件、结尾悬念
2. **主线进展**：主角当前目标、最近障碍、下一步方向
3. **支线关系**：哪些人物关系在发展中？处于什么阶段？
4. **暗线伏笔**：有哪些未解之谜？最近一次伏笔回收是什么时候？
5. **节奏位置**：处于情绪周期的哪个阶段？（压抑期/上升期/高潮期/下降期）

### 第二步：规划本批章节的叙事任务
基于分析结果，确定本批{number_of_chapters}章要完成：
- **情绪任务**：带读者经历什么情绪旅程？
- **剧情任务**：必须推进哪些关键节点？
- **悬念任务**：要设置/回收哪些悬念？

### 第三步：逐章设计（应用模板）
为第{start_chapter}章到第{end_chapter}章，每章按以下顺序设计：
1. **先定情绪**：本章要给读者什么核心情绪体验？
2. **再定三线**：主线推多少？支线怎么发展？暗线怎么埋？
3. **设计钩子**：本章结尾的断章悬念是什么？（必须具体）
4. **检查连贯**：与前后章节是否形成合理推进？

### 第四步：整体节奏调整
完成初稿后检查：
- 情绪是否形成波浪（压抑→释放→新压抑）
- 三线是否均衡分布（避免连续主线轰炸）
- 悬念密度是否有起伏（高低相间）
- 每章是否都有明确的“读者获得感”

## Init
现在，基于以下信息生成第{start_chapter}章到第{end_chapter}章的章节目录：

**小说核心架构**：
{novel_architecture}

**已有章节目录（请仔细分析当前状态）**：
{existing_chapters}

**需要生成的章节**：
- 起始：第{start_chapter}章
- 结束：第{end_chapter}章
- 数量：{number_of_chapters}章

**作者特别指导**：
{user_guidance}

请输出第{start_chapter}章到第{end_chapter}章的完整章节目录，严格遵守增强格式，不要任何解释性文字。
"""
</file>

<file path="ui/config_tab.py">
# ui/config_tab.py
# -*- coding: utf-8 -*-
from tkinter import messagebox
import uuid
import datetime

import customtkinter as ctk

from config_manager import load_config, save_config
from tooltips import tooltips

import os


def create_label_with_help(self, parent, label_text, tooltip_key, row, column,
                           font=None, sticky="e", padx=5, pady=5):
    """
    封装一个带"?"按钮的Label，用于展示提示信息。
    """
    frame = ctk.CTkFrame(parent)
    frame.grid(row=row, column=column, padx=padx, pady=pady, sticky=sticky)
    frame.columnconfigure(0, weight=0)

    label = ctk.CTkLabel(frame, text=label_text, font=font)
    label.pack(side="left")

    btn = ctk.CTkButton(
        frame,
        text="?",
        width=22,
        height=22,
        font=("Microsoft YaHei", 10),
        command=lambda: messagebox.showinfo("参数说明", tooltips.get(tooltip_key, "暂无说明"))
    )
    btn.pack(side="left", padx=3)

    return frame

def build_config_tabview(self):
    """
    创建包含 LLM Model settings 和 Embedding settings 的选项卡。
    """
    self.config_tabview = ctk.CTkTabview(self.config_frame)
    self.config_tabview.grid(row=0, column=0, sticky="we", padx=5, pady=5)

    self.ai_config_tab = self.config_tabview.add("LLM Model settings")
    self.embeddings_config_tab = self.config_tabview.add("Embedding settings")
    self.config_choose = self.config_tabview.add("Config choose")

    # PenBo 增加代理功能支持
    self.proxy_setting_tab = self.config_tabview.add("Proxy setting")


    build_ai_config_tab(self)
    build_embeddings_config_tab(self)
    build_config_choose_tab(self)

    # PenBo 增加代理功能支持
    build_proxy_setting_tab(self)

def build_ai_config_tab(self):
    def refresh_config_dropdown():
        """刷新配置下拉菜单"""
        config_names = list(self.loaded_config.get("llm_configs", {}).keys())
        interface_config_dropdown.configure(values=config_names)
        if config_names and self.interface_config_var.get() not in config_names:
            self.interface_config_var.set(config_names[0])

    def on_config_selected(new_value):
        """当选择不同配置时的回调"""
        if new_value in self.loaded_config.get("llm_configs", {}):
            config = self.loaded_config["llm_configs"][new_value]
            # 更新所有UI变量
            self.api_key_var.set(config.get("api_key", ""))
            self.base_url_var.set(config.get("base_url", ""))
            self.model_name_var.set(config.get("model_name", ""))
            self.temperature_var.set(float(config.get("temperature", 0.7)))
            self.max_tokens_var.set(int(config.get("max_tokens", 8192)))
            self.timeout_var.set(int(config.get("timeout", 600)))
            self.interface_format_var.set(config.get("interface_format", "OpenAI"))
            
            # 更新显示标签
            self.temp_value_label.configure(text=f"{float(config.get('temperature', 0.7)):.2f}")
            self.max_tokens_value_label.configure(text=str(int(config.get('max_tokens', 8192))))
            self.timeout_value_label.configure(text=str(int(config.get('timeout', 600))))

    def add_new_config():
        """添加新配置 - 弹出对话框让用户输入名称"""
        dialog = ctk.CTkInputDialog(
            text="请输入新配置名称:",
            title="新增配置"
        )
        new_name = dialog.get_input()
        
        if not new_name:
            return
            
        new_name = new_name.strip()
        
        if new_name in self.loaded_config.get("llm_configs", {}):
            messagebox.showerror("错误", f"配置名称 '{new_name}' 已存在!")
            return
            
        if "llm_configs" not in self.loaded_config:
            self.loaded_config["llm_configs"] = {}
            
        self.loaded_config["llm_configs"][new_name] = {
            "id": str(uuid.uuid4()),
            "api_key": "",
            "base_url": "",
            "model_name": "",
            "temperature": 0.7,
            "max_tokens": 8192,
            "timeout": 600,
            "interface_format": "OpenAI",
            "created_at": datetime.datetime.now().isoformat()
        }
        
        refresh_config_dropdown()
        self.interface_config_var.set(new_name)
        messagebox.showinfo("提示", f"已成功创建新配置: {new_name}")

    def delete_current_config():
        """删除当前选中的配置并保存到JSON文件"""
        selected_config = self.interface_config_var.get()
        
        if selected_config not in self.loaded_config.get("llm_configs", {}):
            messagebox.showerror("错误", "未找到选中的配置!")
            return
        
        if len(self.loaded_config["llm_configs"]) <= 1:
            messagebox.showerror("错误", "至少需要保留一个配置!")
            return
        
        confirm = messagebox.askyesno(
            "确认删除",
            f"确定要删除配置 '{selected_config}' 吗?\n此操作不可撤销!"
        )
        
        if not confirm:
            return
        
        try:
            del self.loaded_config["llm_configs"][selected_config]
            refresh_config_dropdown()
            save_config(self.loaded_config, self.config_file)
            messagebox.showinfo("提示", f"已删除配置: {selected_config}，并已更新配置文件")
        except Exception as e:
            messagebox.showerror("错误", f"保存配置文件失败: {str(e)}")

    def save_current_config():
        """保存当前配置的修改到JSON文件"""
        config_name = self.interface_config_var.get()
        if config_name not in self.loaded_config.get("llm_configs", {}):
            messagebox.showerror("错误", "配置不存在!")
            return
            
        config = self.loaded_config["llm_configs"][config_name]
        config.update({
            "api_key": self.api_key_var.get(),
            "base_url": self.base_url_var.get(),
            "model_name": self.model_name_var.get(),
            "temperature": float(self.temperature_var.get()),
            "max_tokens": int(self.max_tokens_var.get()),
            "timeout": int(self.timeout_var.get()),
            "interface_format": self.interface_format_var.get(),
            "updated_at": datetime.datetime.now().isoformat()
        })
        
        # 如果修改了配置名称
        new_name = self.interface_config_var.get()
        if new_name != config_name:
            self.loaded_config["llm_configs"][new_name] = self.loaded_config["llm_configs"].pop(config_name)
            refresh_config_dropdown()
        embedding_config = {
        "api_key": self.embedding_api_key_var.get(),
        "base_url": self.embedding_url_var.get(),
        "model_name": self.embedding_model_name_var.get(),
        "retrieval_k": self.safe_get_int(self.embedding_retrieval_k_var, 4),
        "interface_format": self.embedding_interface_format_var.get().strip()

        }
        other_params = {
            "topic": self.topic_text.get("0.0", "end").strip(),
            "genre": self.genre_var.get(),
            "num_chapters": self.safe_get_int(self.num_chapters_var, 10),
            "word_number": self.safe_get_int(self.word_number_var, 3000),
            "filepath": self.filepath_var.get(),
            "chapter_num": self.chapter_num_var.get(),
            "user_guidance": self.user_guide_text.get("0.0", "end").strip(),
            "characters_involved": self.characters_involved_var.get(),
            "key_items": self.key_items_var.get(),
            "scene_location": self.scene_location_var.get(),
            "time_constraint": self.time_constraint_var.get()
        }
        self.loaded_config["embedding_configs"][self.embedding_interface_format_var.get().strip()] = embedding_config
        self.loaded_config["other_params"] = other_params


        # 保存到JSON文件
        try:
            save_config(self.loaded_config, self.config_file)
            messagebox.showinfo("提示", f"配置 {new_name} 已保存并持久化到文件")
        except Exception as e:
            messagebox.showerror("错误", f"保存配置文件失败: {str(e)}")

    def rename_current_config():
        """重命名当前配置"""
        old_name = self.interface_config_var.get()
        if old_name not in self.loaded_config.get("llm_configs", {}):
            messagebox.showerror("错误", "当前配置不存在!")
            return
            
        dialog = ctk.CTkInputDialog(
            text=f"请输入新的配置名称 (原名称: {old_name}):",
            title="重命名配置"
        )
        new_name = dialog.get_input()
        
        if not new_name:
            return
            
        new_name = new_name.strip()
        
        if new_name == old_name:
            return
            
        if new_name in self.loaded_config.get("llm_configs", {}):
            messagebox.showerror("错误", f"配置名称 '{new_name}' 已存在!")
            return
            
        # 更新配置名称
        self.loaded_config["llm_configs"][new_name] = self.loaded_config["llm_configs"].pop(old_name)
        self.interface_config_var.set(new_name)
        refresh_config_dropdown()

        messagebox.showinfo("提示", f"配置已从 '{old_name}' 重命名为 '{new_name}'")

    # 初始化UI布局
    for i in range(10):
        self.ai_config_tab.grid_rowconfigure(i, weight=0)
    self.ai_config_tab.grid_columnconfigure(0, weight=0)
    self.ai_config_tab.grid_columnconfigure(1, weight=1)
    self.ai_config_tab.grid_columnconfigure(2, weight=0)

    # 配置选择控件
    create_label_with_help(self, self.ai_config_tab, "当前配置", "interface_config", 0, 0)
    config_names = list(self.loaded_config.get("llm_configs", {}).keys())
    if not config_names:
        self.loaded_config["llm_configs"] = {
            "默认配置": {
                "id": str(uuid.uuid4()),
                "api_key": "",
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 8192,
                "timeout": 600,
                "interface_format": "OpenAI",
                "created_at": datetime.datetime.now().isoformat()
            }
        }
        config_names = ["默认配置"]
    
    self.interface_config_var = ctk.StringVar(value=config_names[0])

    interface_config_dropdown = ctk.CTkOptionMenu(
        self.ai_config_tab, 
        values=config_names,
        variable=self.interface_config_var,
        command=on_config_selected,
        font=("Microsoft YaHei", 12)
    )
    interface_config_dropdown.grid(row=0, column=1, columnspan=2, padx=5, pady=5, sticky="nsew")

    # 配置管理按钮组
    btn_frame = ctk.CTkFrame(self.ai_config_tab)
    btn_frame.grid(row=1, column=0, columnspan=3, padx=5, pady=5, sticky="ew")
    btn_frame.columnconfigure(0, weight=1)
    btn_frame.columnconfigure(1, weight=1)
    btn_frame.columnconfigure(2, weight=1)
    btn_frame.columnconfigure(3, weight=1)

    add_btn = ctk.CTkButton(
        btn_frame, 
        text="➕ 新增", 
        command=add_new_config,
        font=("Microsoft YaHei", 12),
        fg_color="#2E8B57",
        width=80
    )
    add_btn.grid(row=0, column=0, padx=2, pady=2, sticky="ew")

    rename_btn = ctk.CTkButton(
        btn_frame, 
        text="✏️ 重命名", 
        command=rename_current_config,
        font=("Microsoft YaHei", 12),
        fg_color="#DAA520",
        width=80
    )
    rename_btn.grid(row=0, column=1, padx=2, pady=2, sticky="ew")

    del_btn = ctk.CTkButton(
        btn_frame, 
        text="🗑️ 删除", 
        command=delete_current_config,
        font=("Microsoft YaHei", 12),
        fg_color="#8B0000",
        width=80
    )
    del_btn.grid(row=0, column=2, padx=2, pady=2, sticky="ew")

    save_btn = ctk.CTkButton(
        btn_frame, 
        text="💾 保存", 
        command=save_current_config,
        font=("Microsoft YaHei", 12),
        fg_color="#1E90FF",
        width=80
    )
    save_btn.grid(row=0, column=3, padx=2, pady=2, sticky="ew")

    # 配置参数控件
    row_start = 2
    # 1) API Key
    create_label_with_help(self, self.ai_config_tab, "API Key:", "api_key", row_start, 0)
    self.api_key_var = ctk.StringVar(value="")
    api_key_entry = ctk.CTkEntry(
        self.ai_config_tab, 
        textvariable=self.api_key_var,
        font=("Microsoft YaHei", 12),
        show="*"
    )
    api_key_entry.grid(row=row_start, column=1, columnspan=2, padx=5, pady=5, sticky="nsew")
    
    # 2) Base URL
    create_label_with_help(self, self.ai_config_tab, "Base URL:", "base_url", row_start+1, 0)
    self.base_url_var = ctk.StringVar(value="")
    base_url_entry = ctk.CTkEntry(
        self.ai_config_tab, 
        textvariable=self.base_url_var,
        font=("Microsoft YaHei", 12)
    )
    base_url_entry.grid(row=row_start+1, column=1, columnspan=2, padx=5, pady=5, sticky="nsew")
    
    # 3) 接口格式
    create_label_with_help(self, self.ai_config_tab, "接口格式:", "interface_format", row_start+2, 0)
    self.interface_format_var = ctk.StringVar(value="OpenAI")
    interface_options = ["OpenAI", "Azure OpenAI", "Ollama", "DeepSeek", "Gemini", "ML Studio", "硅基流动"]
    interface_dropdown = ctk.CTkOptionMenu(
        self.ai_config_tab,
        values=interface_options,
        variable=self.interface_format_var,
        font=("Microsoft YaHei", 12)
    )
    interface_dropdown.grid(row=row_start+2, column=1, columnspan=2, padx=5, pady=5, sticky="nsew")
    
    # 4) Model Name
    create_label_with_help(self, self.ai_config_tab, "模型名称:", "model_name", row_start+3, 0)
    self.model_name_var = ctk.StringVar(value="")
    model_name_entry = ctk.CTkEntry(
        self.ai_config_tab, 
        textvariable=self.model_name_var,
        font=("Microsoft YaHei", 12)
    )
    model_name_entry.grid(row=row_start+3, column=1, columnspan=2, padx=5, pady=5, sticky="nsew")
    
    # 5) Temperature
    create_label_with_help(self, self.ai_config_tab, "Temperature:", "temperature", row_start+4, 0)
    self.temperature_var = ctk.DoubleVar(value=0.7)
    def update_temp_label(value):
        self.temp_value_label.configure(text=f"{float(value):.2f}")
    temp_scale = ctk.CTkSlider(
        self.ai_config_tab, 
        from_=0, 
        to=2, 
        number_of_steps=200, 
        command=update_temp_label,
        variable=self.temperature_var
    )
    temp_scale.grid(row=row_start+4, column=1, padx=5, pady=5, sticky="we")
    self.temp_value_label = ctk.CTkLabel(
        self.ai_config_tab, 
        text=f"{self.temperature_var.get():.2f}",
        font=("Microsoft YaHei", 12)
    )
    self.temp_value_label.grid(row=row_start+4, column=2, padx=5, pady=5, sticky="w")
    
    # 6) Max Tokens
    create_label_with_help(self, self.ai_config_tab, "Max Tokens:", "max_tokens", row_start+5, 0)
    self.max_tokens_var = ctk.IntVar(value=8192)
    def update_max_tokens_label(value):
        self.max_tokens_value_label.configure(text=str(int(float(value))))
    max_tokens_slider = ctk.CTkSlider(
        self.ai_config_tab, 
        from_=0, 
        to=102400, 
        number_of_steps=100, 
        command=update_max_tokens_label,
        variable=self.max_tokens_var
    )
    max_tokens_slider.grid(row=row_start+5, column=1, padx=5, pady=5, sticky="we")
    self.max_tokens_value_label = ctk.CTkLabel(
        self.ai_config_tab, 
        text=str(self.max_tokens_var.get()),
        font=("Microsoft YaHei", 12)
    )
    self.max_tokens_value_label.grid(row=row_start+5, column=2, padx=5, pady=5, sticky="w")
    
    # 7) Timeout
    create_label_with_help(self, self.ai_config_tab, "Timeout (sec):", "timeout", row_start+6, 0)
    self.timeout_var = ctk.IntVar(value=600)
    def update_timeout_label(value):
        self.timeout_value_label.configure(text=str(int(float(value))))
    timeout_slider = ctk.CTkSlider(
        self.ai_config_tab, 
        from_=0, 
        to=3600, 
        number_of_steps=3600, 
        command=update_timeout_label,
        variable=self.timeout_var
    )
    timeout_slider.grid(row=row_start+6, column=1, padx=5, pady=5, sticky="we")
    self.timeout_value_label = ctk.CTkLabel(
        self.ai_config_tab, 
        text=str(self.timeout_var.get()),
        font=("Microsoft YaHei", 12)
    )
    self.timeout_value_label.grid(row=row_start+6, column=2, padx=5, pady=5, sticky="w")
    
    # 测试按钮
    test_btn = ctk.CTkButton(
        self.ai_config_tab, 
        text="测试配置", 
        command=self.test_llm_config,
        font=("Microsoft YaHei", 12)
    )
    test_btn.grid(row=row_start+7, column=0, columnspan=3, padx=5, pady=5, sticky="ew")

    # 初始化当前配置
    on_config_selected(config_names[0])


    # 初始化UI布局
    for i in range(10):  # 增加一行给按钮组
        self.ai_config_tab.grid_rowconfigure(i, weight=0)
    self.ai_config_tab.grid_columnconfigure(0, weight=0)
    self.ai_config_tab.grid_columnconfigure(1, weight=1)
    self.ai_config_tab.grid_columnconfigure(2, weight=0)

    # 配置选择控件
    create_label_with_help(self, self.ai_config_tab, "当前配置", "interface_config", 0, 0)
    config_names = list(self.loaded_config.get("llm_configs", {}).keys())
    if not config_names:  # 如果没有配置，创建一个默认配置
        self.loaded_config["llm_configs"] = {
            "默认配置": {
                "id": str(uuid.uuid4()),
                "api_key": "",
                "base_url": "https://api.openai.com/v1",
                "model_name": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 8192,
                "timeout": 600,
                "interface_format": "OpenAI",
                "created_at": datetime.datetime.now().isoformat()
            }
        }
        config_names = ["默认配置"]
    
    interface_config_dropdown = ctk.CTkOptionMenu(
        self.ai_config_tab, 
        values=config_names,
        variable=self.interface_config_var,
        command=on_config_selected,
        font=("Microsoft YaHei", 12)
    )
    interface_config_dropdown.grid(row=0, column=1, columnspan=2, padx=5, pady=5, sticky="nsew")

def build_embeddings_config_tab(self):
    def on_embedding_interface_changed(new_value):
        self.embedding_interface_format_var.set(new_value)
        config_data = load_config(self.config_file)
        if config_data:
            config_data["last_embedding_interface_format"] = new_value
            save_config(config_data, self.config_file)
        if self.loaded_config and "embedding_configs" in self.loaded_config and new_value in self.loaded_config["embedding_configs"]:
            emb_conf = self.loaded_config["embedding_configs"][new_value]
            self.embedding_api_key_var.set(emb_conf.get("api_key", ""))
            self.embedding_url_var.set(emb_conf.get("base_url", self.embedding_url_var.get()))
            self.embedding_model_name_var.set(emb_conf.get("model_name", ""))
            self.embedding_retrieval_k_var.set(str(emb_conf.get("retrieval_k", 4)))
        else:
            if new_value == "Ollama":
                self.embedding_url_var.set("http://localhost:11434/api")
            elif new_value == "ML Studio":
                self.embedding_url_var.set("http://localhost:1234/v1")
            elif new_value == "OpenAI":
                self.embedding_url_var.set("https://api.openai.com/v1")
                self.embedding_model_name_var.set("text-embedding-ada-002")
            elif new_value == "Azure OpenAI":
                self.embedding_url_var.set("https://[az].openai.azure.com/openai/deployments/[model]/embeddings?api-version=2023-05-15")
            elif new_value == "DeepSeek":
                self.embedding_url_var.set("https://api.deepseek.com/v1")
            elif new_value == "Gemini":
                self.embedding_url_var.set("https://generativelanguage.googleapis.com/v1beta/")
                self.embedding_model_name_var.set("models/text-embedding-004")
            elif new_value == "SiliconFlow":
                self.embedding_url_var.set("https://api.siliconflow.cn/v1/embeddings")
                self.embedding_model_name_var.set("BAAI/bge-m3")

    for i in range(5):
        self.embeddings_config_tab.grid_rowconfigure(i, weight=0)
    self.embeddings_config_tab.grid_columnconfigure(0, weight=0)
    self.embeddings_config_tab.grid_columnconfigure(1, weight=1)
    self.embeddings_config_tab.grid_columnconfigure(2, weight=0)

    # 1) Embedding API Key
    create_label_with_help(self, parent=self.embeddings_config_tab, label_text="Embedding API Key:", tooltip_key="embedding_api_key", row=0, column=0, font=("Microsoft YaHei", 12))
    emb_api_key_entry = ctk.CTkEntry(self.embeddings_config_tab, textvariable=self.embedding_api_key_var, font=("Microsoft YaHei", 12), show="*")
    emb_api_key_entry.grid(row=0, column=1, padx=5, pady=5, sticky="nsew")

    # 2) Embedding 接口格式
    create_label_with_help(self, parent=self.embeddings_config_tab, label_text="Embedding 接口格式:", tooltip_key="embedding_intexrface_format", row=1, column=0, font=("Microsoft YaHei", 12))

    emb_interface_options = ["DeepSeek", "OpenAI", "Azure OpenAI", "Gemini", "Ollama", "ML Studio","SiliconFlow"]

    emb_interface_dropdown = ctk.CTkOptionMenu(self.embeddings_config_tab, values=emb_interface_options, variable=self.embedding_interface_format_var, command=on_embedding_interface_changed, font=("Microsoft YaHei", 12))
    emb_interface_dropdown.grid(row=1, column=1, padx=5, pady=5, sticky="nsew")

    # 3) Embedding Base URL
    create_label_with_help(self, parent=self.embeddings_config_tab, label_text="Embedding Base URL:", tooltip_key="embedding_url", row=2, column=0, font=("Microsoft YaHei", 12))
    emb_url_entry = ctk.CTkEntry(self.embeddings_config_tab, textvariable=self.embedding_url_var, font=("Microsoft YaHei", 12))
    emb_url_entry.grid(row=2, column=1, padx=5, pady=5, sticky="nsew")

    # 4) Embedding Model Name
    create_label_with_help(self, parent=self.embeddings_config_tab, label_text="Embedding Model Name:", tooltip_key="embedding_model_name", row=3, column=0, font=("Microsoft YaHei", 12))
    emb_model_name_entry = ctk.CTkEntry(self.embeddings_config_tab, textvariable=self.embedding_model_name_var, font=("Microsoft YaHei", 12))
    emb_model_name_entry.grid(row=3, column=1, padx=5, pady=5, sticky="nsew")

    # 5) Retrieval Top-K
    create_label_with_help(self, parent=self.embeddings_config_tab, label_text="Retrieval Top-K:", tooltip_key="embedding_retrieval_k", row=4, column=0, font=("Microsoft YaHei", 12))
    emb_retrieval_k_entry = ctk.CTkEntry(self.embeddings_config_tab, textvariable=self.embedding_retrieval_k_var, font=("Microsoft YaHei", 12))
    emb_retrieval_k_entry.grid(row=4, column=1, padx=5, pady=5, sticky="nsew")

    # 添加测试按钮
    test_btn = ctk.CTkButton(self.embeddings_config_tab, text="测试配置", command=self.test_embedding_config, font=("Microsoft YaHei", 12))
    test_btn.grid(row=5, column=0, columnspan=2, padx=5, pady=5, sticky="ew")

def build_config_choose_tab(self):
    """
    构建【模型指派】界面的具体内容
    """
    self.config_choose.grid_rowconfigure(0, weight=0)
    self.config_choose.grid_columnconfigure(0, weight=0)
    self.config_choose.grid_columnconfigure(1, weight=1)

    # 获取配置列表
    config_choose_options = list(self.loaded_config.get("llm_configs", {}).keys())
    if not config_choose_options:
        config_choose_options = ["Default"]

    # --- 辅助：确保变量已初始化 ---
    def ensure_var(var_name, default_val=None):
        if not hasattr(self, var_name):
            val = default_val if default_val else config_choose_options[0]
            setattr(self, var_name, ctk.StringVar(value=val))
        # 再次检查值是否有效
        var = getattr(self, var_name)
        if var.get() not in config_choose_options:
            var.set(config_choose_options[0])

    # 初始化所有需要的变量
    ensure_var("architecture_llm_var")
    ensure_var("chapter_outline_llm_var")
    ensure_var("prompt_draft_llm_var")
    ensure_var("refine_logic_llm_var")    # 新增
    ensure_var("final_chapter_llm_var")   # 新增
    ensure_var("consistency_review_llm_var")
    ensure_var("directory_continuation_llm_var")  # 新增：目录续写模型
    ensure_var("directory_refinement_llm_var")    # 新增：目录微调模型

    # 1. 架构模型
    create_label_with_help(self, parent=self.config_choose, label_text="生成架构所用大模型", tooltip_key="architecture_llm_config", row=0, column=0, font=("Microsoft YaHei", 12))
    architecture_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.architecture_llm_var, font=("Microsoft YaHei", 12))
    architecture_dropdown.grid(row=0, column=1, padx=5, pady=5, sticky="nsew")

    # 2. 目录模型
    create_label_with_help(self, parent=self.config_choose, label_text="生成大目录所用大模型", tooltip_key="chapter_outline_llm_config", row=1, column=0, font=("Microsoft YaHei", 12))
    chapter_outline_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.chapter_outline_llm_var, font=("Microsoft YaHei", 12))
    chapter_outline_dropdown.grid(row=1, column=1, padx=5, pady=5, sticky="nsew")

    # 3. 草稿模型
    create_label_with_help(self, parent=self.config_choose, label_text="生成草稿所用大模型", tooltip_key="prompt_draft_llm_config", row=2, column=0, font=("Microsoft YaHei", 12))
    prompt_draft_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.prompt_draft_llm_var, font=("Microsoft YaHei", 12))
    prompt_draft_dropdown.grid(row=2, column=1, padx=5, pady=5, sticky="nsew")

    # 4. 【新增】逻辑/选角模型
    create_label_with_help(self, parent=self.config_choose, label_text="逻辑/选角所用大模型", tooltip_key="refine_logic_llm_config", row=3, column=0, font=("Microsoft YaHei", 12))
    refine_logic_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.refine_logic_llm_var, font=("Microsoft YaHei", 12))
    refine_logic_dropdown.grid(row=3, column=1, padx=5, pady=5, sticky="nsew")

    # 5. 【新增】定稿模型
    create_label_with_help(self, parent=self.config_choose, label_text="定稿章节所用大模型", tooltip_key="final_chapter_llm_config", row=4, column=0, font=("Microsoft YaHei", 12))
    final_chapter_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.final_chapter_llm_var, font=("Microsoft YaHei", 12))
    final_chapter_dropdown.grid(row=4, column=1, padx=5, pady=5, sticky="nsew")

    # 6. 审校模型
    create_label_with_help(self, parent=self.config_choose, label_text="一致性审校所用大模型", tooltip_key="consistency_review_llm_config", row=5, column=0, font=("Microsoft YaHei", 12))
    consistency_review_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.consistency_review_llm_var, font=("Microsoft YaHei", 12))
    consistency_review_dropdown.grid(row=5, column=1, padx=5, pady=5, sticky="nsew")

    # 7. 【新增】目录续写模型
    create_label_with_help(self, parent=self.config_choose, label_text="目录续写所用大模型", tooltip_key="directory_continuation_llm_config", row=6, column=0, font=("Microsoft YaHei", 12))
    directory_continuation_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.directory_continuation_llm_var, font=("Microsoft YaHei", 12))
    directory_continuation_dropdown.grid(row=6, column=1, padx=5, pady=5, sticky="nsew")

    # 8. 【新增】目录微调模型
    create_label_with_help(self, parent=self.config_choose, label_text="目录微调所用大模型", tooltip_key="directory_refinement_llm_config", row=7, column=0, font=("Microsoft YaHei", 12))
    directory_refinement_dropdown = ctk.CTkOptionMenu(self.config_choose, values=config_choose_options, variable=self.directory_refinement_llm_var, font=("Microsoft YaHei", 12))
    directory_refinement_dropdown.grid(row=7, column=1, padx=5, pady=5, sticky="nsew")


    # --- 功能函数 ---
    def save_config_choose():
        full_config = load_config(self.config_file)
        if "choose_configs" not in full_config:
            full_config["choose_configs"] = {}
        
        config_data = full_config["choose_configs"]
        config_data["architecture_llm"] = self.architecture_llm_var.get()
        config_data["chapter_outline_llm"] = self.chapter_outline_llm_var.get()
        config_data["prompt_draft_llm"] = self.prompt_draft_llm_var.get()
        
        # 保存新增项
        config_data["refine_logic_llm"] = self.refine_logic_llm_var.get()
        config_data["final_chapter_llm"] = self.final_chapter_llm_var.get()
        config_data["consistency_review_llm"] = self.consistency_review_llm_var.get()
        config_data["directory_continuation_llm"] = self.directory_continuation_llm_var.get()  # 新增
        config_data["directory_refinement_llm"] = self.directory_refinement_llm_var.get()      # 新增

        full_config["choose_configs"] = config_data
        save_config(full_config, self.config_file)
        messagebox.showinfo("提示", "模型指派配置已保存。")

    def refresh_config_dropdowns():
        """刷新所有配置下拉菜单"""
        config_names = list(self.loaded_config.get("llm_configs", {}).keys())
        if not config_names: config_names = ["Default"]
        
        dropdowns = [
            architecture_dropdown, chapter_outline_dropdown, prompt_draft_dropdown,
            refine_logic_dropdown, final_chapter_dropdown, consistency_review_dropdown,
            directory_continuation_dropdown, directory_refinement_dropdown  # 添加新下拉菜单
        ]
        
        for dropdown in dropdowns:
            dropdown.configure(values=config_names)
            if dropdown.cget("variable").get() not in config_names:
                dropdown.cget("variable").set(config_names[0])

    # --- 按钮 ---
    save_btn = ctk.CTkButton(
        self.config_choose, 
        text="保存配置", 
        command=save_config_choose,
        font=("Microsoft YaHei", 12)
    )
    save_btn.grid(row=10, column=0, padx=2, pady=10, sticky="ew")

    refresh_btn = ctk.CTkButton(
        self.config_choose, 
        text="刷新配置", 
        command=refresh_config_dropdowns,
        font=("Microsoft YaHei", 12)
    )
    refresh_btn.grid(row=10, column=1, padx=2, pady=10, sticky="ew")



# PenBo 增加代理功能支持
def build_proxy_setting_tab(self):
    # 代理设置标签页布局
    for i in range(5):
        self.proxy_setting_tab.grid_rowconfigure(i, weight=0)
    self.proxy_setting_tab.grid_columnconfigure(0, weight=0)
    self.proxy_setting_tab.grid_columnconfigure(1, weight=1)

    # 从配置文件加载代理设置
    config_data = load_config(self.config_file)
    proxy_setting = config_data.get("proxy_setting", {})
    
    # 代理启用开关
    create_label_with_help(self, self.proxy_setting_tab, "启用代理:", "proxy_enabled", 0, 0)
    self.proxy_enabled_var = ctk.BooleanVar(value=proxy_setting.get("enabled", False))
    proxy_enabled_switch = ctk.CTkSwitch(
        self.proxy_setting_tab,
        text="",
        variable=self.proxy_enabled_var,
        onvalue=True,
        offvalue=False,
        font=("Microsoft YaHei", 12)
    )
    proxy_enabled_switch.grid(row=0, column=1, padx=5, pady=5, sticky="w")

    # 地址输入框
    create_label_with_help(self, self.proxy_setting_tab, "地址:", "proxy_address", 1, 0)
    self.proxy_address_var = ctk.StringVar(value=proxy_setting.get("proxy_url", "127.0.0.1"))
    proxy_address_entry = ctk.CTkEntry(
        self.proxy_setting_tab,
        textvariable=self.proxy_address_var,
        font=("Microsoft YaHei", 12)
    )
    proxy_address_entry.grid(row=1, column=1, padx=5, pady=5, sticky="nsew")

    # 端口输入框
    create_label_with_help(self, self.proxy_setting_tab, "端口:", "proxy_port", 2, 0)
    self.proxy_port_var = ctk.StringVar(value=proxy_setting.get("proxy_port", "10809"))
    proxy_port_entry = ctk.CTkEntry(
        self.proxy_setting_tab,
        textvariable=self.proxy_port_var,
        font=("Microsoft YaHei", 12)
    )
    proxy_port_entry.grid(row=2, column=1, padx=5, pady=5, sticky="nsew")


    def open_proxy(address, port):
        """启动代理"""
        # 设置环境变量
        os.environ['HTTP_PROXY'] = f"http://{address}:{port}"
        os.environ['HTTPS_PROXY'] = f"http://{address}:{port}"

    def save_proxy_setting():
        config_data = load_config(self.config_file)
        if "proxy_setting" not in config_data:
            config_data["proxy_setting"] = {}
            
        config_data["proxy_setting"]["enabled"] = self.proxy_enabled_var.get()
        config_data["proxy_setting"]["proxy_url"] = self.proxy_address_var.get()
        config_data["proxy_setting"]["proxy_port"] = self.proxy_port_var.get()

        save_config(config_data, self.config_file)
        messagebox.showinfo("提示", "代理配置已保存。")

        if self.proxy_enabled_var.get():
            open_proxy(self.proxy_address_var.get(), self.proxy_port_var.get())
        else:
            os.environ.pop('HTTP_PROXY', None)
            os.environ.pop('HTTPS_PROXY', None)

    # 添加保存按钮
    save_btn = ctk.CTkButton(
        self.proxy_setting_tab,
        text="保存代理设置",
        command=save_proxy_setting,
        font=("Microsoft YaHei", 12)
    )
    save_btn.grid(row=3, column=0, columnspan=2, padx=5, pady=5, sticky="ew")


    

def load_config_btn(self):
    cfg = load_config(self.config_file)
    if cfg:
        last_llm = cfg.get("last_interface_format", "OpenAI")
        last_embedding = cfg.get("last_embedding_interface_format", "OpenAI")
        self.interface_format_var.set(last_llm)
        self.embedding_interface_format_var.set(last_embedding)
        llm_configs = cfg.get("llm_configs", {})
        if last_llm in llm_configs:
            llm_conf = llm_configs[last_llm]
            self.interface_format_var.set(llm_conf.get("interface_format", "OpenAI"))
            self.api_key_var.set(llm_conf.get("api_key", ""))
            self.base_url_var.set(llm_conf.get("base_url", "https://api.openai.com/v1"))
            self.model_name_var.set(llm_conf.get("model_name", "gpt-4o-mini"))
            self.temperature_var.set(llm_conf.get("temperature", 0.7))
            self.max_tokens_var.set(llm_conf.get("max_tokens", 8192))
            self.timeout_var.set(llm_conf.get("timeout", 600))
        embedding_configs = cfg.get("embedding_configs", {})
        if last_embedding in embedding_configs:
            emb_conf = embedding_configs[last_embedding]
            self.embedding_api_key_var.set(emb_conf.get("api_key", ""))
            self.embedding_url_var.set(emb_conf.get("base_url", "https://api.openai.com/v1"))
            self.embedding_model_name_var.set(emb_conf.get("model_name", "text-embedding-ada-002"))
            self.embedding_retrieval_k_var.set(str(emb_conf.get("retrieval_k", 4)))
        other_params = cfg.get("other_params", {})
        self.topic_text.delete("0.0", "end")
        self.topic_text.insert("0.0", other_params.get("topic", ""))
        self.genre_var.set(other_params.get("genre", "玄幻"))
        self.num_chapters_var.set(str(other_params.get("num_chapters", 10)))
        self.word_number_var.set(str(other_params.get("word_number", 3000)))
        self.filepath_var.set(other_params.get("filepath", ""))
        self.chapter_num_var.set(str(other_params.get("chapter_num", "1")))
        self.user_guide_text.delete("0.0", "end")
        self.user_guide_text.insert("0.0", other_params.get("user_guidance", ""))
        self.characters_involved_var.set(other_params.get("characters_involved", ""))
        self.key_items_var.set(other_params.get("key_items", ""))
        self.scene_location_var.set(other_params.get("scene_location", ""))
        self.time_constraint_var.set(other_params.get("time_constraint", ""))
        self.log("已加载配置。")
    else:
        messagebox.showwarning("提示", "未找到或无法读取配置文件。")

def save_config_btn(self):
    current_llm_interface = self.interface_format_var.get().strip()
    current_embedding_interface = self.embedding_interface_format_var.get().strip()
    llm_config = {
        "api_key": self.api_key_var.get(),
        "base_url": self.base_url_var.get(),
        "model_name": self.model_name_var.get(),
        "temperature": self.temperature_var.get(),
        "max_tokens": self.max_tokens_var.get(),
        "timeout": self.safe_get_int(self.timeout_var, 600),
        "interface_format": current_llm_interface
    }
    embedding_config = {
        "api_key": self.embedding_api_key_var.get(),
        "base_url": self.embedding_url_var.get(),
        "model_name": self.embedding_model_name_var.get(),
        "retrieval_k": self.safe_get_int(self.embedding_retrieval_k_var, 4),
        "interface_format": current_embedding_interface

    }
    other_params = {
        "topic": self.topic_text.get("0.0", "end").strip(),
        "genre": self.genre_var.get(),
        "num_chapters": self.safe_get_int(self.num_chapters_var, 10),
        "word_number": self.safe_get_int(self.word_number_var, 3000),
        "filepath": self.filepath_var.get(),
        "chapter_num": self.chapter_num_var.get(),
        "user_guidance": self.user_guide_text.get("0.0", "end").strip(),
        "characters_involved": self.characters_involved_var.get(),
        "key_items": self.key_items_var.get(),
        "scene_location": self.scene_location_var.get(),
        "time_constraint": self.time_constraint_var.get()
    }
    llm_config_name = self.base_url_var.get().split("/")[2] + " " + self.model_name_var.get()

    existing_config = load_config(self.config_file)
    if not existing_config:
        existing_config = {}
    existing_config["last_interface_format"] = current_llm_interface
    existing_config["last_embedding_interface_format"] = current_embedding_interface
    if "llm_configs" not in existing_config:
        existing_config["llm_configs"] = {}
    llm_config["config_name"] = llm_config_name

    existing_config["llm_configs"][llm_config_name] = llm_config

    if "embedding_configs" not in existing_config:
        existing_config["embedding_configs"] = {}
    existing_config["embedding_configs"][current_embedding_interface] = embedding_config

    existing_config["other_params"] = other_params

    if save_config(existing_config, self.config_file):
        messagebox.showinfo("提示", "配置已保存至 config.json")
        self.log("配置已保存。")
    else:
        messagebox.showerror("错误", "保存配置失败。")
</file>

<file path="ui/generation_handlers.py">
# ui/generation_handlers.py
# -*- coding: utf-8 -*-
import os
import re
import threading
import tkinter as tk
from tkinter import messagebox, filedialog
import customtkinter as ctk
import traceback
import glob
from utils import read_file, save_string_to_txt, clear_file_content
from novel_generator import (
    Novel_architecture_generate,
    Chapter_blueprint_generate,
    generate_chapter_draft,
    finalize_chapter,
    import_knowledge_file,
    clear_vector_store,
    enrich_chapter_text,
    build_chapter_prompt,
    analyze_chapter_logic,
    rewrite_chapter_with_feedback,
    refine_chapter_detail,
    answer_novel_question
)
from consistency_checker import check_consistency

def generate_novel_architecture_ui(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先选择保存文件路径")
        return

    def task():
        confirm = messagebox.askyesno("确认", "确定要生成小说架构吗？")
        if not confirm:
            self.enable_button_safe(self.btn_generate_architecture)
            return

        self.disable_button_safe(self.btn_generate_architecture)
        try:


            interface_format = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["interface_format"]
            api_key = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["api_key"]
            base_url = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["base_url"]
            model_name = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["model_name"]
            temperature = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["temperature"]
            max_tokens = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["max_tokens"]
            timeout_val = self.loaded_config["llm_configs"][self.architecture_llm_var.get()]["timeout"]



            topic = self.topic_text.get("0.0", "end").strip()
            genre = self.genre_var.get().strip()
            num_chapters = self.safe_get_int(self.num_chapters_var, 10)
            word_number = self.safe_get_int(self.word_number_var, 3000)
            # 获取内容指导
            user_guidance = self.user_guide_text.get("0.0", "end").strip()

            self.safe_log("开始生成小说架构...")
            Novel_architecture_generate(
                interface_format=interface_format,
                api_key=api_key,
                base_url=base_url,
                llm_model=model_name,
                topic=topic,
                genre=genre,
                number_of_chapters=num_chapters,
                word_number=word_number,
                filepath=filepath,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=timeout_val,
                user_guidance=user_guidance  # 添加内容指导参数
            )
            self.safe_log("✅ 小说架构生成完成。请在 'Novel Architecture' 标签页查看或编辑。")
        except Exception:
            self.handle_exception("生成小说架构时出错")
        finally:
            self.enable_button_safe(self.btn_generate_architecture)
    threading.Thread(target=task, daemon=True).start()

def generate_chapter_blueprint_ui(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先选择保存文件路径")
        return

    def task():
        if not messagebox.askyesno("确认", "确定要生成章节目录吗？"):
            self.enable_button_safe(self.btn_generate_chapter)
            return
        self.disable_button_safe(self.btn_generate_directory)
        try:

            number_of_chapters = self.safe_get_int(self.num_chapters_var, 10)

            interface_format = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["interface_format"]
            api_key = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["api_key"]
            base_url = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["base_url"]
            model_name = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["model_name"]
            temperature = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["temperature"]
            max_tokens = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["max_tokens"]
            timeout_val = self.loaded_config["llm_configs"][self.chapter_outline_llm_var.get()]["timeout"]


            user_guidance = self.user_guide_text.get("0.0", "end").strip()  # 新增获取用户指导

            self.safe_log("开始生成章节蓝图...")
            Chapter_blueprint_generate(
                interface_format=interface_format,
                api_key=api_key,
                base_url=base_url,
                llm_model=model_name,
                number_of_chapters=number_of_chapters,
                filepath=filepath,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=timeout_val,
                user_guidance=user_guidance  # 新增参数
            )
            self.safe_log("✅ 章节蓝图生成完成。请在 'Chapter Blueprint' 标签页查看或编辑。")
        except Exception:
            self.handle_exception("生成章节蓝图时出错")
        finally:
            self.enable_button_safe(self.btn_generate_directory)
    threading.Thread(target=task, daemon=True).start()

def generate_chapter_draft_ui(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径。")
        return

    def task():
        self.disable_button_safe(self.btn_generate_chapter)
        try:
            # === 1. 准备大模型参数 ===
            # 生成用的 LLM 配置
            draft_config_key = self.prompt_draft_llm_var.get()
            draft_config = self.loaded_config["llm_configs"][draft_config_key]
            
            draft_interface = draft_config["interface_format"]
            draft_key = draft_config["api_key"]
            draft_url = draft_config["base_url"]
            draft_model = draft_config["model_name"]
            draft_temp = draft_config["temperature"]
            draft_tokens = draft_config["max_tokens"]
            draft_timeout = draft_config["timeout"]

            # 自检用的 LLM 配置 (建议使用一致性审校的模型，通常逻辑更强)
            review_config_key = self.refine_logic_llm_var.get() 
            review_config = self.loaded_config["llm_configs"][review_config_key]
            
            review_interface = review_config["interface_format"]
            review_key = review_config["api_key"]
            review_url = review_config["base_url"]
            review_model = review_config["model_name"]
            # 补充审校模型的可选参数
            review_temp = review_config.get("temperature", 0.3)
            review_tokens = review_config.get("max_tokens", draft_tokens)
            
            # Embedding 参数
            emb_key = self.embedding_api_key_var.get().strip()
            emb_url = self.embedding_url_var.get().strip()
            emb_fmt = self.embedding_interface_format_var.get().strip()
            emb_model = self.embedding_model_name_var.get().strip()
            emb_k = self.safe_get_int(self.embedding_retrieval_k_var, 4)

            # 章节参数
            chap_num = self.safe_get_int(self.chapter_num_var, 1)
            word_num = self.safe_get_int(self.word_number_var, 3000)
            user_guide = self.user_guide_text.get("0.0", "end").strip()
            char_inv = self.characters_involved_var.get().strip()
            key_items = self.key_items_var.get().strip()
            scene_loc = self.scene_location_var.get().strip()
            time_constr = self.time_constraint_var.get().strip()

            self.safe_log(f"模型：{draft_model}，正在生成第{chap_num}章草稿提示词...")

            # === 2. 构造提示词并让用户确认 ===
            prompt_text = build_chapter_prompt(
                api_key=draft_key,
                base_url=draft_url,
                model_name=draft_model,
                filepath=filepath,
                novel_number=chap_num,
                word_number=word_num,
                temperature=draft_temp,
                user_guidance=user_guide,
                characters_involved=char_inv,
                key_items=key_items,
                scene_location=scene_loc,
                time_constraint=time_constr,
                embedding_api_key=emb_key,
                embedding_url=emb_url,
                embedding_interface_format=emb_fmt,
                embedding_model_name=emb_model,
                embedding_retrieval_k=emb_k,
                interface_format=draft_interface,
                max_tokens=draft_tokens,
                timeout=draft_timeout,
                # 选角/逻辑模型用于人物卡和主动验证
                cast_api_key=review_key,
                cast_base_url=review_url,
                cast_model_name=review_model,
                cast_interface_format=review_interface,
                cast_temperature=review_temp,
                cast_max_tokens=review_tokens,
                cast_timeout=draft_timeout,
            )

            # 弹出确认框逻辑 (含字数统计)
            result: dict[str, str | None] = {"prompt": None}
            event = threading.Event()

            def create_prompt_dialog():
                dialog = ctk.CTkToplevel(self.master)
                dialog.title("确认提示词")
                dialog.geometry("800x600")
                
                # 顶部栏：标题 + 字数统计
                header_frame = ctk.CTkFrame(dialog, fg_color="transparent")
                header_frame.pack(fill="x", padx=10, pady=(10,0))
                ctk.CTkLabel(header_frame, text="生成提示词内容", font=("Microsoft YaHei", 12, "bold")).pack(side="left")
                prompt_wc_label = ctk.CTkLabel(header_frame, text="字数：0", font=("Microsoft YaHei", 12))
                prompt_wc_label.pack(side="right")

                text_box = ctk.CTkTextbox(dialog, wrap="word", font=("Microsoft YaHei", 12))
                text_box.pack(fill="both", expand=True, padx=10, pady=5)
                
                # 处理角色内容插入 (保持原有逻辑)
                final_prompt = prompt_text
                role_names = [name.strip() for name in self.char_inv_text.get("0.0", "end").strip().split(',') if name.strip()]
                role_lib_path = os.path.join(filepath, "角色库")
                role_contents = []
                
                if os.path.exists(role_lib_path):
                    for root, dirs, files in os.walk(role_lib_path):
                        for file in files:
                            if file.endswith(".txt") and os.path.splitext(file)[0] in role_names:
                                try:
                                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                                        role_contents.append(f.read().strip())
                                except Exception: pass
                
                if role_contents:
                    role_content_str = "\n".join(role_contents)
                    placeholder_variations = [
                        "核心人物(可能未指定)：{characters_involved}",
                        "核心人物：{characters_involved}",
                        "核心人物:{characters_involved}"
                    ]
                    for ph in placeholder_variations:
                        if ph in final_prompt:
                            final_prompt = final_prompt.replace(ph, f"核心人物：\n{role_content_str}")
                            break
                    else:
                        lines = final_prompt.split('\n')
                        for i, line in enumerate(lines):
                            if "核心人物" in line and "：" in line:
                                lines[i] = f"核心人物：\n{role_content_str}"
                                break
                        final_prompt = '\n'.join(lines)

                text_box.insert("0.0", final_prompt)
                
                # 提示词字数更新逻辑
                def update_prompt_wc(e=None):
                    t = text_box.get("0.0", "end-1c")
                    prompt_wc_label.configure(text=f"字数：{len(t)}")
                text_box.bind("<KeyRelease>", update_prompt_wc)
                update_prompt_wc() # 初始化

                btn_frame = ctk.CTkFrame(dialog)
                btn_frame.pack(pady=10)
                
                def on_confirm():
                    result["prompt"] = text_box.get("1.0", "end").strip()
                    dialog.destroy()
                    event.set()
                
                def on_cancel():
                    dialog.destroy()
                    event.set()

                ctk.CTkButton(btn_frame, text="生成草稿", command=on_confirm).pack(side="left", padx=10)
                ctk.CTkButton(btn_frame, text="取消", command=on_cancel, fg_color="gray").pack(side="left", padx=10)
                dialog.protocol("WM_DELETE_WINDOW", on_cancel)

            self.master.after(0, create_prompt_dialog)
            event.wait()
            
            final_prompt = result["prompt"]
            if not final_prompt:
                self.safe_log("已取消生成。")
                return

            # === 3. 生成初稿 ===
            self.safe_log("正在生成草稿正文，请稍候...")
            draft_text = generate_chapter_draft(
                api_key=draft_key, base_url=draft_url, model_name=draft_model,
                filepath=filepath, novel_number=chap_num, word_number=word_num,
                temperature=draft_temp, user_guidance=user_guide,
                characters_involved=char_inv, key_items=key_items,
                scene_location=scene_loc, time_constraint=time_constr,
                embedding_api_key=emb_key, embedding_url=emb_url,
                embedding_interface_format=emb_fmt, embedding_model_name=emb_model,
                embedding_retrieval_k=emb_k, interface_format=draft_interface,
                max_tokens=draft_tokens, timeout=draft_timeout,
                custom_prompt_text=final_prompt
            )

            if not draft_text:
                self.safe_log("生成失败：返回内容为空。")
                return

            self.safe_log(f"✅模型：{draft_model}, 初稿生成完毕，正在进行逻辑自检...")

            # === 4. 自动逻辑自检 ===
            logic_report = analyze_chapter_logic(
                interface_format=review_interface,
                api_key=review_key,
                base_url=review_url,
                model_name=review_model,
                chapter_content=draft_text,
                filepath=filepath,
                novel_number=chap_num,
                timeout=draft_timeout
            )

            # === 5. 弹出“逻辑自检与修订”窗口 (含正文字数统计) ===
            def show_logic_check_window():
                check_win = ctk.CTkToplevel(self.master)
                check_win.title(f"逻辑自检与修订 - 第{chap_num}章")
                check_win.geometry("1200x800")
                
                # 布局配置
                check_win.grid_columnconfigure(0, weight=3) # 正文区域更宽
                check_win.grid_columnconfigure(1, weight=2)
                check_win.grid_rowconfigure(0, weight=1)
                
                # --- 左侧：正文编辑区 ---
                left_frame = ctk.CTkFrame(check_win)
                left_frame.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)
                
                # 左侧顶部栏（标题 + 字数统计）
                left_header = ctk.CTkFrame(left_frame, fg_color="transparent")
                left_header.pack(fill="x", pady=5, padx=5)
                ctk.CTkLabel(left_header, text="章节正文 (可手动修改)", font=("Microsoft YaHei", 14, "bold")).pack(side="left")
                content_wc_label = ctk.CTkLabel(left_header, text="字数：0", font=("Microsoft YaHei", 12))
                content_wc_label.pack(side="right")

                content_box = ctk.CTkTextbox(left_frame, wrap="word", font=("Microsoft YaHei", 12))
                content_box.pack(fill="both", expand=True, padx=5, pady=5)
                content_box.insert("0.0", draft_text)
                
                # 正文字数更新逻辑
                def update_content_wc(event=None):
                    text = content_box.get("0.0", "end-1c")
                    content_wc_label.configure(text=f"字数：{len(text)}")
                
                content_box.bind("<KeyRelease>", update_content_wc)
                content_box.bind("<ButtonRelease>", update_content_wc)
                update_content_wc() # 初始化统计

                # --- 右侧：逻辑反馈区 ---
                right_frame = ctk.CTkFrame(check_win)
                right_frame.grid(row=0, column=1, sticky="nsew", padx=5, pady=5)
                ctk.CTkLabel(right_frame, text="逻辑漏洞报告 (可编辑反馈意见)", font=("Microsoft YaHei", 14, "bold")).pack(pady=5)

                # 解析报告中的特定段落，便于单独查看与复制
                def _parse_report_sections(text: str) -> dict:
                    sections = {
                        "knowledge_pov": "",
                        "future_conflict": "",
                        "full": text
                    }
                    try:
                        k_marker = "【知识不一致 & POV 异常】"
                        f_marker = "【后文目录冲突】"
                        k_idx = text.find(k_marker)
                        f_idx = text.find(f_marker)
                        if k_idx != -1:
                            # 从 k_marker 到 f_marker 或 文本末尾
                            start = k_idx + len(k_marker)
                            end = f_idx if (f_idx != -1 and f_idx > k_idx) else len(text)
                            sections["knowledge_pov"] = text[start:end].strip()
                        if f_idx != -1:
                            start = f_idx + len(f_marker)
                            sections["future_conflict"] = text[start:].strip()
                    except Exception:
                        pass
                    return sections

                parsed_sections = _parse_report_sections(logic_report if logic_report else "")

                # 快速查看按钮
                btns_frame = ctk.CTkFrame(right_frame, fg_color="transparent")
                btns_frame.pack(fill="x", padx=5, pady=(0, 4))

                def _open_section_popup(title: str, content: str):
                    popup = ctk.CTkToplevel(self.master)
                    popup.title(title)
                    popup.geometry("700x400")
                    txt = ctk.CTkTextbox(popup, wrap="word")
                    txt.pack(fill="both", expand=True, padx=8, pady=8)
                    txt.insert("0.0", content if content else "(无内容)")

                    def _copy_to_feedback():
                        feedback_box.delete("0.0", "end")
                        feedback_box.insert("0.0", txt.get("0.0", "end").strip())
                        popup.destroy()

                    footer = ctk.CTkFrame(popup)
                    footer.pack(fill="x", padx=8, pady=6)
                    ctk.CTkButton(footer, text="复制到反馈框", command=_copy_to_feedback, fg_color="#3498DB").pack(side="right", padx=6)
                    ctk.CTkButton(footer, text="关闭", command=popup.destroy, fg_color="#95A5A6").pack(side="right")

                kb_btn = ctk.CTkButton(btns_frame, text="查看 知识不一致 & POV", width=200, command=lambda: _open_section_popup("知识不一致 & POV 异常", parsed_sections.get("knowledge_pov", "")))
                kb_btn.pack(side="right", padx=6)
                fc_btn = ctk.CTkButton(btns_frame, text="查看 后文目录冲突", width=200, command=lambda: _open_section_popup("后文目录冲突", parsed_sections.get("future_conflict", "")))
                fc_btn.pack(side="right", padx=6)

                feedback_box = ctk.CTkTextbox(right_frame, wrap="word", font=("Microsoft YaHei", 12))
                feedback_box.pack(fill="both", expand=True, padx=5, pady=5)
                feedback_box.insert("0.0", logic_report)
                
                # --- 底部：按钮区 ---
                btn_frame = ctk.CTkFrame(check_win)
                btn_frame.grid(row=1, column=0, columnspan=2, sticky="ew", padx=10, pady=10)
                
                status_lbl = ctk.CTkLabel(btn_frame, text="等待操作...", text_color="gray")
                status_lbl.pack(side="left", padx=10)

                def _run_rewrite_with_model(use_interface, use_key, use_url, use_model, use_temp, use_tokens, use_timeout):
                    current_content = content_box.get("0.0", "end").strip()
                    current_feedback = feedback_box.get("0.0", "end").strip()
                    if not current_content:
                        return

                    status_lbl.configure(text=f"{use_model}⏳ 正在根据反馈重写，请稍候...", text_color="blue")
                    logic_fix_btn.configure(state="disabled")
                    plot_refine_btn.configure(state="disabled")
                    confirm_btn.configure(state="disabled")

                    def run_rewrite():
                        try:
                            new_text = rewrite_chapter_with_feedback(
                                interface_format=use_interface,
                                api_key=use_key,
                                base_url=use_url,
                                model_name=use_model,
                                original_content=current_content,
                                feedback=current_feedback,
                                temperature=use_temp,
                                max_tokens=use_tokens,
                                timeout=use_timeout
                            )
                            if new_text:
                                self.master.after(0, lambda: content_box.delete("0.0", "end"))
                                self.master.after(0, lambda: content_box.insert("0.0", new_text))
                                self.master.after(0, lambda: feedback_box.delete("0.0", "end"))
                                self.master.after(0, lambda: feedback_box.insert("0.0", "（已根据意见重写。请检查左侧内容，如有新问题可继续输入反馈。）"))
                                # 重写完成后更新字数
                                self.master.after(0, update_content_wc)
                                self.master.after(0, lambda: status_lbl.configure(text="✅ 重写完成", text_color="green"))
                            else:
                                self.master.after(0, lambda: status_lbl.configure(text="❌ 重写失败", text_color="red"))
                        except Exception as e:
                            self.master.after(0, lambda: status_lbl.configure(text=f"❌ 出错: {str(e)}", text_color="red"))
                        finally:
                            self.master.after(0, lambda: logic_fix_btn.configure(state="normal"))
                            self.master.after(0, lambda: plot_refine_btn.configure(state="normal"))
                            self.master.after(0, lambda: confirm_btn.configure(state="normal"))

                    threading.Thread(target=run_rewrite, daemon=True).start()

                def on_rewrite_logic():
                    # 使用逻辑/审校模型进行修正（侧重逻辑一致性）
                    _run_rewrite_with_model(
                        use_interface=review_interface,
                        use_key=review_key,
                        use_url=review_url,
                        use_model=review_model,
                        use_temp=review_temp if 'review_temp' in locals() else 0.3,
                        use_tokens=review_tokens if 'review_tokens' in locals() else draft_tokens,
                        use_timeout=draft_timeout
                    )

                def on_rewrite_plot():
                    # 使用初稿生成模型进行剧情微调（保持文风与拓展）
                    _run_rewrite_with_model(
                        use_interface=draft_interface,
                        use_key=draft_key,
                        use_url=draft_url,
                        use_model=draft_model,
                        use_temp=draft_temp,
                        use_tokens=draft_tokens,
                        use_timeout=draft_timeout
                    )

                def on_confirm():
                    final_content = content_box.get("0.0", "end").strip()
                    
                    # 保存到文件
                    chapters_dir = os.path.join(filepath, "chapters")
                    os.makedirs(chapters_dir, exist_ok=True)
                    chapter_file = os.path.join(chapters_dir, f"chapter_{chap_num}.txt")
                    
                    clear_file_content(chapter_file)
                    save_string_to_txt(final_content, chapter_file)
                    
                    # 更新主界面显示
                    self.show_chapter_in_textbox(final_content)
                    self.safe_log(f"✅ 第{chap_num}章已确认并保存。")
                    check_win.destroy()

                logic_fix_btn = ctk.CTkButton(btn_frame, text="逻辑纠正 (Logic Fix)", command=on_rewrite_logic, fg_color="#E67E22", width=180)
                logic_fix_btn.pack(side="right", padx=6)

                plot_refine_btn = ctk.CTkButton(btn_frame, text="剧情微调 (Plot Refine)", command=on_rewrite_plot, fg_color="#8E44AD", width=180)
                plot_refine_btn.pack(side="right", padx=6)
                
                confirm_btn = ctk.CTkButton(btn_frame, text="确认无误，使用此版本 (Confirm)", command=on_confirm, fg_color="#27AE60", width=200)
                confirm_btn.pack(side="right", padx=10)
                
                check_win.protocol("WM_DELETE_WINDOW", on_confirm)

            self.master.after(0, show_logic_check_window)

        except Exception as e:
            self.handle_exception("生成草稿流程出错")
        finally:
            self.enable_button_safe(self.btn_generate_chapter)

    threading.Thread(target=task, daemon=True).start()

def finalize_chapter_ui(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径。")
        return

    def task():
        if not messagebox.askyesno("确认", "确定要定稿当前章节吗？"):
            self.enable_button_safe(self.btn_finalize_chapter)
            return

        self.disable_button_safe(self.btn_finalize_chapter)
        try:

            interface_format = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["interface_format"]
            api_key = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["api_key"]
            base_url = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["base_url"]
            model_name = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["model_name"]
            temperature = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["temperature"]
            max_tokens = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["max_tokens"]
            timeout_val = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["timeout"]


            embedding_api_key = self.embedding_api_key_var.get().strip()
            embedding_url = self.embedding_url_var.get().strip()
            embedding_interface_format = self.embedding_interface_format_var.get().strip()
            embedding_model_name = self.embedding_model_name_var.get().strip()

            chap_num = self.safe_get_int(self.chapter_num_var, 1)
            word_number = self.safe_get_int(self.word_number_var, 3000)

            self.safe_log(f"开始定稿第{chap_num}章...")

            chapters_dir = os.path.join(filepath, "chapters")
            os.makedirs(chapters_dir, exist_ok=True)
            chapter_file = os.path.join(chapters_dir, f"chapter_{chap_num}.txt")

            edited_text = self.chapter_result.get("0.0", "end").strip()

            if len(edited_text) < 0.7 * word_number:
                ask = messagebox.askyesno("字数不足", f"当前章节字数 ({len(edited_text)}) 低于目标字数({word_number})的70%，是否要尝试扩写？")
                if ask:
                    self.safe_log("正在扩写章节内容...")
                    enriched = enrich_chapter_text(
                        chapter_text=edited_text,
                        word_number=word_number,
                        api_key=api_key,
                        base_url=base_url,
                        model_name=model_name,
                        temperature=temperature,
                        interface_format=interface_format,
                        max_tokens=max_tokens,
                        timeout=timeout_val
                    )
                    edited_text = enriched
                    self.master.after(0, lambda: self.chapter_result.delete("0.0", "end"))
                    self.master.after(0, lambda: self.chapter_result.insert("0.0", edited_text))
            clear_file_content(chapter_file)
            save_string_to_txt(edited_text, chapter_file)

            finalize_chapter(
                novel_number=chap_num,
                word_number=word_number,
                api_key=api_key,
                base_url=base_url,
                model_name=model_name,
                temperature=temperature,
                filepath=filepath,
                embedding_api_key=embedding_api_key,
                embedding_url=embedding_url,
                embedding_interface_format=embedding_interface_format,
                embedding_model_name=embedding_model_name,
                interface_format=interface_format,
                max_tokens=max_tokens,
                timeout=timeout_val
            )
            self.safe_log(f"✅ 第{chap_num}章定稿完成（已更新前文摘要、角色状态、向量库）。")

            final_text = read_file(chapter_file)
            self.master.after(0, lambda: self.show_chapter_in_textbox(final_text))
        except Exception:
            self.handle_exception("定稿章节时出错")
        finally:
            self.enable_button_safe(self.btn_finalize_chapter)
    threading.Thread(target=task, daemon=True).start()

def do_consistency_check(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径。")
        return

    def task():
        self.disable_button_safe(self.btn_check_consistency)
        try:
            interface_format = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["interface_format"]
            api_key = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["api_key"]
            base_url = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["base_url"]
            model_name = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["model_name"]
            temperature = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["temperature"]
            max_tokens = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["max_tokens"]
            timeout = self.loaded_config["llm_configs"][self.consistency_review_llm_var.get()]["timeout"]


            chap_num = self.safe_get_int(self.chapter_num_var, 1)
            chap_file = os.path.join(filepath, "chapters", f"chapter_{chap_num}.txt")
            chapter_text = read_file(chap_file)

            if not chapter_text.strip():
                self.safe_log("⚠️ 当前章节文件为空或不存在，无法审校。")
                return

            self.safe_log("开始一致性审校...")
            result = check_consistency(
                novel_setting="",
                character_state=read_file(os.path.join(filepath, "character_state.txt")),
                global_summary=read_file(os.path.join(filepath, "global_summary.txt")),
                chapter_text=chapter_text,
                api_key=api_key,
                base_url=base_url,
                model_name=model_name,
                temperature=temperature,
                interface_format=interface_format,
                max_tokens=max_tokens,
                timeout=timeout,
                plot_arcs=""
            )
            self.safe_log("审校结果：")
            self.safe_log(result)
        except Exception:
            self.handle_exception("审校时出错")
        finally:
            self.enable_button_safe(self.btn_check_consistency)
    threading.Thread(target=task, daemon=True).start()
def generate_batch_ui(self):

    # PenBo 优化界面，使用customtkinter进行批量生成章节界面
    def open_batch_dialog():
        dialog = ctk.CTkToplevel()
        dialog.title("批量生成章节")
        
        chapter_file = os.path.join(self.filepath_var.get().strip(), "chapters")
        files = glob.glob(os.path.join(chapter_file, "chapter_*.txt"))
        if not files:
            num = 1
        else:
            num = max(int(os.path.basename(f).split('_')[1].split('.')[0]) for f in files) + 1
            
        dialog.geometry("400x200")
        dialog.resizable(False, False)
        
        # 创建网格布局
        dialog.grid_columnconfigure(0, weight=0)
        dialog.grid_columnconfigure(1, weight=1)
        dialog.grid_columnconfigure(2, weight=0)
        dialog.grid_columnconfigure(3, weight=1)
        
        # 起始章节
        ctk.CTkLabel(dialog, text="起始章节:").grid(row=0, column=0, padx=10, pady=10, sticky="w")
        entry_start = ctk.CTkEntry(dialog)
        entry_start.grid(row=0, column=1, padx=10, pady=10, sticky="ew")
        entry_start.insert(0, str(num))
        
        # 结束章节
        ctk.CTkLabel(dialog, text="结束章节:").grid(row=0, column=2, padx=10, pady=10, sticky="w")
        entry_end = ctk.CTkEntry(dialog)
        entry_end.grid(row=0, column=3, padx=10, pady=10, sticky="ew")
        
        # 期望字数
        ctk.CTkLabel(dialog, text="期望字数:").grid(row=1, column=0, padx=10, pady=10, sticky="w")
        entry_word = ctk.CTkEntry(dialog)
        entry_word.grid(row=1, column=1, padx=10, pady=10, sticky="ew")
        entry_word.insert(0, self.word_number_var.get())
        
        # 最低字数
        ctk.CTkLabel(dialog, text="最低字数:").grid(row=1, column=2, padx=10, pady=10, sticky="w")
        entry_min = ctk.CTkEntry(dialog)
        entry_min.grid(row=1, column=3, padx=10, pady=10, sticky="ew")
        entry_min.insert(0, self.word_number_var.get())

        # 自动扩写选项
        auto_enrich_bool = ctk.BooleanVar()
        auto_enrich_bool_ck = ctk.CTkCheckBox(dialog, text="低于最低字数时自动扩写", variable=auto_enrich_bool)
        auto_enrich_bool_ck.grid(row=2, column=0, columnspan=2, padx=10, pady=10, sticky="w")

        result = {"start": None, "end": None, "word": None, "min": None, "auto_enrich": None, "close": False}

        def on_confirm():
            nonlocal result
            if not entry_start.get() or not entry_end.get() or not entry_word.get() or not entry_min.get():
                messagebox.showwarning("警告", "请填写完整信息。")
                return

            result = {
                "start": entry_start.get(),
                "end": entry_end.get(),
                "word": entry_word.get(),
                "min": entry_min.get(),
                "auto_enrich": auto_enrich_bool.get(),
                "close": False
            }
            dialog.destroy()

        def on_cancel():
            nonlocal result
            result["close"] = True
            dialog.destroy()
            
        # 按钮框架
        button_frame = ctk.CTkFrame(dialog)
        button_frame.grid(row=3, column=0, columnspan=4, padx=10, pady=10, sticky="ew")
        button_frame.grid_columnconfigure(0, weight=1)
        button_frame.grid_columnconfigure(1, weight=1)
        
        ctk.CTkButton(button_frame, text="确认", command=on_confirm).grid(row=0, column=0, padx=10, pady=10, sticky="e")
        ctk.CTkButton(button_frame, text="取消", command=on_cancel).grid(row=0, column=1, padx=10, pady=10, sticky="w")
        
        dialog.protocol("WM_DELETE_WINDOW", on_cancel)
        dialog.transient(self.master)
        dialog.wait_window(dialog)
        return result
    
    def generate_chapter_batch(self ,i ,word, min, auto_enrich):
        draft_interface_format = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["interface_format"]
        draft_api_key = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["api_key"]
        draft_base_url = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["base_url"]
        draft_model_name = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["model_name"]
        draft_temperature = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["temperature"]
        draft_max_tokens = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["max_tokens"]
        draft_timeout = self.loaded_config["llm_configs"][self.prompt_draft_llm_var.get()]["timeout"]
        user_guidance = self.user_guide_text.get("0.0", "end").strip()  

        char_inv = self.characters_involved_var.get().strip()
        key_items = self.key_items_var.get().strip()
        scene_loc = self.scene_location_var.get().strip()
        time_constr = self.time_constraint_var.get().strip()

        embedding_api_key = self.embedding_api_key_var.get().strip()
        embedding_url = self.embedding_url_var.get().strip()
        embedding_interface_format = self.embedding_interface_format_var.get().strip()
        embedding_model_name = self.embedding_model_name_var.get().strip()
        embedding_k = self.safe_get_int(self.embedding_retrieval_k_var, 4)

        # 逻辑/选角模型配置（用于人物卡/主动验证）
        logic_cfg = self.loaded_config["llm_configs"][self.refine_logic_llm_var.get()]

        prompt_text = build_chapter_prompt(
            api_key=draft_api_key,
            base_url=draft_base_url,
            model_name=draft_model_name,
            filepath=self.filepath_var.get().strip(),
            novel_number=i,
            word_number=word,
            temperature=draft_temperature,
            user_guidance=user_guidance,
            characters_involved=char_inv,
            key_items=key_items,
            scene_location=scene_loc,
            time_constraint=time_constr,
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            embedding_retrieval_k=embedding_k,
            interface_format=draft_interface_format,
            max_tokens=draft_max_tokens,
            timeout=draft_timeout,
            cast_api_key=logic_cfg.get("api_key", ""),
            cast_base_url=logic_cfg.get("base_url", ""),
            cast_model_name=logic_cfg.get("model_name", ""),
            cast_interface_format=logic_cfg.get("interface_format", draft_interface_format),
            cast_temperature=logic_cfg.get("temperature", draft_temperature),
            cast_max_tokens=logic_cfg.get("max_tokens", draft_max_tokens),
            cast_timeout=logic_cfg.get("timeout", draft_timeout),
        )
        final_prompt = prompt_text
        role_names = [name.strip() for name in self.char_inv_text.get("0.0", "end").split("\n")]
        role_lib_path = os.path.join(self.filepath_var.get().strip(), "角色库")
        role_contents = []
        if os.path.exists(role_lib_path):
            for root, dirs, files in os.walk(role_lib_path):
                for file in files:
                    if file.endswith(".txt") and os.path.splitext(file)[0] in role_names:
                        file_path = os.path.join(root, file)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                role_contents.append(f.read().strip())  # 直接使用文件内容，不添加重复名字
                        except Exception as e:
                            self.safe_log(f"读取角色文件 {file} 失败: {str(e)}")
        if role_contents:
            role_content_str = "\n".join(role_contents)
            # 更精确的替换逻辑，处理不同情况下的占位符
            placeholder_variations = [
                "核心人物(可能未指定)：{characters_involved}",
                "核心人物：{characters_involved}",
                "核心人物(可能未指定):{characters_involved}",
                "核心人物:{characters_involved}"
            ]
            
            for placeholder in placeholder_variations:
                if placeholder in final_prompt:
                    final_prompt = final_prompt.replace(
                        placeholder,
                        f"核心人物：\n{role_content_str}"
                    )
                    break
            else:  # 如果没有找到任何已知占位符变体
                lines = final_prompt.split('\n')
                for i, line in enumerate(lines):
                    if "核心人物" in line and "：" in line:
                        lines[i] = f"核心人物：\n{role_content_str}"
                        break
                final_prompt = '\n'.join(lines)
        draft_text = generate_chapter_draft(
            api_key=draft_api_key,
            base_url=draft_base_url,
            model_name=draft_model_name,
            filepath=self.filepath_var.get().strip(),
            novel_number=i,
            word_number=word,
            temperature=draft_temperature,
            user_guidance=user_guidance,
            characters_involved=char_inv,
            key_items=key_items,
            scene_location=scene_loc,
            time_constraint=time_constr,
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            embedding_retrieval_k=embedding_k,
            interface_format=draft_interface_format,
            max_tokens=draft_max_tokens,
            timeout=draft_timeout,
            custom_prompt_text=final_prompt  
        )

        finalize_interface_format = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["interface_format"]
        finalize_api_key = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["api_key"]
        finalize_base_url = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["base_url"]
        finalize_model_name = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["model_name"]
        finalize_temperature = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["temperature"]
        finalize_max_tokens = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["max_tokens"]
        finalize_timeout = self.loaded_config["llm_configs"][self.final_chapter_llm_var.get()]["timeout"]

        chapters_dir = os.path.join(self.filepath_var.get().strip(), "chapters")
        os.makedirs(chapters_dir, exist_ok=True)
        chapter_path = os.path.join(chapters_dir, f"chapter_{i}.txt")
        if len(draft_text) < 0.7 * min and auto_enrich:
            self.safe_log(f"第{i}章草稿字数 ({len(draft_text)}) 低于目标字数({min})的70%，正在扩写...")
            enriched = enrich_chapter_text(
                chapter_text=draft_text,
                word_number=word,
                api_key=draft_api_key,
                base_url=draft_base_url,
                model_name=draft_model_name,
                temperature=draft_temperature,
                interface_format=draft_interface_format,
                max_tokens=draft_max_tokens,
                timeout=draft_timeout
            )
            draft_text = enriched
        clear_file_content(chapter_path)
        save_string_to_txt(draft_text, chapter_path)
        finalize_chapter(
            novel_number=i,
            word_number=word,
            api_key=finalize_api_key,
            base_url=finalize_base_url,
            model_name=finalize_model_name,
            temperature=finalize_temperature,
            filepath=self.filepath_var.get().strip(),
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            interface_format=finalize_interface_format,
            max_tokens=finalize_max_tokens,
            timeout=finalize_timeout
        )


    result = open_batch_dialog()
    if result["close"]:
        return

    for i in range(int(result["start"]), int(result["end"]) + 1):
        generate_chapter_batch(self, i, int(result["word"]), int(result["min"]), result["auto_enrich"])


def import_knowledge_handler(self):
    selected_file = filedialog.askopenfilename(
        title="选择要导入的知识库文件",
        filetypes=[("Text Files", "*.txt"), ("All Files", "*.*")]
    )
    if selected_file:
        def task():
            self.disable_button_safe(self.btn_import_knowledge)
            try:
                emb_api_key = self.embedding_api_key_var.get().strip()
                emb_url = self.embedding_url_var.get().strip()
                emb_format = self.embedding_interface_format_var.get().strip()
                emb_model = self.embedding_model_name_var.get().strip()

                # 尝试不同编码读取文件
                content = None
                encodings = ['utf-8', 'gbk', 'gb2312', 'ansi']
                for encoding in encodings:
                    try:
                        with open(selected_file, 'r', encoding=encoding) as f:
                            content = f.read()
                            break
                    except UnicodeDecodeError:
                        continue
                    except Exception as e:
                        self.safe_log(f"读取文件时发生错误: {str(e)}")
                        raise

                if content is None:
                    raise Exception("无法以任何已知编码格式读取文件")

                # 创建临时UTF-8文件
                import tempfile
                import os
                with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt') as temp:
                    temp.write(content)
                    temp_path = temp.name

                try:
                    self.safe_log(f"开始导入知识库文件: {selected_file}")
                    import_knowledge_file(
                        embedding_api_key=emb_api_key,
                        embedding_url=emb_url,
                        embedding_interface_format=emb_format,
                        embedding_model_name=emb_model,
                        file_path=temp_path,
                        filepath=self.filepath_var.get().strip()
                    )
                    self.safe_log("✅ 知识库文件导入完成。")
                finally:
                    # 清理临时文件
                    try:
                        os.unlink(temp_path)
                    except:
                        pass

            except Exception:
                self.handle_exception("导入知识库时出错")
            finally:
                self.enable_button_safe(self.btn_import_knowledge)

        try:
            thread = threading.Thread(target=task, daemon=True)
            thread.start()
        except Exception as e:
            self.enable_button_safe(self.btn_import_knowledge)
            messagebox.showerror("错误", f"线程启动失败: {str(e)}")

def clear_vectorstore_handler(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径。")
        return

    first_confirm = messagebox.askyesno("警告", "确定要清空本地向量库吗？此操作不可恢复！")
    if first_confirm:
        second_confirm = messagebox.askyesno("二次确认", "你确定真的要删除所有向量数据吗？此操作不可恢复！")
        if second_confirm:
            if clear_vector_store(filepath):
                self.log("已清空向量库。")
            else:
                self.log(f"未能清空向量库，请关闭程序后手动删除 {filepath} 下的 vectorstore 文件夹。")

def show_plot_arcs_ui(self):
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先在主Tab中设置保存文件路径")
        return

    plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
    if not os.path.exists(plot_arcs_file):
        messagebox.showinfo("剧情要点", "当前还未生成任何剧情要点或冲突记录。")
        return

    arcs_text = read_file(plot_arcs_file).strip()
    if not arcs_text:
        arcs_text = "当前没有记录的剧情要点或冲突。"

    top = ctk.CTkToplevel(self.master)
    top.title("剧情要点/未解决冲突")
    top.geometry("600x400")
    text_area = ctk.CTkTextbox(top, wrap="word", font=("Microsoft YaHei", 12))
    text_area.pack(fill="both", expand=True, padx=10, pady=10)
    text_area.insert("0.0", arcs_text)
    text_area.configure(state="disabled")


def refine_directory_card_ui(self):
    """
    微调章节目录的交互界面 (支持多章节范围修改)
    """
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径。")
        return
        
    directory_file = os.path.join(filepath, "Novel_directory.txt")
    if not os.path.exists(directory_file):
        messagebox.showwarning("警告", "尚未生成目录文件 (Novel_directory.txt)。")
        return

    # 创建弹窗
    dialog = ctk.CTkToplevel(self.master)
    dialog.title("微调章节大纲 (支持多章节)")
    dialog.geometry("1000x750")
    
    # 布局配置
    dialog.grid_columnconfigure(0, weight=1)
    dialog.grid_rowconfigure(1, weight=1)

    # --- 顶部控制区 ---
    top_frame = ctk.CTkFrame(dialog)
    top_frame.grid(row=0, column=0, sticky="ew", padx=10, pady=10)
    
    ctk.CTkLabel(top_frame, text="起始章节:").pack(side="left", padx=(10, 5))
    start_chap_entry = ctk.CTkEntry(top_frame, width=60)
    start_chap_entry.pack(side="left", padx=5)
    
    ctk.CTkLabel(top_frame, text="结束章节:").pack(side="left", padx=(15, 5))
    end_chap_entry = ctk.CTkEntry(top_frame, width=60)
    end_chap_entry.pack(side="left", padx=5)
    
    # 辅助函数：构造正则
    def get_range_pattern(start_num, end_num):
        # 匹配从 "第{start}章" 开始，一直到 "第{end+1}章" 之前（或文件末尾）的所有内容
        # 兼容 "章节编号：第X章" 和 "第X章" 两种格式
        # 这里的逻辑是：找到 Start 的开头，然后向后找，直到找到 (End+1) 的开头
        next_num = end_num + 1
        pattern_str = (
            f"(?:(?:章节编号：\\s*)?第\\s*{start_num}\\s*章)"  # 起始标记
            f".*?"                                          # 中间内容 (非贪婪)
            f"(?=(?:\\n\\s*(?:章节编号：\\s*)?第\\s*{next_num}\\s*章)|\\Z)" # 结束标记 (前瞻断言：是下一章开头 或 文件末尾)
        )
        return re.compile(pattern_str, re.DOTALL)

    def load_chapter_info():
        s_val = start_chap_entry.get().strip()
        e_val = end_chap_entry.get().strip()
        
        if not s_val:
            messagebox.showwarning("提示", "请输入起始章节号")
            return
        
        try:
            start_num = int(s_val)
            # 如果没填结束章节，默认等于起始章节（单章模式）
            end_num = int(e_val) if e_val else start_num
            
            if end_num < start_num:
                messagebox.showerror("错误", "结束章节不能小于起始章节")
                return

            content = read_file(directory_file)
            pattern = get_range_pattern(start_num, end_num)
            
            match = pattern.search(content)
            if match:
                extracted = match.group(0).strip()
                outline_text.delete("0.0", "end")
                outline_text.insert("0.0", extracted)
                status_label.configure(text=f"已加载: 第 {start_num} - {end_num} 章", text_color="green")
                return True
            else:
                status_label.configure(text=f"未找到章节范围 {start_num}-{end_num}，请检查目录文件。", text_color="red")
                return False
                
        except ValueError:
            messagebox.showerror("错误", "章节号必须是数字")
        except Exception as e:
            messagebox.showerror("错误", f"读取失败: {str(e)}")
            return False

    ctk.CTkButton(top_frame, text="读取范围大纲", command=load_chapter_info, width=120).pack(side="left", padx=15)
    status_label = ctk.CTkLabel(top_frame, text="准备就绪", text_color="gray")
    status_label.pack(side="left", padx=10)

    # --- 中间内容区 ---
    content_frame = ctk.CTkFrame(dialog)
    content_frame.grid(row=1, column=0, sticky="nsew", padx=10, pady=5)
    content_frame.grid_columnconfigure(0, weight=1)
    content_frame.grid_rowconfigure(1, weight=1)
    
    ctk.CTkLabel(content_frame, text="大纲内容 (可编辑/AI生成)", font=("Microsoft YaHei", 12, "bold")).grid(row=0, column=0, sticky="w", pady=5)
    outline_text = ctk.CTkTextbox(content_frame, wrap="word", font=("Microsoft YaHei", 12))
    outline_text.grid(row=1, column=0, sticky="nsew", padx=5)
    
    # --- 底部指令区 ---
    bottom_frame = ctk.CTkFrame(dialog)
    bottom_frame.grid(row=2, column=0, sticky="ew", padx=10, pady=10)
    
    ctk.CTkLabel(bottom_frame, text="修改意见 (例如：'让第5章的战斗更惨烈，并在第6章开头增加主角的感悟'):", font=("Microsoft YaHei", 12, "bold")).pack(anchor="w", padx=5)
    instruction_text = ctk.CTkTextbox(bottom_frame, height=80, wrap="word")
    instruction_text.pack(fill="x", padx=5, pady=5)
    
    btn_area = ctk.CTkFrame(bottom_frame, fg_color="transparent")
    btn_area.pack(fill="x", pady=5)

    def on_ai_refine():
        current_content = outline_text.get("0.0", "end").strip()
        instruction = instruction_text.get("0.0", "end").strip()
        s_val = start_chap_entry.get().strip()
        e_val = end_chap_entry.get().strip() or s_val
        
        if not s_val:
            messagebox.showwarning("提示", "请确保已填写章节号。")
            return
        if not current_content:
            messagebox.showwarning("提示", "请先读取章节大纲内容。")
            return
        if not instruction:
            messagebox.showwarning("提示", "请输入修改意见。")
            return
            
        # 读取背景
        arch_file = os.path.join(filepath, "Novel_architecture.txt")
        summary_file = os.path.join(filepath, "global_summary.txt")
        novel_arch_content = read_file(arch_file) if os.path.exists(arch_file) else ""
        global_sum_content = read_file(summary_file) if os.path.exists(summary_file) else ""

        # 获取配置 - 使用专门的目录微调配置
        try:
            llm_var = self.directory_refinement_llm_var.get()  # 使用新的配置变量
            config = self.loaded_config["llm_configs"][llm_var]
        except:
            messagebox.showerror("错误", "无法获取目录微调模型配置。")
            return

        status_label.configure(text="AI 正在思考并微调剧情...", text_color="blue")
        refine_btn.configure(state="disabled")
        
        def run_task():
            try:
                chapter_range_str = f"第{s_val}章" if s_val == e_val else f"第{s_val}章 到 第{e_val}章"
                
                new_outline = refine_chapter_detail(
                    interface_format=config["interface_format"],
                    api_key=config["api_key"],
                    base_url=config["base_url"],
                    model_name=config["model_name"],
                    chapter_range=chapter_range_str, # 传入范围描述
                    novel_architecture=novel_arch_content,
                    global_summary=global_sum_content,
                    current_outline=current_content,
                    user_instruction=instruction,
                    temperature=config["temperature"],
                    max_tokens=config["max_tokens"],
                    timeout=config["timeout"]
                )
                
                if new_outline:
                    self.master.after(0, lambda: outline_text.delete("0.0", "end"))
                    self.master.after(0, lambda: outline_text.insert("0.0", new_outline))
                    self.master.after(0, lambda: status_label.configure(text="✅ 微调完成，请检查", text_color="green"))
                else:
                    self.master.after(0, lambda: status_label.configure(text="❌ 微调失败 (返回空)", text_color="red"))
            except Exception as e:
                self.master.after(0, lambda: status_label.configure(text=f"❌ 出错: {str(e)}", text_color="red"))

            finally:
                self.master.after(0, lambda: refine_btn.configure(state="normal"))

        threading.Thread(target=run_task, daemon=True).start()

    refine_btn = ctk.CTkButton(btn_area, text="AI 微调大纲", command=on_ai_refine, width=120, fg_color="#E67E22")
    refine_btn.pack(side="left", padx=5)

    def on_save_changes():
        content = outline_text.get("0.0", "end").strip()
        if not content:
            messagebox.showwarning("提示", "大纲内容为空。")
            return
            
        s_val = start_chap_entry.get().strip()
        e_val = end_chap_entry.get().strip() or s_val
        
        if not s_val:
            messagebox.showwarning("提示", "请输入起始章节号。")
            return

        try:
            start_num = int(s_val)
            end_num = int(e_val)
            
            if end_num < start_num:
                messagebox.showerror("错误", "结束章节不能小于起始章节")
                return

            # 读取现有目录
            directory_content = read_file(directory_file)
            
            # 使用正则表达式替换指定范围内的章节
            pattern = get_range_pattern(start_num, end_num)
            updated_content = pattern.sub(content, directory_content, count=1)
            
            # 保存修改后的目录
            clear_file_content(directory_file)
            save_string_to_txt(updated_content.strip(), directory_file)
            
            status_label.configure(text="✅ 保存成功", text_color="green")
            
        except ValueError:
            messagebox.showerror("错误", "章节号必须是数字")
        except Exception as e:
            messagebox.showerror("错误", f"保存失败: {str(e)}")

    save_btn = ctk.CTkButton(btn_area, text="保存修改", command=on_save_changes, width=120)
    save_btn.pack(side="left", padx=5)

    def on_cancel():
        dialog.destroy()

    cancel_btn = ctk.CTkButton(btn_area, text="取消", command=on_cancel, width=120)
    cancel_btn.pack(side="right", padx=5)

def show_foreshadowing_records_ui(self):
    """
    [新增] 显示伏笔记录库的弹窗
    """
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先在主Tab中设置保存文件路径")
        return

    record_file = os.path.join(filepath, "foreshadowing_records.txt")
    if not os.path.exists(record_file):
        messagebox.showinfo("提示", "当前还未生成任何伏笔记录。\n请先进行章节定稿(Finalize)以自动生成。")
        return

    content = read_file(record_file).strip()
    if not content:
        content = "伏笔记录为空。"

    top = ctk.CTkToplevel(self.master)
    top.title("全书伏笔线索库 (Foreshadowing Records)")
    top.geometry("700x600")

    # === 【修改】 设置为从属窗口 (Transient) ===
    # 这样它永远会在 self.master (主窗口) 之上，但不会挡住其他软件
    top.transient(self.master)  
    top.lift() # 首次打开时提升一下层级
    
    # 顶部说明
    ctk.CTkLabel(top, text="这里记录了每一章定稿时AI提取的伏笔线索", text_color="gray").pack(pady=5)

    # 文本区域
    text_area = ctk.CTkTextbox(top, wrap="word", font=("Microsoft YaHei", 12))
    text_area.pack(fill="both", expand=True, padx=10, pady=5)
    text_area.insert("0.0", content)
    
    # 允许用户手动编辑和保存整理
    def on_save_edit():
        new_text = text_area.get("0.0", "end").strip()
        save_string_to_txt(new_text, record_file)
        messagebox.showinfo("成功", "伏笔记录已保存更新。")

    btn_frame = ctk.CTkFrame(top)
    btn_frame.pack(fill="x", padx=10, pady=10)
    
    ctk.CTkButton(btn_frame, text="保存修改", command=on_save_edit, fg_color="green").pack(side="right")


def show_novel_qa_ui(self):
    """
    [新增] 全书问答 UI (类似聊天窗口)
    """
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先在主Tab中设置保存文件路径")
        return
        
    # 创建弹窗
    top = ctk.CTkToplevel(self.master)
    top.title("全书知识库问答 (Novel Q&A)")
    top.geometry("600x700")

    # === 【修改】 设置为从属窗口 ===
    top.transient(self.master)
    top.lift()
    
    # 1. 聊天记录显示区
    history_frame = ctk.CTkFrame(top)
    history_frame.pack(fill="both", expand=True, padx=10, pady=10)
    
    chat_box = ctk.CTkTextbox(history_frame, font=("Microsoft YaHei", 12), state="disabled")
    chat_box.pack(fill="both", expand=True, padx=5, pady=5)
    
    # 2. 输入区
    input_frame = ctk.CTkFrame(top)
    input_frame.pack(fill="x", padx=10, pady=(0, 10))
    
    input_entry = ctk.CTkEntry(input_frame, placeholder_text="输入关于小说的问题，例如：叶落现在的等级是多少？", font=("Microsoft YaHei", 12))
    input_entry.pack(side="left", fill="x", expand=True, padx=(5, 5), pady=5)
    
    # 3. 发送逻辑
    def send_question(event=None):
        question = input_entry.get().strip()
        if not question: return
        
        # 显示用户问题
        chat_box.configure(state="normal")
        chat_box.insert("end", f"You: {question}\n\n", "user")
        chat_box.insert("end", "AI: 正在翻阅全书...\n", "system")
        chat_box.see("end")
        chat_box.configure(state="disabled")
        input_entry.delete(0, "end")
        
        def run_qa():
            try:
                # === 获取配置 ===
                # 使用【逻辑/微调模型】来回答问题，因为它通常更便宜且逻辑够用
                # 如果没配置，回退到 draft 模型
                try:
                    llm_key = self.refine_logic_llm_var.get()
                    llm_conf = self.loaded_config["llm_configs"][llm_key]
                except:
                    llm_key = self.prompt_draft_llm_var.get()
                    llm_conf = self.loaded_config["llm_configs"][llm_key]
                
                # === 2. 获取 Embedding 配置 (用于检索) ===
                # 【关键修改】：直接读取主界面当前绑定的变量，而不是读配置文件里的第一个
                # 这样可以确保和你“定稿”时用的是同一个配置，且 API Key 不会为空（只要你界面上填了）
                emb_api_key = self.embedding_api_key_var.get().strip()
                emb_base_url = self.embedding_url_var.get().strip()
                emb_model_name = self.embedding_model_name_var.get().strip()
                emb_interface_format = self.embedding_interface_format_var.get().strip()

                # 简单校验
                if not emb_api_key and "ollama" not in emb_interface_format.lower():
                    update_chat("错误：Embedding API Key 为空，请在配置页检查。", is_error=True)
                    return

                # === 3. 调用后端 ===
                answer = answer_novel_question(
                    filepath=filepath,
                    question=question,
                    # LLM 参数
                    llm_api_key=llm_conf["api_key"],
                    llm_base_url=llm_conf["base_url"],
                    llm_model_name=llm_conf["model_name"],
                    interface_format=llm_conf["interface_format"],
                    # Embedding 参数 (使用直接获取的值)
                    emb_api_key=emb_api_key,
                    emb_base_url=emb_base_url,
                    emb_model_name=emb_model_name,
                    emb_interface_format=emb_interface_format
                )
                
                update_chat(answer)
                
            except Exception as e:
                # 打印完整堆栈以便调试
                traceback.print_exc() 
                update_chat(f"发生错误: {str(e)}", is_error=True)

        threading.Thread(target=run_qa, daemon=True).start()

    def update_chat(text, is_error=False):
        """线程安全更新 UI"""
        def _update():
            chat_box.configure(state="normal")
            # 删除“正在翻阅...”
            # 简单起见，直接追加新内容。为了体验更好，可以把上一行删掉，但追加也没问题。
            prefix = "❌ Error: " if is_error else "AI: "
            chat_box.insert("end", f"\n{prefix}{text}\n" + "-"*30 + "\n\n")
            chat_box.see("end")
            chat_box.configure(state="disabled")
        top.after(0, _update)

    # 按钮
    send_btn = ctk.CTkButton(input_frame, text="发送", width=80, command=send_question)
    send_btn.pack(side="right", padx=5, pady=5)
    
    # 绑定回车发送
    input_entry.bind("<Return>", send_question)

def continue_directory_ui(self):
    """
    续写章节目录的交互界面
    """
    filepath = self.filepath_var.get().strip()
    if not filepath:
        messagebox.showwarning("警告", "请先配置保存文件路径。")
        return
        
    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    if not os.path.exists(arch_file):
        messagebox.showwarning("警告", "尚未生成架构文件 (Novel_architecture.txt)。")
        return

    directory_file = os.path.join(filepath, "Novel_directory.txt")
    existing_chapters_count = 0
    if os.path.exists(directory_file):
        content = read_file(directory_file)
        # 使用正则表达式找出所有章节编号
        pattern = r"第\s*(\d+)\s*章"
        chapter_numbers = re.findall(pattern, content)
        if chapter_numbers:
            chapter_numbers = [int(num) for num in chapter_numbers if num.isdigit()]
            if chapter_numbers:
                existing_chapters_count = max(chapter_numbers)

    # 创建弹窗
    dialog = ctk.CTkToplevel(self.master)
    dialog.title("续写章节大纲")
    dialog.geometry("1000x700")
    
    # 布局配置
    dialog.grid_columnconfigure(0, weight=1)
    dialog.grid_rowconfigure(1, weight=1)

    # --- 顶部控制区 ---
    top_frame = ctk.CTkFrame(dialog)
    top_frame.grid(row=0, column=0, sticky="ew", padx=10, pady=10)
    
    ctk.CTkLabel(top_frame, text="起始章节:").pack(side="left", padx=(10, 5))
    start_chap_entry = ctk.CTkEntry(top_frame, width=60)
    start_chap_entry.pack(side="left", padx=5)
    
    # 设置默认起始章节为现有最大章节+1
    if existing_chapters_count > 0:
        start_chap_entry.insert(0, str(existing_chapters_count + 1))
    
    ctk.CTkLabel(top_frame, text="结束章节:").pack(side="left", padx=(15, 5))
    end_chap_entry = ctk.CTkEntry(top_frame, width=60)
    end_chap_entry.pack(side="left", padx=5)

    # --- 信息提示 ---
    info_frame = ctk.CTkFrame(dialog)
    info_frame.grid(row=1, column=0, sticky="nsew", padx=10, pady=5)
    info_frame.grid_columnconfigure(0, weight=1)
    info_frame.grid_rowconfigure(0, weight=1)
    
    info_text = ctk.CTkTextbox(info_frame, wrap="word", font=("Microsoft YaHei", 12))
    info_text.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)
    
    # 插入提示信息
    info_text.insert("0.0", f"当前已有章节: {existing_chapters_count} 章\n\n")
    info_text.insert("end", "此功能将根据现有架构和目录信息，生成后续章节的目录。\n\n")
    info_text.insert("end", "提示：\n")
    info_text.insert("end", "- 起始章节建议设置为当前最大章节号+1\n")
    info_text.insert("end", "- 结束章节设置为你希望生成到的章节号\n")
    info_text.insert("end", "- 生成的内容将追加到现有目录文件末尾")
    info_text.configure(state="disabled")  # 设为只读

    # --- 底部控制区 ---
    bottom_frame = ctk.CTkFrame(dialog)
    bottom_frame.grid(row=2, column=0, sticky="ew", padx=10, pady=10)
    
    status_label = ctk.CTkLabel(bottom_frame, text=f"当前已有 {existing_chapters_count} 章", text_color="green")
    status_label.pack(side="left", padx=10)
    
    continue_btn = ctk.CTkButton(bottom_frame, text="续写目录 (AI)", command=lambda: on_continue_directory(), fg_color="#3498DB")
    continue_btn.pack(side="right", padx=10)

    def on_continue_directory():
        s_val = start_chap_entry.get().strip()
        e_val = end_chap_entry.get().strip()
        
        if not s_val or not e_val:
            messagebox.showwarning("提示", "请填写起始和结束章节号。")
            return
            
        try:
            start_num = int(s_val)
            end_num = int(e_val)
            
            if end_num < start_num:
                messagebox.showerror("错误", "结束章节不能小于起始章节")
                return
                
            if start_num <= 0 or end_num <= 0:
                messagebox.showerror("错误", "章节号必须是正整数")
                return
                
            if existing_chapters_count > 0 and start_num <= existing_chapters_count:
                messagebox.showwarning("提示", f"起始章节 ({start_num}) 应该大于现有最大章节 ({existing_chapters_count})，否则可能导致章节重复。")
                
        except ValueError:
            messagebox.showerror("错误", "章节号必须是数字")
            return

        # 获取配置 - 使用专门的目录续写配置
        try:
            llm_var = self.directory_continuation_llm_var.get()  # 使用新的配置变量
            config = self.loaded_config["llm_configs"][llm_var]
        except:
            messagebox.showerror("错误", "无法获取目录续写模型配置。")
            return

        status_label.configure(text="AI 正在分析现有目录并续写...", text_color="blue")
        continue_btn.configure(state="disabled")
        
        def run_task():
            try:
                from novel_generator.blueprint import continue_chapter_blueprint
                
                result = continue_chapter_blueprint(
                    interface_format=config["interface_format"],
                    api_key=config["api_key"],
                    base_url=config["base_url"],
                    llm_model=config["model_name"],
                    filepath=filepath,
                    start_chapter=start_num,
                    end_chapter=end_num,
                    user_guidance="",  # 可以扩展以支持用户指导
                    temperature=config["temperature"],
                    max_tokens=config["max_tokens"],
                    timeout=config["timeout"]
                )
                
                if result:
                    self.master.after(0, lambda: status_label.configure(text="✅ 续写完成，请检查", text_color="green"))
                    # 更新主界面的目录显示
                    self.master.after(0, lambda: self.load_chapter_blueprint())
                else:
                    self.master.after(0, lambda: status_label.configure(text="❌ 续写失败 (返回空)", text_color="red"))
            except Exception as e:
                self.master.after(0, lambda: status_label.configure(text=f"❌ 出错: {str(e)}", text_color="red"))
            finally:
                self.master.after(0, lambda: continue_btn.configure(state="normal"))

        threading.Thread(target=run_task, daemon=True).start()
</file>

</files>
